{
  "tasks": [
    {
      "id": 351,
      "command": ["grep"],
      "difficulty": 2,
      "rating": 3,
      "category": "text processing",
      "tags": ["email", "validation", "basic-regex"],
      "task": "Extract all email addresses from contacts.txt",
      "solution": "grep -oE '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' contacts.txt",
      "explanation": "grep -oE (show only matching parts with extended regex) '[a-zA-Z0-9._%+-]+ (username part with allowed characters) @ (literal @) [a-zA-Z0-9.-]+ (domain part) \\. (literal dot) [a-zA-Z]{2,} (TLD with 2+ letters)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract contact emails from mixed text documents"
    },
    {
      "id": 352,
      "command": ["grep", "sed"],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": ["phone", "normalization", "substitution"],
      "task": "Extract and normalize all phone numbers to +1-XXX-XXX-XXXX format from contacts.txt",
      "solution": "grep -oE '\\+?1?[-. (]?[0-9]{3}[-. )][0-9]{3}[-. ][0-9]{4}' contacts.txt | sed 's/[^0-9]//g' | sed 's/^1\\?\\([0-9]\\{3\\}\\)\\([0-9]\\{3\\}\\)\\([0-9]\\{4\\}\\)$/+1-\\1-\\2-\\3/'",
      "explanation": "grep -oE (extract phone patterns) \\+?1? (optional +1 country code) [-. (]? (optional separator) [0-9]{3} (area code) [-. )] (separator) [0-9]{3} (exchange) [-. ] (separator) [0-9]{4} (number) | sed 's/[^0-9]//g' (remove non-digits) | sed (reformat to standard +1-XXX-XXX-XXXX)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Standardize phone number formats in customer databases"
    },
    {
      "id": 353,
      "command": ["grep"],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": ["logs", "ip-address", "security"],
      "task": "Find all IP addresses in server.log that are not from private networks",
      "solution": "grep -oE '([0-9]{1,3}\\.){3}[0-9]{1,3}' server.log | grep -vE '^(10\\.|172\\.(1[6-9]|2[0-9]|3[01])\\.|192\\.168\\.)'",
      "explanation": "grep -oE (extract IP patterns) ([0-9]{1,3}\\.) (octet with dot) {3} (repeated 3 times) [0-9]{1,3} (final octet) | grep -vE (exclude patterns) ^(10\\.|172\\.(1[6-9]|2[0-9]|3[01])\\.|192\\.168\\.) (private IP ranges RFC 1918)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security monitoring to identify external IP addresses in logs"
    },
    {
      "id": 354,
      "command": ["grep"],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": ["logs", "error-analysis", "timestamps"],
      "task": "Extract all ERROR entries from server.log with their timestamps and error details",
      "solution": "grep -E '^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2} \\[ERROR\\].*' server.log",
      "explanation": "grep -E (extended regex) ^ (line start) [0-9]{4}-[0-9]{2}-[0-9]{2} (YYYY-MM-DD date format) [0-9]{2}:[0-9]{2}:[0-9]{2} (HH:MM:SS time) \\[ERROR\\] (literal [ERROR]) .* (rest of line)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quickly identify and analyze error patterns in application logs"
    },
    {
      "id": 355,
      "command": ["sed"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "data-cleaning", "dates"],
      "task": "Standardize all date formats in data_mixed.txt to YYYY-MM-DD format",
      "solution": "sed -E 's|([0-9]{1,2})/([0-9]{1,2})/([0-9]{4})|\\3-\\1-\\2|g; s|([A-Za-z]{3}) ([0-9]{1,2}) ([0-9]{4})|\\3-\\1-\\2|g; s|([0-9]{1,2})\\.([0-9]{1,2})\\.([0-9]{4})|\\3-\\1-\\2|g; s|([0-9]{1,2})-([A-Za-z]{3})-([0-9]{4})|\\3-\\1-\\2|g' data_mixed.txt",
      "explanation": "sed -E (extended regex) with multiple substitution patterns: s|MM/DD/YYYY|YYYY-MM-DD|g for slash format, s|Mon DD YYYY|YYYY-MM-DD|g for text month, s|DD.MM.YYYY|YYYY-MM-DD|g for dot format, s|DD-Mon-YYYY|YYYY-MM-DD|g for dash format",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Clean and standardize date formats in mixed data sources"
    },
    {
      "id": 356,
      "command": ["grep", "wc"],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": ["validation", "credit-card", "patterns"],
      "task": "Count how many valid credit card patterns (4 groups of 4 digits) exist in mixed_patterns.txt",
      "solution": "grep -oE '[0-9]{4}-[*]{4}-[*]{4}-[0-9]{4}|[0-9]{4}[ -][0-9]{4}[ -][0-9]{4}[ -][0-9]{4}' mixed_patterns.txt | wc -l",
      "explanation": "grep -oE (extract patterns) [0-9]{4}-[*]{4}-[*]{4}-[0-9]{4} (masked format XXXX-****-****-XXXX) | (OR) [0-9]{4}[ -][0-9]{4}[ -][0-9]{4}[ -][0-9]{4} (full number with space/dash separators) | wc -l (count lines)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Validate credit card format compliance in data files"
    },
    {
      "id": 357,
      "command": ["grep"],
      "difficulty": 4,
      "rating": 5,
      "category": "security",
      "tags": ["validation", "api-keys", "patterns"],
      "task": "Extract all API keys and tokens from mixed_patterns.txt with their prefixes",
      "solution": "grep -oE '(sk|pk)_[a-zA-Z0-9_]{16,}|[a-fA-F0-9]{32,}' mixed_patterns.txt",
      "explanation": "grep -oE (extract patterns) (sk|pk) (secret/public key prefixes) _ (underscore) [a-zA-Z0-9_]{16,} (alphanumeric+underscore, 16+ chars) | (OR) [a-fA-F0-9]{32,} (hexadecimal strings 32+ chars for hash values)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security audit to find exposed API keys and sensitive tokens in code/logs"
    },
    {
      "id": 358,
      "command": ["sed"],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": ["web-logs", "url-parsing", "query-parameters"],
      "task": "Extract query parameters from URLs in access.log and show parameter=value pairs",
      "solution": "sed -nE 's/.*\"[A-Z]+ ([^?]+\\?([^\"]*)).*/\\2/p' access.log | sed -E 's/&/\\n/g' | grep -E '.+=.+'",
      "explanation": "sed -nE (extended regex, suppress output) s/.*\"[A-Z]+ ([^?]+\\?([^\"]*)).*/\\2/p (extract query string after ?) | sed -E 's/&/\\n/g' (split parameters on new lines) | grep -E '.+=.+' (show only param=value pairs)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze search queries and parameters in web server logs"
    },
    {
      "id": 359,
      "command": ["grep", "sort"],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": ["web-logs", "http-status", "analysis"],
      "task": "Extract and count HTTP status codes from access.log showing frequency distribution",
      "solution": "grep -oE '\" [0-9]{3} ' access.log | grep -oE '[0-9]{3}' | sort | uniq -c | sort -nr",
      "explanation": "grep -oE '\" [0-9]{3} ' (extract HTTP status pattern with quotes and spaces) | grep -oE '[0-9]{3}' (extract just the 3-digit status code) | sort (arrange for counting) | uniq -c (count occurrences) | sort -nr (sort by count descending)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Monitor web application health by analyzing HTTP response codes"
    },
    {
      "id": 360,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["csv", "regex", "validation", "data-quality"],
      "task": "Validate email addresses in employees.csv and report invalid entries with line numbers",
      "solution": "awk -F, 'NR>1 && $3 !~ /^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$/ {print \"Line \" NR \": Invalid email - \" $3}' employees.csv",
      "explanation": "awk -F, (comma delimiter) NR>1 (skip header) && (AND) $3 !~ (field 3 does NOT match) /^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$/ (email regex pattern) {print \"Line \" NR \": Invalid email - \" $3} (show line number and invalid email)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Data quality validation for email fields in CSV imports"
    },
    {
      "id": 361,
      "command": ["sed"],
      "difficulty": 4,
      "rating": 4,
      "category": "development",
      "tags": ["code", "regex-patterns", "extraction"],
      "task": "Extract all regex patterns from code_samples.txt showing the pattern and programming language",
      "solution": "sed -nE 's/.*(pattern|regex|PATTERN)\\s*=\\s*[\"'\\'']([^\"'\\'']*)[\"\\''].*/\\2/p; s/.*\\/([^\\/]*)\\/.*/\\1/p' code_samples.txt | sed -E 's/^/Regex: /'",
      "explanation": "sed -nE (extended regex, no auto-print) s/.*(pattern|regex|PATTERN)\\s*=\\s*[\"'\\'']([^\"'\\'']*)[\"\\''].*/\\2/p (extract patterns from variable assignments) s/.*\\/([^\\/]*)\\/.*/\\1/p (extract patterns from /pattern/ format) | sed -E 's/^/Regex: /' (add prefix)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Code analysis to inventory regex patterns used in projects"
    },
    {
      "id": 362,
      "command": ["grep", "cut"],
      "difficulty": 3,
      "rating": 3,
      "category": "data analysis",
      "tags": ["csv", "salary", "filtering"],
      "task": "Find employees with salaries above $75000 and show only their names and salaries",
      "solution": "grep -E ',7[5-9][0-9]{3}|,8[0-9]{4}|,[9][0-9]{4}|,[0-9]{6,},' employees.csv | cut -d, -f2,6",
      "explanation": "grep -E (extended regex) ,7[5-9][0-9]{3} (75000-79999) | ,8[0-9]{4} (80000-89999) | ,[9][0-9]{4} (90000-99999) | ,[0-9]{6,}, (100000+) employees.csv | cut -d, -f2,6 (extract name and salary fields)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "HR analysis to identify high-earning employees for reviews"
    },
    {
      "id": 363,
      "command": ["sed"],
      "difficulty": 4,
      "rating": 5,
      "category": "security",
      "tags": ["masking", "sensitive-data", "privacy"],
      "task": "Mask all but last 4 digits of credit card numbers in mixed_patterns.txt",
      "solution": "sed -E 's/([0-9]{4})[-. ]?([0-9]{4})[-. ]?([0-9]{4})[-. ]?([0-9]{4})/****-****-****-\\4/g' mixed_patterns.txt",
      "explanation": "sed -E (extended regex) s/ (substitute) ([0-9]{4}) (capture first 4 digits) [-. ]? (optional separator) ([0-9]{4}) (capture second group) [-. ]? (optional separator) ([0-9]{4}) (capture third group) [-. ]? (optional separator) ([0-9]{4}) (capture last 4 digits) /****-****-****-\\4/g (replace with masked format keeping only last 4 digits)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Data privacy compliance by masking sensitive financial information"
    },
    {
      "id": 364,
      "command": ["grep", "awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": ["web-logs", "performance", "response-time"],
      "task": "Find slow requests (>2 second response time) in access.log and show URL and response time",
      "solution": "awk '$NF > 2000 {print $(NF-1), $NF \"ms\"}' access.log 2>/dev/null || grep -oE '\"[A-Z]+ [^\"]*\" [0-9]{3} [0-9]+' access.log | awk '$4 > 2000 {print $2, $4 \"ms\"}'",
      "explanation": "awk '$NF > 2000 (if last field > 2000ms) {print $(NF-1), $NF \"ms\"} (show URL and time) || fallback to grep -oE (extract request pattern) with awk filtering for response times > 2000ms",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Performance monitoring to identify slow web requests"
    },
    {
      "id": 365,
      "command": ["sed", "sort"],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": ["urls", "domain-extraction", "analytics"],
      "task": "Extract unique domains from all URLs in sample.html and count their frequency",
      "solution": "sed -nE 's/.*href=\"https?:\\/\\/([^/\"]*).*/\\1/p' sample.html | sort | uniq -c | sort -nr",
      "explanation": "sed -nE (extended regex, no auto-print) s/.*href=\"https?:\\/\\/ (match href with http/https) ([^/\"]*) (capture domain until / or \") .*/\\1/p (extract domain) | sort (arrange for counting) | uniq -c (count occurrences) | sort -nr (sort by frequency)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Web analytics to identify most referenced external domains"
    },
    {
      "id": 366,
      "command": ["grep", "sed"],
      "difficulty": 3,
      "rating": 3,
      "category": "data analysis",
      "tags": ["configuration", "key-value", "parsing"],
      "task": "Extract database configuration values from config.ini and format as JSON",
      "solution": "sed -n '/\\[database\\]/,/\\[.*\\]/p' config.ini | grep -E '^[^\\[].*=' | sed -E 's/^([^=]+)=(.*)$/\"\\1\": \"\\2\",/' | sed '$ s/,$//'",
      "explanation": "sed -n '/\\[database\\]/,/\\[.*\\]/p' (extract database section) | grep -E '^[^\\[].*=' (get key=value lines) | sed -E 's/^([^=]+)=(.*)$/\"\\1\": \"\\2\",/' (format as JSON) | sed '$ s/,$//' (remove last comma)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Configuration management and JSON conversion for APIs"
    },
    {
      "id": 367,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["csv", "statistics", "aggregation"],
      "task": "Calculate average salary by department from employees.csv with department employee count",
      "solution": "awk -F, 'NR>1 {dept[$5] += $6; count[$5]++} END {for (d in dept) printf \"%-15s: $%8.0f (n=%d)\\n\", d, dept[d]/count[d], count[d]}' employees.csv | sort",
      "explanation": "awk -F, (comma delimiter) NR>1 (skip header) {dept[$5] += $6; count[$5]++} (accumulate salary by department and count employees) END {for (d in dept) printf (format output) dept[d]/count[d] (calculate average) | sort (alphabetical order)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "HR analytics for salary analysis and budget planning by department"
    },
    {
      "id": 368,
      "command": ["grep", "sed"],
      "difficulty": 4,
      "rating": 4,
      "category": "security",
      "tags": ["logs", "ip-geolocation", "attack-patterns"],
      "task": "Extract failed login attempts with IP addresses and format for security analysis",
      "solution": "grep 'Failed login\\|login attempt' server.log | sed -E 's/.* from ([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}).*/\\1/' | sort | uniq -c | sort -nr | sed -E 's/^ *([0-9]+) (.*)$/IP: \\2 - Attempts: \\1/'",
      "explanation": "grep 'Failed login\\|login attempt' (find failed login lines) | sed -E (extract IP addresses) | sort | uniq -c (count attempts per IP) | sort -nr (sort by attempt count) | sed -E (format output as 'IP: address - Attempts: count')",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security monitoring to identify brute force attacks and suspicious IPs"
    },
    {
      "id": 369,
      "command": ["sed", "awk"],
      "difficulty": 5,
      "rating": 5,
      "category": "development",
      "tags": ["json", "validation", "data-extraction"],
      "task": "Extract and validate all JSON objects from sample.json and show structure summary",
      "solution": "sed -nE '/\\{/,/\\}/p' sample.json | awk '/^[[:space:]]*\"[^\"]*\"[[:space:]]*:/ {gsub(/[[:space:]]*\"([^\"]*)\".*/,\"\\1\"); fields[NR]=$0} END {print \"JSON Fields Found:\"; for(i in fields) print \"  \" fields[i]}'",
      "explanation": "sed -nE '/\\{/,/\\}/p' (extract JSON object blocks) | awk (process field lines) /^[[:space:]]*\"[^\"]*\"[[:space:]]*:/ (match JSON field pattern) {gsub (extract field names) fields[NR]=$0 (store fields) END {print summary}",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "API development and JSON schema analysis for data validation"
    },
    {
      "id": 370,
      "command": ["grep", "sed", "awk"],
      "difficulty": 5,
      "rating": 5,
      "category": "text processing",
      "tags": ["complex-regex", "data-transformation", "multi-format"],
      "task": "Create a comprehensive data report from mixed_patterns.txt showing counts of each data type found",
      "solution": "echo 'Data Type Analysis Report:'; echo '========================'; echo -n 'Credit Cards: '; grep -cE '[0-9]{4}[-* ][0-9*]{4}[-* ][0-9*]{4}[-* ][0-9]{4}' mixed_patterns.txt; echo -n 'IP Addresses: '; grep -cE '([0-9]{1,3}\\.){3}[0-9]{1,3}' mixed_patterns.txt; echo -n 'Phone Numbers: '; grep -cE '\\+?[0-9() .-]{10,}' mixed_patterns.txt; echo -n 'Email Addresses: '; grep -cE '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' mixed_patterns.txt; echo -n 'URLs: '; grep -cE 'https?://[^[:space:]]+' mixed_patterns.txt; echo -n 'MAC Addresses: '; grep -cE '([0-9A-Fa-f]{2}:){5}[0-9A-Fa-f]{2}' mixed_patterns.txt",
      "explanation": "Complex multi-pattern analysis using grep -cE (count matches) for each data type: credit cards (masked/unmasked), IP addresses (IPv4), phone numbers (various formats), email addresses, URLs (http/https), and MAC addresses (colon-separated hex format)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Data discovery and compliance reporting to identify sensitive information types in documents"
    }
  ]
}