{
  "tasks": [
    {
      "id": 411,
      "command": ["docker", "run"],
      "difficulty": 1,
      "rating": 2,
      "category": "containerization",
      "tags": ["basics", "interactive", "ubuntu"],
      "task": "Run an interactive Ubuntu container and execute bash shell",
      "solution": "docker run -it ubuntu bash",
      "explanation": "docker run (create and start container) -it (interactive with TTY) ubuntu (base image) bash (command to execute inside container)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Quick testing environment or debugging within a clean Ubuntu system"
    },
    {
      "id": 412,
      "command": ["docker", "ps"],
      "difficulty": 1,
      "rating": 2,
      "category": "monitoring",
      "tags": ["status", "containers", "listing"],
      "task": "List all running containers with their CPU and memory usage",
      "solution": "docker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\"",
      "explanation": "docker ps (list containers) --format (custom output format) with table format showing container names, status, CPU percentage, and memory usage",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Monitor resource consumption of running containers for performance optimization"
    },
    {
      "id": 413,
      "command": ["docker", "build"],
      "difficulty": 2,
      "rating": 3,
      "category": "development",
      "tags": ["dockerfile", "build", "caching"],
      "task": "Build a Docker image with custom tag and no cache",
      "solution": "docker build --no-cache -t myapp:v1.0 .",
      "explanation": "docker build (build image from Dockerfile) --no-cache (ignore build cache) -t myapp:v1.0 (tag image with name and version) . (use current directory as build context)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "dockerfile"],
      "warnings": null,
      "use_case": "Ensure clean build when dependencies or base image have changed"
    },
    {
      "id": 414,
      "command": ["docker", "exec"],
      "difficulty": 2,
      "rating": 3,
      "category": "debugging",
      "tags": ["troubleshooting", "running-container", "shell"],
      "task": "Execute interactive bash session in a running container named 'webapp'",
      "solution": "docker exec -it webapp bash",
      "explanation": "docker exec (execute command in running container) -it (interactive with TTY) webapp (container name) bash (shell command)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Debug issues, inspect files, or run commands inside running containers"
    },
    {
      "id": 415,
      "command": ["docker", "logs"],
      "difficulty": 2,
      "rating": 3,
      "category": "debugging",
      "tags": ["logs", "troubleshooting", "real-time"],
      "task": "Follow logs of a container in real-time showing only last 50 lines",
      "solution": "docker logs -f --tail 50 container_name",
      "explanation": "docker logs (show container logs) -f (follow/stream logs in real-time) --tail 50 (show only last 50 lines) container_name (target container)",
      "execution_time": "long-running",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Monitor application behavior and troubleshoot issues in real-time"
    },
    {
      "id": 416,
      "command": ["docker", "volume"],
      "difficulty": 3,
      "rating": 3,
      "category": "data management",
      "tags": ["volumes", "persistent-data", "backup"],
      "task": "Create a named volume and mount it to /data in a container",
      "solution": "docker volume create mydata && docker run -v mydata:/data alpine ls /data",
      "explanation": "docker volume create mydata (create named volume) && (chain commands) docker run -v mydata:/data (mount volume to /data) alpine ls /data (list contents of mounted directory)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Persist data between container restarts or share data between containers"
    },
    {
      "id": 417,
      "command": ["docker", "network"],
      "difficulty": 3,
      "rating": 4,
      "category": "networking",
      "tags": ["custom-network", "container-communication", "isolation"],
      "task": "Create custom bridge network and run two containers that can communicate",
      "solution": "docker network create mynet && docker run -d --network mynet --name web nginx && docker run --network mynet alpine ping web",
      "explanation": "docker network create mynet (create custom bridge network) && docker run -d --network mynet --name web nginx (run nginx in background on custom network) && docker run --network mynet alpine ping web (test connectivity between containers using container name)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Isolate container groups and enable service discovery by container names"
    },
    {
      "id": 418,
      "command": ["docker", "compose"],
      "difficulty": 3,
      "rating": 4,
      "category": "orchestration",
      "tags": ["multi-container", "yaml", "services"],
      "task": "Start all services defined in docker-compose.yml in detached mode",
      "solution": "docker compose up -d",
      "explanation": "docker compose up (start services defined in docker-compose.yml) -d (detached mode, run in background)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "docker-compose"],
      "warnings": null,
      "use_case": "Deploy multi-container applications with predefined configuration"
    },
    {
      "id": 419,
      "command": ["docker", "cp"],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": ["file-transfer", "container-host", "backup"],
      "task": "Copy files from running container to host and vice versa",
      "solution": "docker cp container_name:/app/config.json ./backup/ && docker cp ./new_config.json container_name:/app/",
      "explanation": "docker cp container_name:/app/config.json ./backup/ (copy file from container to host) && docker cp ./new_config.json container_name:/app/ (copy file from host to container)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Backup configuration files or deploy updated files to running containers"
    },
    {
      "id": 420,
      "command": ["docker", "stats"],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": ["performance", "resources", "real-time"],
      "task": "Monitor real-time resource usage of all containers without streaming",
      "solution": "docker stats --no-stream --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.NetIO}}\"",
      "explanation": "docker stats (show container resource usage) --no-stream (single snapshot, don't continuously update) --format (custom output format showing container name, CPU%, memory usage, and network I/O)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Quick performance check for resource optimization and capacity planning"
    },
    {
      "id": 421,
      "command": ["docker", "system"],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": ["cleanup", "disk-space", "maintenance"],
      "task": "Clean up all unused Docker resources and show space reclaimed",
      "solution": "docker system prune -af --volumes",
      "explanation": "docker system prune (remove unused Docker objects) -a (remove all unused images, not just dangling) -f (force, don't prompt for confirmation) --volumes (also remove unused volumes)",
      "execution_time": "1-5 min",
      "requirements": ["docker"],
      "warnings": "This will remove ALL unused containers, networks, images, and volumes",
      "use_case": "Regular maintenance to reclaim disk space on Docker hosts"
    },
    {
      "id": 422,
      "command": ["docker", "inspect"],
      "difficulty": 3,
      "rating": 3,
      "category": "debugging",
      "tags": ["metadata", "configuration", "json"],
      "task": "Extract specific configuration details from a container or image",
      "solution": "docker inspect --format='{{.Config.Env}}' container_name",
      "explanation": "docker inspect (return detailed information about Docker objects) --format='{{.Config.Env}}' (extract only environment variables using Go template) container_name (target container)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Debug configuration issues or extract specific metadata for automation scripts"
    },
    {
      "id": 423,
      "command": ["docker", "save", "load"],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": ["image-backup", "offline-transfer", "archive"],
      "task": "Export Docker image to tar file and import it on another system",
      "solution": "docker save -o myapp.tar myapp:latest && docker load -i myapp.tar",
      "explanation": "docker save -o myapp.tar myapp:latest (export image to tar archive) && docker load -i myapp.tar (import image from tar archive)",
      "execution_time": "1-5 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Transfer images to air-gapped systems or create offline backups"
    },
    {
      "id": 424,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 4,
      "category": "advanced deployment",
      "tags": ["port-mapping", "environment", "volumes", "resource-limits"],
      "task": "Run container with port mapping, environment variables, volume mount, and resource limits",
      "solution": "docker run -d -p 8080:80 -e NODE_ENV=production -v $(pwd)/data:/app/data --memory=512m --cpus=1.5 --name webapp myapp:latest",
      "explanation": "docker run -d (detached) -p 8080:80 (map host port 8080 to container port 80) -e NODE_ENV=production (set environment variable) -v $(pwd)/data:/app/data (mount host directory) --memory=512m (limit memory) --cpus=1.5 (limit CPU) --name webapp (container name) myapp:latest (image)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Production deployment with proper resource constraints and configuration"
    },
    {
      "id": 425,
      "command": ["docker", "commit"],
      "difficulty": 3,
      "rating": 3,
      "category": "development",
      "tags": ["image-creation", "container-snapshot", "versioning"],
      "task": "Create new image from modified container with custom message and author",
      "solution": "docker commit -m \"Added custom configuration\" -a \"developer@company.com\" container_name myapp:v2.0",
      "explanation": "docker commit (create image from container) -m \"Added custom configuration\" (commit message) -a \"developer@company.com\" (author) container_name (source container) myapp:v2.0 (new image name and tag)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": "Committed images are larger than built images; prefer Dockerfile approach",
      "use_case": "Quick prototyping or saving container state after manual configuration"
    },
    {
      "id": 426,
      "command": ["docker", "tag", "push"],
      "difficulty": 3,
      "rating": 3,
      "category": "registry",
      "tags": ["docker-hub", "image-sharing", "deployment"],
      "task": "Tag local image for Docker Hub and push it to registry",
      "solution": "docker tag myapp:latest username/myapp:latest && docker push username/myapp:latest",
      "explanation": "docker tag myapp:latest username/myapp:latest (create tag with registry format) && docker push username/myapp:latest (upload image to Docker Hub)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "docker-hub-account"],
      "warnings": null,
      "use_case": "Share images publicly or deploy to cloud platforms"
    },
    {
      "id": 427,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 5,
      "category": "security",
      "tags": ["read-only", "user", "capabilities", "security"],
      "task": "Run container with enhanced security: read-only filesystem, non-root user, dropped capabilities",
      "solution": "docker run --read-only --user 1000:1000 --cap-drop ALL --cap-add NET_BIND_SERVICE --tmpfs /tmp alpine",
      "explanation": "docker run --read-only (filesystem cannot be modified) --user 1000:1000 (run as specific user:group, not root) --cap-drop ALL (remove all Linux capabilities) --cap-add NET_BIND_SERVICE (add only needed capability) --tmpfs /tmp (temporary filesystem for /tmp) alpine",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Production security hardening to minimize attack surface"
    },
    {
      "id": 428,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": ["bind-mount", "data-processing", "jupyter"],
      "task": "Run Jupyter notebook with local data directory mounted and specific port",
      "solution": "docker run -d -p 8888:8888 -v $(pwd)/data:/home/jovyan/work --name jupyter jupyter/scipy-notebook",
      "explanation": "docker run -d (detached) -p 8888:8888 (expose Jupyter port) -v $(pwd)/data:/home/jovyan/work (mount local data directory to Jupyter workspace) --name jupyter (container name) jupyter/scipy-notebook (data science image with libraries)",
      "execution_time": "1-5 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Quick data science environment with access to local datasets"
    },
    {
      "id": 429,
      "command": ["docker", "run"],
      "difficulty": 5,
      "rating": 5,
      "category": "development",
      "tags": ["gpu", "machine-learning", "nvidia", "resources"],
      "task": "Run GPU-enabled container for machine learning with shared memory and device access",
      "solution": "docker run --gpus all --shm-size=2g -v $(pwd)/models:/workspace/models -p 8080:8080 tensorflow/tensorflow:latest-gpu",
      "explanation": "docker run --gpus all (enable all GPU devices) --shm-size=2g (increase shared memory for large datasets) -v $(pwd)/models:/workspace/models (mount model directory) -p 8080:8080 (expose service port) tensorflow/tensorflow:latest-gpu (GPU-enabled TensorFlow image)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "nvidia-docker", "gpu"],
      "warnings": "Requires NVIDIA Docker runtime and compatible GPU drivers",
      "use_case": "Machine learning training and inference with GPU acceleration"
    },
    {
      "id": 430,
      "command": ["docker", "exec"],
      "difficulty": 4,
      "rating": 4,
      "category": "debugging",
      "tags": ["database", "backup", "automation", "mysql"],
      "task": "Create database backup from running MySQL container and save to host",
      "solution": "docker exec mysql_container mysqldump -u root -p$MYSQL_ROOT_PASSWORD --all-databases > backup_$(date +%Y%m%d).sql",
      "explanation": "docker exec mysql_container (execute in MySQL container) mysqldump -u root -p$MYSQL_ROOT_PASSWORD (MySQL dump with credentials) --all-databases (backup all databases) > backup_$(date +%Y%m%d).sql (redirect to timestamped file on host)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "mysql-container"],
      "warnings": "Ensure MYSQL_ROOT_PASSWORD environment variable is set",
      "use_case": "Automated database backups in containerized environments"
    },
    {
      "id": 431,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["monai", "dicom", "preprocessing", "gpu"],
      "task": "Run MONAI container to preprocess DICOM files with GPU acceleration",
      "solution": "docker run --gpus all projectmonai/monai:latest python -c \"from monai.apps import download_and_extract; download_and_extract('https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar', './data'); import monai; monai.transforms.LoadImaged(keys=['image'])\"",
      "explanation": "docker run --gpus all (enable GPU) projectmonai/monai:latest (official MONAI image) python -c (download MSD Spleen dataset automatically and execute MONAI preprocessing with built-in LoadImaged transform)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "nvidia-docker", "gpu"],
      "warnings": "Requires NVIDIA Docker runtime for GPU acceleration",
      "use_case": "Preprocess medical DICOM images for deep learning pipelines"
    },
    {
      "id": 432,
      "command": ["docker", "run"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dcmtk", "dicom", "conversion", "anonymization"],
      "task": "Run DCMTK container to anonymize and convert DICOM files",
      "solution": "docker run dcmtk/dcmtk sh -c \"echo '(0008,0020) DA [20240101]' > sample.dcm && dcmodify --replace 'PatientName=Anonymous' --replace 'PatientID=ANON001' sample.dcm\"",
      "explanation": "docker run dcmtk/dcmtk (DCMTK toolkit image) sh -c (shell command to create sample DICOM and modify it) echo (create minimal DICOM-like file) && dcmodify (DICOM modifier tool) --replace (replace DICOM tags) PatientName and PatientID with anonymous values",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Anonymize patient data before sharing for research or training"
    },
    {
      "id": 433,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["fastai", "medical", "training", "jupyter"],
      "task": "Run FastAI medical imaging environment with pre-trained models",
      "solution": "docker run -d -p 8888:8888 --gpus all fastdotai/fastai:latest sh -c \"python -c 'from fastai.medical.imaging import *; URLs.SIIM_SMALL.download()' && jupyter lab --ip=0.0.0.0 --allow-root\"",
      "explanation": "docker run -d (detached) -p 8888:8888 (Jupyter port) --gpus all (GPU access) fastdotai/fastai:latest sh -c (download SIIM medical imaging dataset automatically using FastAI's built-in URLs.SIIM_SMALL and start Jupyter)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "nvidia-docker", "gpu"],
      "warnings": null,
      "use_case": "Medical image classification and segmentation with transfer learning"
    },
    {
      "id": 434,
      "command": ["docker", "run"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["3dslicer", "visualization", "nifti", "interactive"],
      "task": "Run 3D Slicer container with X11 forwarding for interactive visualization",
      "solution": "docker run -it --rm -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix slicer/slicer:latest sh -c \"python -c 'import SampleData; SampleData.downloadSample(\\\"MRHead\\\")' && Slicer\"",
      "explanation": "docker run -it --rm (interactive, remove after exit) -e DISPLAY=$DISPLAY (set display environment) -v /tmp/.X11-unix:/tmp/.X11-unix (X11 socket) slicer/slicer:latest sh -c (download built-in MRHead sample data and start 3D Slicer)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "x11-server"],
      "warnings": "Requires X11 server and xhost +local:docker for GUI access",
      "use_case": "Interactive 3D visualization and analysis of medical volumes"
    },
    {
      "id": 435,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["monai", "segmentation", "training", "unet"],
      "task": "Train MONAI U-Net model for medical image segmentation with custom dataset",
      "solution": "docker run --gpus all projectmonai/monai:latest python -c \"from monai.apps import download_and_extract; download_and_extract('https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar', './data'); from monai.networks.nets import UNet; model = UNet(spatial_dims=3, in_channels=1, out_channels=2, channels=(16, 32, 64), strides=(2, 2)); print('Training setup ready with MSD Spleen data')\"",
      "explanation": "docker run --gpus all (GPU training) projectmonai/monai:latest (MONAI image) python -c (download Medical Segmentation Decathlon Spleen dataset automatically and initialize U-Net model with 3D spatial dimensions, 1 input channel, 2 output classes)",
      "execution_time": "5+ min",
      "requirements": ["docker", "nvidia-docker", "gpu", "training-script"],
      "warnings": "Long-running training process, monitor GPU memory usage",
      "use_case": "Train custom segmentation models for organ or pathology detection"
    },
    {
      "id": 436,
      "command": ["docker", "run"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dcm2niix", "conversion", "bids", "neuroimaging"],
      "task": "Convert DICOM files to NIfTI format using dcm2niix in container",
      "solution": "docker run rordenlab/dcm2niix sh -c \"python -c 'import pydicom; ds = pydicom.Dataset(); ds.PatientName = \\\"Test^Patient\\\"; ds.save_as(\\\"sample.dcm\\\")' && dcm2niix -o /tmp -f %s_%p sample.dcm\"",
      "explanation": "docker run rordenlab/dcm2niix (dcm2niix image) sh -c (create sample DICOM file using pydicom and convert it) python -c (generate minimal DICOM file) && dcm2niix -o /tmp (output to temp) -f %s_%p (filename format: series_protocol) sample.dcm (convert generated file)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Convert neuroimaging DICOM to NIfTI for analysis workflows"
    },
    {
      "id": 437,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["itk", "registration", "resampling", "cpp"],
      "task": "Run ITK container for image registration and resampling operations",
      "solution": "docker run insighttoolkit/itk:latest python -c \"import itk; import numpy as np; image = itk.image_from_array(np.random.rand(64,64,64).astype(np.float32)); itk.imwrite(image, 'sample.nii'); print('Generated sample 3D image for ITK processing')\"",
      "explanation": "docker run insighttoolkit/itk:latest (ITK image) python -c (generate sample 3D medical image using numpy random data and save as NIfTI format, then demonstrate ITK image processing capabilities)",
      "execution_time": "1-5 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Register multi-modal medical images or align temporal sequences"
    },
    {
      "id": 438,
      "command": ["docker", "run"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["paraview", "visualization", "vtk", "volume-rendering"],
      "task": "Run ParaView container for 3D medical data visualization with VNC",
      "solution": "docker run -d -p 5901:5901 kitware/paraview:latest sh -c \"python -c 'from paraview.simple import *; wavelet = Wavelet(); Show(wavelet)' && vncserver :1 -geometry 1280x1024\"",
      "explanation": "docker run -d (detached) -p 5901:5901 (VNC port) kitware/paraview:latest (ParaView image) sh -c (create sample wavelet data using ParaView's built-in Wavelet source and start VNC server with 1280x1024 resolution)",
      "execution_time": "< 1 min",
      "requirements": ["docker", "vnc-viewer"],
      "warnings": null,
      "use_case": "Advanced 3D visualization and analysis of complex medical datasets"
    },
    {
      "id": 439,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["monai", "label", "data-augmentation", "transforms"],
      "task": "Run MONAI container for advanced data augmentation pipeline on medical images",
      "solution": "docker run --gpus all projectmonai/monai:latest python -c \"from monai.apps import download_and_extract; download_and_extract('https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar', './data'); from monai.transforms import *; transforms = Compose([LoadImaged(keys=['image']), RandRotate90d(keys=['image'], prob=0.5), RandFlipd(keys=['image'], prob=0.5)]); print('Augmentation pipeline ready')\"",
      "explanation": "docker run --gpus all (GPU acceleration for transforms) projectmonai/monai:latest python -c (download MSD Spleen dataset and execute MONAI transform pipeline with LoadImaged, random rotation and flipping using built-in dataset)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "nvidia-docker"],
      "warnings": null,
      "use_case": "Augment limited medical training datasets to improve model generalization"
    },
    {
      "id": 440,
      "command": ["docker", "run"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["itksnap", "segmentation", "manual", "gui"],
      "task": "Run ITK-SNAP container for manual image segmentation with GUI",
      "solution": "docker run -it --rm -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix pyushkevich/itksnap sh -c \"python -c 'import numpy as np; import nibabel as nib; data=np.random.rand(64,64,64); img=nib.Nifti1Image(data, np.eye(4)); nib.save(img, \\\"sample.nii.gz\\\")' && itksnap sample.nii.gz\"",
      "explanation": "docker run -it --rm (interactive, cleanup) -e DISPLAY=$DISPLAY (X11 display) -v /tmp/.X11-unix:/tmp/.X11-unix (X11 socket) pyushkevich/itksnap sh -c (generate sample 3D image using numpy and nibabel, then open in ITK-SNAP for manual segmentation)",
      "execution_time": "long-running",
      "requirements": ["docker", "x11-server"],
      "warnings": "Requires X11 forwarding and may need xhost +local:docker",
      "use_case": "Manual segmentation and annotation of medical images for ground truth creation"
    },
    {
      "id": 441,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["fastai", "classification", "dicom", "inference"],
      "task": "Run FastAI inference on DICOM images using pre-trained medical model",
      "solution": "docker run --gpus all fastdotai/fastai:latest python -c \"from fastai.medical.imaging import *; from fastai.vision.all import *; path = URLs.SIIM_SMALL.download(); dls = ImageDataLoaders.from_folder(path, train='train', valid='valid'); learn = vision_learner(dls, resnet18); print('FastAI medical model ready for inference')\"",
      "explanation": "docker run --gpus all (GPU inference) fastdotai/fastai:latest python -c (download SIIM medical imaging dataset automatically using FastAI's built-in URLs.SIIM_SMALL and setup pre-trained ResNet18 model for medical image inference)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "nvidia-docker", "gpu", "pre-trained-model"],
      "warnings": null,
      "use_case": "Deploy trained medical imaging models for clinical decision support"
    },
    {
      "id": 442,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["monai", "bundle", "pretrained", "deployment"],
      "task": "Deploy MONAI Bundle pre-trained model for automated organ segmentation",
      "solution": "docker run --gpus all projectmonai/monai:latest python -m monai.bundle run --config_file spleen_ct_segmentation",
      "explanation": "docker run --gpus all (GPU acceleration) projectmonai/monai:latest python -m monai.bundle run (execute MONAI Bundle) --config_file spleen_ct_segmentation (pre-trained spleen segmentation model - downloads data and model automatically)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "nvidia-docker", "gpu"],
      "warnings": null,
      "use_case": "Production-ready organ segmentation using validated pre-trained models"
    },
    {
      "id": 443,
      "command": ["docker", "run"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "metadata", "validation", "pydicom"],
      "task": "Validate and extract metadata from DICOM files using pydicom container",
      "solution": "docker run python:3.9 sh -c \"pip install pydicom && python -c 'import pydicom; from pydicom.dataset import Dataset; ds = Dataset(); ds.PatientName = \\\"Test^Patient\\\"; ds.PatientID = \\\"12345\\\"; ds.StudyDate = \\\"20240101\\\"; print(f\\\"Patient: {ds.PatientName}, ID: {ds.PatientID}, Date: {ds.StudyDate}\\\")'\"",
      "explanation": "docker run python:3.9 sh -c (shell command) pip install pydicom (install library) && python -c (create sample DICOM dataset using pydicom Dataset class and extract patient metadata including name, ID, and study date)",
      "execution_time": "< 1 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Quality control and metadata extraction from DICOM archives"
    },
    {
      "id": 444,
      "command": ["docker", "run"],
      "difficulty": 5,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["monai", "federated-learning", "privacy", "distributed"],
      "task": "Set up MONAI federated learning client for privacy-preserving medical AI training",
      "solution": "docker run -d --gpus all -p 8002:8002 projectmonai/monai:latest python -c \"print('MONAI FL client simulation ready - requires actual FL server setup'); from monai.fl.client import MonaiAlgo; algo = MonaiAlgo(); print('FL algorithm initialized')\"",
      "explanation": "docker run -d --gpus all (detached GPU client) -p 8002:8002 (federated learning port) projectmonai/monai:latest python -c (simulate federated learning client setup using MONAI's FL algorithm without requiring actual multi-institutional data)",
      "execution_time": "long-running",
      "requirements": ["docker", "nvidia-docker", "gpu", "fl-config"],
      "warnings": "Requires secure network configuration for federated learning",
      "use_case": "Train medical AI models across institutions without sharing sensitive data"
    },
    {
      "id": 445,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["ants", "registration", "normalization", "brain"],
      "task": "Run ANTs container for brain image registration and normalization",
      "solution": "docker run antsx/ants:latest sh -c \"python -c 'import ants; import numpy as np; fixed = ants.from_numpy(np.random.rand(64,64,64)); moving = ants.from_numpy(np.random.rand(64,64,64)); reg = ants.registration(fixed, moving); print(\\\"Brain registration simulation completed\\\")' \"",
      "explanation": "docker run antsx/ants:latest (ANTs image processing toolkit) sh -c (generate synthetic 3D brain-like volumes using numpy random data and demonstrate ANTs registration capabilities with simulated fixed and moving images)",
      "execution_time": "5+ min",
      "requirements": ["docker"],
      "warnings": "CPU-intensive process, may take significant time for high-resolution images",
      "use_case": "Standardize brain images to common template space for group analysis"
    },
    {
      "id": 446,
      "command": ["docker", "run"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["plastimatch", "radiotherapy", "dose", "planning"],
      "task": "Run Plastimatch container for radiotherapy dose calculation and planning",
      "solution": "docker run plastimatch/plastimatch sh -c \"plastimatch synth-vf --output test_dose.mha --dim '64 64 64' --spacing '1.0 1.0 1.0' && plastimatch dose --input test_dose.mha --output dose_calc.nii.gz\"",
      "explanation": "docker run plastimatch/plastimatch (medical physics toolkit) sh -c (generate synthetic vector field using plastimatch synth-vf with 64x64x64 dimensions and 1mm spacing, then demonstrate dose calculation workflow)",
      "execution_time": "1-5 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Calculate and visualize radiation dose distributions for treatment planning"
    },
    {
      "id": 447,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["fastai", "pathology", "whole-slide", "tiles"],
      "task": "Process whole slide pathology images with FastAI using tile-based approach",
      "solution": "docker run --gpus all -v $(pwd)/wsi:/input -v $(pwd)/tiles:/output -v $(pwd)/models:/models fastdotai/fastai:latest python tile_processor.py --wsi /input/slide.svs --tile_size 512 --model /models/pathology_classifier.pkl",
      "explanation": "docker run --gpus all (GPU processing) -v $(pwd)/wsi:/input (whole slide images) -v $(pwd)/tiles:/output (processed tiles) -v $(pwd)/models:/models (pathology models) fastdotai/fastai:latest python tile_processor.py (custom script for tile extraction and classification)",
      "execution_time": "5+ min",
      "requirements": ["docker", "nvidia-docker", "gpu", "tile-processor"],
      "warnings": "Memory-intensive for large whole slide images",
      "use_case": "Automated pathology analysis for cancer detection and grading"
    },
    {
      "id": 448,
      "command": ["docker", "run"],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["vtk", "mesh", "reconstruction", "3d"],
      "task": "Run VTK container for 3D surface mesh reconstruction from medical images",
      "solution": "docker run -v $(pwd)/segmentations:/input -v $(pwd)/meshes:/output kitware/vtk:latest python -c \"import vtk; reader=vtk.vtkNIFTIImageReader(); reader.SetFileName('/input/organ.nii'); marching=vtk.vtkMarchingCubes(); marching.SetInputConnection(reader.GetOutputPort())\"",
      "explanation": "docker run -v $(pwd)/segmentations:/input -v $(pwd)/meshes:/output (mount directories) kitware/vtk:latest (VTK toolkit) python -c (execute VTK script for marching cubes algorithm to create 3D mesh from segmented volume)",
      "execution_time": "1-5 min",
      "requirements": ["docker"],
      "warnings": null,
      "use_case": "Create 3D printable models or surgical planning visualizations from medical scans"
    },
    {
      "id": 449,
      "command": ["docker", "run"],
      "difficulty": 5,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["monai", "automl", "nas", "optimization"],
      "task": "Run MONAI AutoML for neural architecture search in medical imaging",
      "solution": "docker run --gpus all -v $(pwd)/training_data:/workspace/data -v $(pwd)/automl_results:/workspace/results projectmonai/monai:latest python -m monai.networks.nets.automl --data_dir /workspace/data --output_dir /workspace/results --search_space unet_variants",
      "explanation": "docker run --gpus all (GPU acceleration for NAS) -v $(pwd)/training_data:/workspace/data -v $(pwd)/automl_results:/workspace/results (mount directories) projectmonai/monai:latest python -m monai.networks.nets.automl (neural architecture search module with U-Net variants)",
      "execution_time": "5+ min",
      "requirements": ["docker", "nvidia-docker", "gpu"],
      "warnings": "Computationally intensive, requires substantial GPU resources and time",
      "use_case": "Automatically discover optimal network architectures for specific medical imaging tasks"
    },
    {
      "id": 450,
      "command": ["docker", "compose"],
      "difficulty": 5,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["orthanc", "pacs", "dicom-server", "workflow"],
      "task": "Deploy complete DICOM PACS system with Orthanc, PostgreSQL, and web viewer",
      "solution": "docker compose -f orthanc-stack.yml up -d && docker compose exec orthanc Orthanc --help",
      "explanation": "docker compose -f orthanc-stack.yml up -d (deploy multi-container PACS stack from compose file) && docker compose exec orthanc Orthanc --help (verify Orthanc DICOM server is running with PostgreSQL database and web interface)",
      "execution_time": "1-5 min",
      "requirements": ["docker", "docker-compose", "orthanc-compose-file"],
      "warnings": null,
      "use_case": "Set up complete medical imaging infrastructure for hospital or research facility"
    }
  ]
}