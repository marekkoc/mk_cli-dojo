{
  "tasks": [
    {
      "id": 41,
      "command": ["jq"],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": ["json", "parsing", "extraction"],
      "task": "Extract all email addresses from a JSON file containing user data",
      "solution": "jq -r '.users[].email' data.json",
      "explanation": "jq -r (raw output without quotes) '.users[]' (iterate through all items in users array) '.email' (extract email field from each user object). The [] operator flattens the array to process each element individually.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Extract contact information from API responses or user databases"
    },
    {
      "id": 42,
      "command": ["jq"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["json", "filtering", "conditional"],
      "task": "Filter JSON array to show only items where price is greater than 100",
      "solution": "jq '.products[] | select(.price > 100)' catalog.json",
      "explanation": "jq '.products[]' (iterate through products array) | select(.price > 100) (filter items where price field is greater than 100). The select() function acts as a conditional filter, only passing through objects that meet the criteria.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Filter product catalogs or datasets based on numeric criteria"
    },
    {
      "id": 43,
      "command": ["jq"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "groupby", "statistics"],
      "task": "Group JSON data by category and calculate average price for each group",
      "solution": "jq 'group_by(.category) | map({category: .[0].category, avg_price: (map(.price) | add / length)})' products.json",
      "explanation": "jq 'group_by(.category)' (group array items by category field) | map({...}) (transform each group into new object) .[0].category (get category from first item in group) map(.price) | add / length (extract prices, sum them, divide by count for average). Complex aggregation with grouping.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Analyze e-commerce data or create summary statistics from JSON datasets"
    },
    {
      "id": 44,
      "command": ["jq"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["json", "nested", "flattening"],
      "task": "Flatten nested JSON structure and extract specific nested fields",
      "solution": "jq '.orders[] | {order_id: .id, customer: .customer.name, total: .billing.total}' orders.json",
      "explanation": "jq '.orders[]' (iterate through orders array) | {...} (create new object with specified fields) .customer.name (access nested field using dot notation) .billing.total (access deeply nested field). This flattens complex nested structures into simpler objects.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Simplify complex JSON structures for analysis or reporting"
    },
    {
      "id": 45,
      "command": ["jq", "curl"],
      "difficulty": 4,
      "rating": 4,
      "category": "web",
      "tags": ["api", "json", "monitoring"],
      "task": "Monitor API endpoint and extract error responses in real-time",
      "solution": "while true; do curl -s https://api.example.com/status | jq 'select(.status != \"ok\") | {time: now, error: .error}'; sleep 10; done",
      "explanation": "while true; do ... done (infinite loop) curl -s (silent mode) | jq 'select(.status != \"ok\")' (filter only non-ok responses) | {...} (create object with current timestamp using now and error field) sleep 10 (wait 10 seconds between checks). Continuous API monitoring with error detection.",
      "execution_time": "long-running",
      "requirements": ["jq", "curl"],
      "warnings": null,
      "use_case": "API monitoring and alerting for production systems"
    },
    {
      "id": 46,
      "command": ["rsync"],
      "difficulty": 2,
      "rating": 3,
      "category": "backup",
      "tags": ["sync", "backup", "incremental"],
      "task": "Synchronize local directory with remote server, showing progress",
      "solution": "rsync -avz --progress /local/path/ user@server:/remote/path/",
      "explanation": "rsync -a (archive mode: recursive, preserve permissions, times, etc.) -v (verbose output) -z (compress during transfer) --progress (show transfer progress) /local/path/ (source with trailing slash for contents) user@server:/remote/path/ (SSH destination). Efficient incremental sync.",
      "execution_time": "5+ min",
      "requirements": ["rsync", "ssh"],
      "warnings": null,
      "use_case": "Regular backup of project files or data to remote server"
    },
    {
      "id": 47,
      "command": ["rsync"],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": ["sync", "exclude", "selective"],
      "task": "Sync directory while excluding temporary files and maintaining deletions",
      "solution": "rsync -avz --delete --exclude='*.tmp' --exclude='*.log' --exclude='.git/' src/ dest/",
      "explanation": "rsync -avz (archive, verbose, compress) --delete (remove files from destination that don't exist in source) --exclude='pattern' (exclude files matching pattern, multiple excludes possible) src/ dest/ (sync source contents to destination). Selective sync with cleanup.",
      "execution_time": "1-5 min",
      "requirements": ["rsync"],
      "warnings": "Use --delete carefully as it removes files from destination",
      "use_case": "Clean synchronization of code projects excluding temporary files"
    },
    {
      "id": 48,
      "command": ["rsync"],
      "difficulty": 4,
      "rating": 5,
      "category": "backup",
      "tags": ["sync", "bandwidth", "resume"],
      "task": "Resume interrupted large file transfer with bandwidth limiting",
      "solution": "rsync -avz --partial --progress --bwlimit=1000 large_dataset/ user@server:/backup/",
      "explanation": "rsync -avz (standard options) --partial (keep partially transferred files for resuming) --progress (show detailed progress) --bwlimit=1000 (limit bandwidth to 1000 KB/s) large_dataset/ (source) user@server:/backup/ (remote destination). Resumable transfers with traffic control.",
      "execution_time": "long-running",
      "requirements": ["rsync", "ssh"],
      "warnings": null,
      "use_case": "Transfer large datasets over limited bandwidth connections"
    },
    {
      "id": 49,
      "command": ["tar"],
      "difficulty": 2,
      "rating": 2,
      "category": "backup",
      "tags": ["archive", "compression", "backup"],
      "task": "Create compressed archive of directory with current date in filename",
      "solution": "tar -czf backup_$(date +%Y%m%d).tar.gz /path/to/directory",
      "explanation": "tar -c (create archive) -z (gzip compression) -f (specify filename) backup_$(date +%Y%m%d).tar.gz (filename with command substitution for date in YYYYMMDD format) /path/to/directory (directory to archive). Creates timestamped backups.",
      "execution_time": "1-5 min",
      "requirements": ["tar"],
      "warnings": null,
      "use_case": "Automated daily backups with timestamped archive names"
    },
    {
      "id": 50,
      "command": ["tar"],
      "difficulty": 3,
      "rating": 3,
      "category": "backup",
      "tags": ["archive", "exclude", "selective"],
      "task": "Create archive excluding specific file patterns and showing progress",
      "solution": "tar -czf archive.tar.gz --exclude='*.log' --exclude='node_modules' --verbose project/",
      "explanation": "tar -czf archive.tar.gz (create compressed archive) --exclude='pattern' (exclude files matching glob pattern, can use multiple times) --verbose (show files being processed) project/ (source directory). Selective archiving with visual feedback.",
      "execution_time": "1-5 min",
      "requirements": ["tar"],
      "warnings": null,
      "use_case": "Archive projects while excluding unnecessary files like logs and dependencies"
    },
    {
      "id": 51,
      "command": ["tar", "find"],
      "difficulty": 4,
      "rating": 4,
      "category": "backup",
      "tags": ["archive", "incremental", "modified"],
      "task": "Create incremental backup of files modified in last 7 days",
      "solution": "find /path/to/backup -type f -mtime -7 | tar -czf incremental_$(date +%Y%m%d).tar.gz -T -",
      "explanation": "find /path/to/backup -type f -mtime -7 (find files modified within 7 days) | tar -czf incremental_$(date +%Y%m%d).tar.gz (create timestamped compressed archive) -T - (read file list from stdin using dash). Efficient incremental backups using file modification times.",
      "execution_time": "1-5 min",
      "requirements": ["tar", "find"],
      "warnings": null,
      "use_case": "Efficient incremental backups focusing only on recently changed files"
    },
    {
      "id": 52,
      "command": ["curl"],
      "difficulty": 2,
      "rating": 3,
      "category": "web",
      "tags": ["api", "json", "authentication"],
      "task": "Make authenticated API request and save response to file",
      "solution": "curl -H \"Authorization: Bearer $TOKEN\" -H \"Content-Type: application/json\" https://api.example.com/data -o response.json",
      "explanation": "curl -H 'Authorization: Bearer $TOKEN' (add auth header with environment variable) -H 'Content-Type: application/json' (specify content type header) https://api.example.com/data (API endpoint) -o response.json (save output to file). Standard authenticated API request pattern.",
      "execution_time": "< 1 min",
      "requirements": ["curl"],
      "warnings": null,
      "use_case": "Fetch data from authenticated APIs for analysis or integration"
    },
    {
      "id": 53,
      "command": ["curl", "jq"],
      "difficulty": 3,
      "rating": 4,
      "category": "web",
      "tags": ["api", "monitoring", "health"],
      "task": "Check API health status and alert if service is down",
      "solution": "response=$(curl -s -w \"%{http_code}\" https://api.example.com/health); if [[ $response != *\"200\" ]]; then echo \"API DOWN: $response\"; fi",
      "explanation": "response=$(curl -s -w '%{http_code}' url) (capture response with HTTP status code, -s for silent, -w for write-out format) if [[ $response != *'200' ]] (check if response doesn't contain '200') then echo 'API DOWN: $response' (alert with status). Health check with conditional alerting.",
      "execution_time": "< 1 min",
      "requirements": ["curl"],
      "warnings": null,
      "use_case": "Automated health checks for web services and APIs"
    },
    {
      "id": 54,
      "command": ["curl"],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": ["download", "parallel", "batch"],
      "task": "Download multiple files in parallel with retry logic",
      "solution": "cat urls.txt | xargs -n 1 -P 5 -I {} sh -c 'curl -L --retry 3 --retry-delay 2 -O {}'",
      "explanation": "cat urls.txt (read URL list) | xargs -n 1 (one URL per command) -P 5 (run 5 processes in parallel) -I {} (placeholder for URL) sh -c 'curl -L (follow redirects) --retry 3 (retry 3 times on failure) --retry-delay 2 (wait 2 seconds between retries) -O {} (download with original filename)'. Robust parallel downloading.",
      "execution_time": "5+ min",
      "requirements": ["curl", "xargs"],
      "warnings": null,
      "use_case": "Batch download of datasets or media files with fault tolerance"
    },
    {
      "id": 55,
      "command": ["parallel"],
      "difficulty": 3,
      "rating": 4,
      "category": "automation",
      "tags": ["parallel", "processing", "efficiency"],
      "task": "Process multiple files in parallel using all CPU cores",
      "solution": "parallel -j+0 'process_file {}' ::: *.txt",
      "explanation": "parallel -j+0 (use all available CPU cores, +0 means number of cores) 'process_file {}' (command template with {} placeholder for filename) ::: *.txt (input sources - all .txt files). GNU parallel automatically distributes work across cores for maximum efficiency.",
      "execution_time": "1-5 min",
      "requirements": ["parallel"],
      "warnings": null,
      "use_case": "Speed up data processing by utilizing all available CPU cores"
    },
    {
      "id": 56,
      "command": ["parallel", "convert"],
      "difficulty": 4,
      "rating": 5,
      "category": "multimedia",
      "tags": ["parallel", "images", "batch"],
      "task": "Resize all images in directory using parallel processing",
      "solution": "parallel -j+0 'convert {} -resize 800x600 resized_{}' ::: *.jpg",
      "explanation": "parallel -j+0 (all CPU cores) 'convert {} -resize 800x600 resized_{}' (ImageMagick convert command: resize to 800x600 pixels, prefix output with 'resized_') ::: *.jpg (process all JPEG files). Parallel image processing for significant speed improvement over sequential processing.",
      "execution_time": "5+ min",
      "requirements": ["parallel", "imagemagick"],
      "warnings": null,
      "use_case": "Batch image processing for web optimization or dataset preparation"
    },
    {
      "id": 57,
      "command": ["parallel", "gzip"],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": ["parallel", "compression", "logs"],
      "task": "Compress log files in parallel while preserving directory structure",
      "solution": "find /var/log -name '*.log' -print0 | parallel -0 -j+0 'gzip {}'",
      "explanation": "find /var/log -name '*.log' -print0 (find log files, output null-separated for filenames with spaces) | parallel -0 (read null-separated input) -j+0 (all cores) 'gzip {}' (compress each file in place). Parallel compression maintains directory structure and handles special characters in filenames.",
      "execution_time": "1-5 min",
      "requirements": ["parallel", "gzip"],
      "warnings": "Ensure log files are not actively being written to",
      "use_case": "Efficient log rotation and compression for system maintenance"
    },
    {
      "id": 58,
      "command": ["watch"],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": ["monitoring", "real-time", "disk"],
      "task": "Monitor disk usage in real-time with color highlighting",
      "solution": "watch -c -n 2 'df -h | grep -E \"(Filesystem|/dev/)\"'",
      "explanation": "watch -c (enable color output) -n 2 (refresh every 2 seconds) 'df -h (human-readable disk usage) | grep -E '(Filesystem|/dev/)' (show header and only real filesystems, exclude tmpfs/proc)'. Real-time monitoring with filtered output and color highlighting for easy reading.",
      "execution_time": "long-running",
      "requirements": ["watch"],
      "warnings": null,
      "use_case": "Real-time monitoring of disk space during large file operations"
    },
    {
      "id": 59,
      "command": ["watch", "ps"],
      "difficulty": 3,
      "rating": 3,
      "category": "monitoring",
      "tags": ["monitoring", "processes", "memory"],
      "task": "Monitor memory usage of specific process in real-time",
      "solution": "watch -n 1 'ps aux | grep python | grep -v grep | sort -k4 -nr'",
      "explanation": "watch -n 1 (refresh every second) 'ps aux (all processes with detailed info) | grep python (filter Python processes) | grep -v grep (exclude grep itself) | sort -k4 -nr (sort by 4th column - memory usage, numeric reverse)'. Real-time process monitoring with automatic filtering and sorting.",
      "execution_time": "long-running",
      "requirements": ["watch", "ps"],
      "warnings": null,
      "use_case": "Track memory consumption of running applications for performance tuning"
    },
    {
      "id": 60,
      "command": ["git", "grep"],
      "difficulty": 3,
      "rating": 4,
      "category": "version control",
      "tags": ["git", "search", "history"],
      "task": "Search for specific text pattern across entire Git history",
      "solution": "git log -S \"search_pattern\" --source --all --oneline",
      "explanation": "git log -S 'search_pattern' (pickaxe search - find commits that add or remove the specified string) --source (show which ref/branch) --all (search all branches and tags) --oneline (compact output format). Searches the actual content changes, not just commit messages.",
      "execution_time": "< 1 min",
      "requirements": ["git"],
      "warnings": null,
      "use_case": "Track when specific code patterns or functions were introduced or removed"
    },
    {
      "id": 61,
      "command": ["git", "find"],
      "difficulty": 4,
      "rating": 5,
      "category": "version control",
      "tags": ["git", "cleanup", "large files"],
      "task": "Find and remove large files from Git history to reduce repository size",
      "solution": "git rev-list --objects --all | git cat-file --batch-check='%(objecttype) %(objectname) %(objectsize) %(rest)' | grep '^blob' | sort -k3 -nr | head -10",
      "explanation": "git rev-list --objects --all (list all objects in repository) | git cat-file --batch-check='...' (get object info: type, hash, size, name) | grep '^blob' (filter to files only) | sort -k3 -nr (sort by size column, descending) | head -10 (show top 10 largest). Identifies large files for repository cleanup.",
      "execution_time": "1-5 min",
      "requirements": ["git"],
      "warnings": "Rewriting Git history affects all collaborators",
      "use_case": "Repository maintenance and size optimization for faster clones"
    },
    {
      "id": 62,
      "command": ["git", "awk"],
      "difficulty": 4,
      "rating": 4,
      "category": "version control",
      "tags": ["git", "statistics", "contributors"],
      "task": "Generate contributor statistics showing lines added/removed per author",
      "solution": "git log --numstat --pretty=\"%ae\" | awk '/^[0-9]/ {add+=$1; del+=$2} /^[a-zA-Z]/ {print; if(add+del>0) print \"  +\" add \" -\" del; add=del=0}'",
      "explanation": "git log --numstat (show added/deleted lines per file) --pretty='%ae' (show author email) | awk with pattern-action: '/^[0-9]/' (lines starting with numbers - file stats) accumulate additions/deletions '/^[a-zA-Z]/' (author email lines) print stats and reset counters. Generates detailed contributor statistics.",
      "execution_time": "< 1 min",
      "requirements": ["git", "awk"],
      "warnings": null,
      "use_case": "Project analysis and contributor recognition for team management"
    },
    {
      "id": 63,
      "command": ["ssh"],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": ["ssh", "tunneling", "port forwarding"],
      "task": "Create SSH tunnel to access remote database through bastion host",
      "solution": "ssh -L 5432:database.internal:5432 user@bastion.example.com",
      "explanation": "ssh -L 5432:database.internal:5432 (local port forwarding: bind local port 5432 to database.internal:5432 through SSH connection) user@bastion.example.com (SSH to bastion host). Creates secure tunnel allowing local database connections through jump server.",
      "execution_time": "long-running",
      "requirements": ["ssh"],
      "warnings": null,
      "use_case": "Secure access to internal databases through jump servers"
    },
    {
      "id": 64,
      "command": ["ssh", "find"],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": ["ssh", "remote", "cleanup"],
      "task": "Execute complex command on multiple remote servers simultaneously",
      "solution": "for server in server1 server2 server3; do ssh $server 'find /tmp -mtime +7 -delete' & done; wait",
      "explanation": "for server in server1 server2 server3 (iterate through server list) do ssh $server 'command' & (execute SSH command in background with &) done; wait (wait for all background jobs to complete). Parallel execution across multiple servers with synchronization.",
      "execution_time": "1-5 min",
      "requirements": ["ssh"],
      "warnings": "Ensure SSH keys are properly configured for passwordless access",
      "use_case": "Automated maintenance tasks across multiple servers"
    },
    {
      "id": 65,
      "command": ["cron", "find"],
      "difficulty": 3,
      "rating": 3,
      "category": "automation",
      "tags": ["cron", "cleanup", "scheduling"],
      "task": "Schedule automated cleanup of old log files every night at 2 AM",
      "solution": "echo '0 2 * * * find /var/log -name \"*.log\" -mtime +30 -delete' | crontab -",
      "explanation": "echo '0 2 * * * command' (cron format: minute hour day month weekday, 0 2 * * * = 2:00 AM daily) find /var/log -name '*.log' -mtime +30 -delete (find log files older than 30 days and delete) | crontab - (install crontab from stdin). Automated log cleanup scheduling.",
      "execution_time": "< 1 min",
      "requirements": ["cron"],
      "warnings": "Test commands manually before scheduling",
      "use_case": "Automated system maintenance and log rotation"
    },
    {
      "id": 66,
      "command": ["cron", "rsync"],
      "difficulty": 4,
      "rating": 4,
      "category": "automation",
      "tags": ["cron", "backup", "scheduling"],
      "task": "Schedule daily incremental backups with email notifications",
      "solution": "echo '0 3 * * * rsync -avz /data/ backup@server:/backups/ && echo \"Backup completed\" | mail -s \"Daily Backup Status\" admin@company.com' | crontab -",
      "explanation": "echo '0 3 * * * ...' (3:00 AM daily) rsync -avz /data/ backup@server:/backups/ (sync data to backup server) && (if successful) echo 'Backup completed' | mail -s 'Daily Backup Status' admin@company.com (send success notification) | crontab - (install cron job). Automated backup with notification.",
      "execution_time": "< 1 min",
      "requirements": ["cron", "rsync", "mail"],
      "warnings": null,
      "use_case": "Automated data backup with monitoring and alerting"
    },
    {
      "id": 67,
      "command": ["diff"],
      "difficulty": 2,
      "rating": 2,
      "category": "text processing",
      "tags": ["diff", "comparison", "changes"],
      "task": "Compare two configuration files and show only the differences",
      "solution": "diff -u config.old config.new",
      "explanation": "diff -u (unified format showing context around changes with + and - prefixes) config.old config.new (compare old and new versions). Unified format is more readable than default format and shows context lines around changes for better understanding.",
      "execution_time": "< 1 min",
      "requirements": ["diff"],
      "warnings": null,
      "use_case": "Configuration management and change tracking"
    },
    {
      "id": 68,
      "command": ["diff", "find"],
      "difficulty": 4,
      "rating": 5,
      "category": "file management",
      "tags": ["diff", "directory", "sync"],
      "task": "Find files that exist in one directory but not another",
      "solution": "diff <(find dir1 -type f | sort) <(find dir2 -type f | sort)",
      "explanation": "diff <(find dir1 -type f | sort) <(find dir2 -type f | sort) uses process substitution <() to create temporary files from command output. find dir1 -type f | sort (get sorted list of files in dir1) compared with same for dir2. Shows files unique to each directory.",
      "execution_time": "< 1 min",
      "requirements": ["diff", "find"],
      "warnings": null,
      "use_case": "Directory synchronization verification and missing file detection"
    },
    {
      "id": 69,
      "command": ["imagemagick"],
      "difficulty": 3,
      "rating": 4,
      "category": "multimedia",
      "tags": ["images", "batch", "conversion"],
      "task": "Batch convert images to different format with quality optimization",
      "solution": "for img in *.jpg; do convert \"$img\" -quality 85 -resize 1920x1080> \"${img%.jpg}.webp\"; done",
      "explanation": "for img in *.jpg (iterate through JPEG files) convert '$img' (ImageMagick convert tool) -quality 85 (85% quality for good compression/quality balance) -resize 1920x1080> (resize only if larger, > prevents upscaling) '${img%.jpg}.webp' (remove .jpg extension, add .webp). Efficient batch conversion to modern format.",
      "execution_time": "5+ min",
      "requirements": ["imagemagick"],
      "warnings": null,
      "use_case": "Web optimization by converting images to modern formats"
    },
    {
      "id": 70,
      "command": ["imagemagick", "find"],
      "difficulty": 4,
      "rating": 4,
      "category": "multimedia",
      "tags": ["images", "metadata", "analysis"],
      "task": "Extract and analyze image metadata from photo collection",
      "solution": "find . -name \"*.jpg\" -exec identify -verbose {} \\; | grep -E \"(Geometry|Filesize|Date)\"",
      "explanation": "find . -name '*.jpg' -exec identify -verbose {} \\; (run ImageMagick identify with verbose output on each JPEG file) | grep -E '(Geometry|Filesize|Date)' (filter for specific metadata: dimensions, file size, and date information using extended regex with alternation). Extracts key image properties.",
      "execution_time": "1-5 min",
      "requirements": ["imagemagick"],
      "warnings": null,
      "use_case": "Photo collection analysis and metadata extraction for organization"
    },
    {
      "id": 71,
      "command": ["ffmpeg"],
      "difficulty": 3,
      "rating": 4,
      "category": "multimedia",
      "tags": ["video", "conversion", "compression"],
      "task": "Batch convert video files to web-optimized format",
      "solution": "for video in *.mp4; do ffmpeg -i \"$video\" -c:v libx264 -crf 23 -c:a aac \"web_${video}\"; done",
      "explanation": "for video in *.mp4 (iterate through MP4 files) ffmpeg -i '$video' (input file) -c:v libx264 (video codec: H.264) -crf 23 (constant rate factor: 23 is good quality/size balance) -c:a aac (audio codec: AAC) 'web_${video}' (output with 'web_' prefix). Optimizes videos for web streaming.",
      "execution_time": "long-running",
      "requirements": ["ffmpeg"],
      "warnings": "Video processing is CPU intensive and time consuming",
      "use_case": "Prepare video content for web streaming with optimal file sizes"
    },
    {
      "id": 72,
      "command": ["ffmpeg", "find"],
      "difficulty": 4,
      "rating": 5,
      "category": "multimedia",
      "tags": ["video", "analysis", "duration"],
      "task": "Calculate total duration of all video files in directory tree",
      "solution": "find . -name \"*.mp4\" -exec ffprobe -v quiet -show_entries format=duration -of csv=p=0 {} \\; | awk '{sum+=$1} END {printf \"Total: %.2f hours\\n\", sum/3600}'",
      "explanation": "find . -name '*.mp4' -exec ffprobe (FFmpeg probe tool) -v quiet (suppress verbose output) -show_entries format=duration (show only duration) -of csv=p=0 (output as CSV without headers) {} \\; | awk '{sum+=$1}' (accumulate durations) END {printf ...} (convert seconds to hours). Analyzes media collection duration.",
      "execution_time": "1-5 min",
      "requirements": ["ffmpeg", "awk"],
      "warnings": null,
      "use_case": "Media collection analysis and storage planning"
    },
    {
      "id": 73,
      "command": ["ps", "grep", "awk"],
      "difficulty": 3,
      "rating": 3,
      "category": "monitoring",
      "tags": ["processes", "memory", "monitoring"],
      "task": "Monitor processes consuming more than 10% CPU and log them",
      "solution": "ps aux | awk '$3 > 10 {print strftime(\"%Y-%m-%d %H:%M:%S\"), $2, $3\"%\", $11}' >> high_cpu.log",
      "explanation": "ps aux (show all processes with detailed info) | awk '$3 > 10' (filter processes where 3rd column - CPU% > 10) '{print strftime('%Y-%m-%d %H:%M:%S') (format current timestamp) $2 (PID) $3'%' (CPU percentage) $11 (command)} >> high_cpu.log (append to log file). Monitors and logs high CPU usage with timestamps.",
      "execution_time": "< 1 min",
      "requirements": ["ps", "awk"],
      "warnings": null,
      "use_case": "Performance monitoring and troubleshooting high CPU usage"
    },
    {
      "id": 74,
      "command": ["netstat", "awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "network",
      "tags": ["network", "connections", "analysis"],
      "task": "Analyze network connections and show top connecting IP addresses",
      "solution": "netstat -tn | awk '/^tcp/ {print $5}' | cut -d: -f1 | sort | uniq -c | sort -nr | head -10",
      "explanation": "netstat -tn (show TCP connections, numeric addresses) | awk '/^tcp/' (filter TCP connections) '{print $5}' (extract remote address column) | cut -d: -f1 (extract IP part before port) | sort | uniq -c (count occurrences) | sort -nr (sort by count, descending) | head -10 (top 10). Network connection analysis.",
      "execution_time": "< 1 min",
      "requirements": ["netstat", "awk"],
      "warnings": null,
      "use_case": "Network security analysis and identifying connection patterns"
    },
    {
      "id": 75,
      "command": ["date", "awk"],
      "difficulty": 4,
      "rating": 4,
      "category": "automation",
      "tags": ["date", "calculation", "scheduling"],
      "task": "Calculate business days between two dates excluding weekends",
      "solution": "start=$(date -d \"2024-01-01\" +%s); end=$(date -d \"2024-12-31\" +%s); echo \"scale=0; (($end - $start) / 86400) * 5/7\" | bc",
      "explanation": "start=$(date -d '2024-01-01' +%s) (convert start date to Unix timestamp) end=$(date -d '2024-12-31' +%s) (convert end date to timestamp) echo 'scale=0; (($end - $start) / 86400) * 5/7' | bc (calculate: difference in seconds / 86400 seconds per day * 5/7 for weekdays only, using bc calculator with scale=0 for integer result). Business day calculation.",
      "execution_time": "< 1 min",
      "requirements": ["date", "bc"],
      "warnings": null,
      "use_case": "Project planning and deadline calculations for business scenarios"
    }
  ]
}