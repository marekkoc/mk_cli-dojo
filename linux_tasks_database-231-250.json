{
  "tasks": [
    {
      "id": 231,
      "command": ["awk", "grep"],
      "difficulty": 2,
      "rating": 3,
      "category": "log analysis",
      "tags": ["log parsing", "error extraction", "filtering"],
      "task": "Extract all error messages from Apache log with timestamps",
      "solution": "awk '$9 >= 400 {print $4, $5, $7, $9, $10}' access.log | grep -v \"favicon\\|robots.txt\"",
      "explanation": "awk '$9 >= 400' (filter HTTP status codes 400 and above - client and server errors) {print $4, $5, $7, $9, $10} (print timestamp, request path, status code, response size) | grep -v \"favicon\\|robots.txt\" (exclude common non-critical requests). Extracts meaningful error entries from Apache access logs.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Web server troubleshooting and error pattern identification"
    },
    {
      "id": 232,
      "command": ["awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "log analysis",
      "tags": ["log statistics", "ip analysis", "frequency"],
      "task": "Analyze top IP addresses by request count with bandwidth usage",
      "solution": "awk '{ip[$1]++; bytes[$1]+=$10} END {for(i in ip) printf \"%-15s %8d requests %12d bytes\\n\", i, ip[i], bytes[i]}' access.log | sort -k2 -nr | head -10",
      "explanation": "awk '{ip[$1]++; bytes[$1]+=$10}' (count requests per IP and sum bandwidth usage) END {for(i in ip) printf...} (format output with IP, request count, total bytes) | sort -k2 -nr (sort by request count descending) | head -10 (top 10 IPs). Comprehensive IP analysis with traffic metrics.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "DDoS detection, bandwidth analysis, and suspicious activity identification"
    },
    {
      "id": 233,
      "command": ["awk", "date"],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": ["time-based analysis", "hourly stats", "traffic patterns"],
      "task": "Generate hourly traffic statistics from log files",
      "solution": "awk '{gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; traffic[hour]++; bytes[hour]+=$10} END {for(h=0; h<24; h++) printf \"%02d:00 - %8d requests, %12d bytes\\n\", h, traffic[h]+0, bytes[h]+0}' access.log",
      "explanation": "awk '{gsub(/\\[|\\]/, \"\", $4)}' (remove brackets from timestamp) split($4, dt, \":\"); hour=dt[2] (extract hour from timestamp) traffic[hour]++; bytes[hour]+=$10 (count requests and sum bytes per hour) END {for(h=0; h<24; h++) printf...} (output 24-hour statistics with zero-padding). Hourly traffic pattern analysis.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Traffic pattern analysis, capacity planning, and peak hour identification"
    },
    {
      "id": 234,
      "command": ["grep", "awk", "sort"],
      "difficulty": 3,
      "rating": 4,
      "category": "log analysis",
      "tags": ["security analysis", "attack detection", "suspicious patterns"],
      "task": "Detect potential security attacks in web server logs",
      "solution": "grep -E \"(sql|script|alert|union|select|drop|exec|javascript|<script)\" access.log | awk '{print $1, $7}' | sort | uniq -c | sort -nr | awk '$1 > 5 {printf \"SUSPICIOUS IP: %-15s Count: %d Path: %s\\n\", $2, $1, $3}'",
      "explanation": "grep -E \"(sql|script|alert|union|select|drop|exec|javascript|<script)\" (find common attack patterns: SQL injection, XSS, script injection) | awk '{print $1, $7}' (extract IP and request path) | sort | uniq -c | sort -nr (count occurrences, sort by frequency) | awk '$1 > 5' (filter IPs with more than 5 suspicious requests). Security threat detection pipeline.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Web application security monitoring and attack pattern detection"
    },
    {
      "id": 235,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 5,
      "category": "log analysis",
      "tags": ["response time analysis", "performance metrics", "percentiles"],
      "task": "Calculate response time statistics and identify slow requests",
      "solution": "awk '$NF ~ /^[0-9]+$/ {times[++i]=$NF; sum+=$NF; if($NF>max) max=$NF; if(min==\"\" || $NF<min) min=$NF} END {asort(times); n=length(times); p95=times[int(n*0.95)]; p99=times[int(n*0.99)]; printf \"Requests: %d\\nAvg: %.2fms\\nMin: %dms\\nMax: %dms\\n95th: %dms\\n99th: %dms\\n\", n, sum/n, min, max, p95, p99}' response_time.log",
      "explanation": "awk '$NF ~ /^[0-9]+$/' (ensure last field is numeric response time) {times[++i]=$NF; sum+=$NF; ...} (collect response times, calculate sum, min, max) END {asort(times); n=length(times)} (sort times array) p95=times[int(n*0.95)]; p99=times[int(n*0.99)] (calculate 95th and 99th percentiles) printf... (output comprehensive statistics). Performance analysis with percentile calculations.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Application performance monitoring and SLA compliance verification"
    },
    {
      "id": 236,
      "command": ["awk", "date"],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": ["log rotation", "archive analysis", "time filtering"],
      "task": "Parse rotated logs and extract entries from specific date range",
      "solution": "for log in access.log access.log.1 access.log.2.gz; do if [[ $log == *.gz ]]; then zcat \"$log\"; else cat \"$log\"; fi; done | awk -v start=\"$(date -d '7 days ago' '+%d/%b/%Y')\" -v end=\"$(date '+%d/%b/%Y')\" '$4 ~ start || $4 ~ end {print}'",
      "explanation": "for log in access.log access.log.1 access.log.2.gz (iterate through current and rotated logs) do if [[ $log == *.gz ]]; then zcat \"$log\"; else cat \"$log\"; fi (handle compressed logs with zcat, regular logs with cat) done | awk -v start=\"$(date -d '7 days ago' '+%d/%b/%Y')\" -v end=\"$(date '+%d/%b/%Y')\" (pass date range as awk variables) '$4 ~ start || $4 ~ end' (filter entries matching date range). Multi-log analysis across rotation.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Historical log analysis across log rotation periods"
    },
    {
      "id": 237,
      "command": ["awk", "jq"],
      "difficulty": 5,
      "rating": 5,
      "category": "log analysis",
      "tags": ["json logs", "structured parsing", "application logs"],
      "task": "Parse JSON application logs and extract error details with context",
      "solution": "awk '/^{.*}$/ {print}' app.log | jq -r 'select(.level == \"ERROR\" or .level == \"FATAL\") | \"\\(.timestamp) [\\(.level)] \\(.message) - Context: \\(.context // \"none\")\"' | awk '{count++} END {print \"\\nTotal errors found:\", count+0}'",
      "explanation": "awk '/^{.*}$/ {print}' (extract lines that look like JSON objects) | jq -r 'select(.level == \"ERROR\" or .level == \"FATAL\")' (filter error and fatal level messages) | \"\\(.timestamp) [\\(.level)] \\(.message) - Context: \\(.context // \"none\")\" (format output with timestamp, level, message, and context with fallback) | awk '{count++} END {print \"\\nTotal errors found:\", count+0}' (count total errors). Structured JSON log analysis.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Modern application debugging with structured logging analysis"
    },
    {
      "id": 238,
      "command": ["awk", "gnuplot"],
      "difficulty": 5,
      "rating": 5,
      "category": "log visualization",
      "tags": ["log visualization", "graphing", "traffic trends"],
      "task": "Generate traffic visualization from log data",
      "solution": "awk '{gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; traffic[hour]++} END {for(h=0; h<24; h++) printf \"%02d %d\\n\", h, traffic[h]+0}' access.log > traffic_data.txt && gnuplot -e \"set terminal png; set output 'traffic.png'; set xlabel 'Hour'; set ylabel 'Requests'; plot 'traffic_data.txt' with lines title 'Hourly Traffic'\"",
      "explanation": "awk '{gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; traffic[hour]++}' (extract hour from log timestamp and count requests) END {for(h=0; h<24; h++) printf \"%02d %d\\n\", h, traffic[h]+0} (output hourly data) > traffic_data.txt && gnuplot -e \"set terminal png; set output 'traffic.png'; set xlabel 'Hour'; set ylabel 'Requests'; plot 'traffic_data.txt' with lines title 'Hourly Traffic'\" (generate PNG graph). Log data visualization pipeline.",
      "execution_time": "< 1 min",
      "requirements": ["gnuplot"],
      "warnings": null,
      "use_case": "Visual traffic analysis and presentation-ready charts for management"
    },
    {
      "id": 239,
      "command": ["awk", "geoip"],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": ["geolocation", "ip analysis", "geographic stats"],
      "task": "Analyze geographic distribution of web traffic",
      "solution": "awk '{print $1}' access.log | sort -u | while read ip; do country=$(geoiplookup \"$ip\" | awk -F': ' '/Country/ {print $2}' | cut -d',' -f1); echo \"$ip,$country\"; done | awk -F',' '{country[$2]++} END {for(c in country) printf \"%-20s %d\\n\", c, country[c]}' | sort -k2 -nr",
      "explanation": "awk '{print $1}' access.log | sort -u (extract unique IP addresses) | while read ip; do country=$(geoiplookup \"$ip\" | awk -F': ' '/Country/ {print $2}' | cut -d',' -f1) (lookup country for each IP) echo \"$ip,$country\" done | awk -F',' '{country[$2]++}' (count visits per country) END {for(c in country) printf...} | sort -k2 -nr (sort by visit count). Geographic traffic analysis with GeoIP lookup.",
      "execution_time": "1-5 min",
      "requirements": ["geoip"],
      "warnings": "Requires GeoIP database installation",
      "use_case": "International traffic analysis and geographic user distribution insights"
    },
    {
      "id": 240,
      "command": ["awk", "mail"],
      "difficulty": 4,
      "rating": 4,
      "category": "log monitoring",
      "tags": ["alerting", "threshold monitoring", "automated response"],
      "task": "Monitor error rate and send alerts when threshold exceeded",
      "solution": "awk 'BEGIN {total=0; errors=0} {total++; if($9 >= 500) errors++} END {rate=errors/total*100; if(rate > 5) {printf \"ALERT: Error rate %.2f%% (%d errors in %d requests)\\n\", rate, errors, total; system(\"echo \\\"Error rate alert: \" rate \"% at $(date)\\\" | mail -s \\\"High Error Rate\\\" admin@company.com\")}}' access.log",
      "explanation": "awk 'BEGIN {total=0; errors=0}' (initialize counters) {total++; if($9 >= 500) errors++} (count total requests and server errors 500+) END {rate=errors/total*100} (calculate error percentage) if(rate > 5) {...system(\"echo...| mail...\")} (if error rate > 5%, send email alert with details). Automated threshold-based alerting system.",
      "execution_time": "< 1 min",
      "requirements": ["mail"],
      "warnings": "Configure mail system for email delivery",
      "use_case": "Production monitoring with automated alerting for service quality degradation"
    },
    {
      "id": 241,
      "command": ["awk", "sed"],
      "difficulty": 3,
      "rating": 3,
      "category": "log processing",
      "tags": ["log cleaning", "anonymization", "privacy"],
      "task": "Anonymize log files by masking IP addresses and sensitive data",
      "solution": "awk '{gsub(/[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}/, \"XXX.XXX.XXX.XXX\", $1); gsub(/email=[^&]*/, \"email=***@***.***\"); print}' access.log | sed 's/password=[^&]*/password=***/g' > anonymized.log",
      "explanation": "awk '{gsub(/[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}/, \"XXX.XXX.XXX.XXX\", $1)}' (replace IP addresses with placeholder) gsub(/email=[^&]*/, \"email=***@***.***\") (mask email parameters) print | sed 's/password=[^&]*/password=***/g' (mask password parameters) > anonymized.log. Data privacy protection for log sharing.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "GDPR compliance and safe log sharing for analysis or debugging"
    },
    {
      "id": 242,
      "command": ["awk", "logrotate"],
      "difficulty": 4,
      "rating": 4,
      "category": "log management",
      "tags": ["log rotation", "compression", "retention"],
      "task": "Implement custom log rotation with size and age-based rules",
      "solution": "size=$(stat -c%s access.log); if [ \"$size\" -gt 104857600 ]; then timestamp=$(date +%Y%m%d_%H%M%S); mv access.log \"access.log.$timestamp\" && gzip \"access.log.$timestamp\" && touch access.log && find . -name \"access.log.*.gz\" -mtime +30 -delete; fi",
      "explanation": "size=$(stat -c%s access.log) (get file size in bytes) if [ \"$size\" -gt 104857600 ] (if size > 100MB) then timestamp=$(date +%Y%m%d_%H%M%S) (create timestamp) mv access.log \"access.log.$timestamp\" (rotate log with timestamp) && gzip \"access.log.$timestamp\" (compress rotated log) && touch access.log (create new log file) && find . -name \"access.log.*.gz\" -mtime +30 -delete (cleanup logs older than 30 days). Custom log rotation implementation.",
      "execution_time": "< 1 min",
      "requirements": ["gzip"],
      "warnings": "Ensure application can handle log file rotation",
      "use_case": "Custom log management for applications without built-in rotation"
    },
    {
      "id": 243,
      "command": ["awk", "tail"],
      "difficulty": 3,
      "rating": 4,
      "category": "log monitoring",
      "tags": ["real-time monitoring", "pattern detection", "live analysis"],
      "task": "Real-time monitoring of multiple log types with pattern detection",
      "solution": "tail -f error.log access.log debug.log | awk '/ERROR|CRITICAL/ {print \"[ERROR]\" strftime(\"%H:%M:%S\") \" \" $0} /404|500/ {print \"[HTTP]\" strftime(\"%H:%M:%S\") \" \" $0} /DEBUG/ && /SQL/ {print \"[SQL]\" strftime(\"%H:%M:%S\") \" \" $0}'",
      "explanation": "tail -f error.log access.log debug.log (follow multiple log files simultaneously) | awk '/ERROR|CRITICAL/ {print \"[ERROR]\" strftime(\"%H:%M:%S\") \" \" $0}' (detect and timestamp error messages) /404|500/ {print \"[HTTP]\" strftime(\"%H:%M:%S\") \" \" $0} (detect HTTP errors) /DEBUG/ && /SQL/ {print \"[SQL]\" strftime(\"%H:%M:%S\") \" \" $0} (detect SQL debug messages). Multi-log real-time pattern detection.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Press Ctrl+C to stop monitoring",
      "use_case": "Development environment monitoring and live debugging across multiple services"
    },
    {
      "id": 244,
      "command": ["awk", "sqlite3"],
      "difficulty": 5,
      "rating": 5,
      "category": "log analysis",
      "tags": ["database storage", "structured analysis", "sql queries"],
      "task": "Import log data into SQLite database for complex analysis",
      "solution": "awk -F' ' '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7); printf \"INSERT INTO logs VALUES(\\047%s\\047,\\047%s\\047,\\047%s\\047,%d,%d);\\n\", $1, $4, $7, $9, $10}' access.log | sqlite3 -init <(echo 'CREATE TABLE logs(ip TEXT, timestamp TEXT, path TEXT, status INT, bytes INT);') logs.db",
      "explanation": "awk -F' ' '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7)}' (clean timestamp and path fields) printf \"INSERT INTO logs VALUES(\\047%s\\047,\\047%s\\047,\\047%s\\047,%d,%d);\\n\", $1, $4, $7, $9, $10 (generate SQL INSERT statements) | sqlite3 -init <(echo 'CREATE TABLE logs...') logs.db (create database table and import data). Log data warehousing for complex SQL analysis.",
      "execution_time": "1-5 min",
      "requirements": ["sqlite3"],
      "warnings": null,
      "use_case": "Complex log analytics requiring SQL capabilities and data persistence"
    },
    {
      "id": 245,
      "command": ["awk", "curl"],
      "difficulty": 4,
      "rating": 4,
      "category": "log integration",
      "tags": ["api integration", "log forwarding", "external systems"],
      "task": "Forward critical log events to external monitoring system via API",
      "solution": "tail -f application.log | awk '/CRITICAL|FATAL/ {gsub(/\"/, \"\\\\\\\"\", $0); system(\"curl -X POST -H \\\"Content-Type: application/json\\\" -d \\\"{\\\\\\\"timestamp\\\\\\\":\\\\\\\"\" strftime(\"%Y-%m-%dT%H:%M:%S\") \"\\\\\\\",\\\\\\\"level\\\\\\\":\\\\\\\"CRITICAL\\\\\\\",\\\\\\\"message\\\\\\\":\\\\\\\"\" $0 \"\\\\\\\"}\\\" https://monitoring.example.com/api/alerts\")}",
      "explanation": "tail -f application.log (follow log file) | awk '/CRITICAL|FATAL/' (detect critical events) {gsub(/\"/, \"\\\\\\\"\", $0)} (escape quotes for JSON) system(\"curl -X POST -H \\\"Content-Type: application/json\\\" -d \\\"...\\\" https://monitoring.example.com/api/alerts\") (send JSON payload to monitoring API). Real-time log event forwarding to external systems.",
      "execution_time": "long-running",
      "requirements": ["curl"],
      "warnings": "Ensure API endpoint is accessible and authentication is configured",
      "use_case": "Integration with external monitoring platforms and incident management systems"
    },
    {
      "id": 246,
      "command": ["awk", "git"],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": ["application logs", "deployment correlation", "version tracking"],
      "task": "Correlate application errors with git deployment history",
      "solution": "error_time=$(awk '/ERROR/ {print $1, $2; exit}' app.log | xargs -I {} date -d '{}' '+%Y-%m-%d %H:%M:%S'); git log --since=\"$error_time\" --until=\"$(date '+%Y-%m-%d %H:%M:%S')\" --pretty=format:'%h %ad %s' --date=local | head -5 && echo && awk -v since=\"$error_time\" '$1 \" \" $2 >= since {print \"Error:\", $0}' app.log | head -10",
      "explanation": "error_time=$(awk '/ERROR/ {print $1, $2; exit}' app.log | xargs -I {} date -d '{}' '+%Y-%m-%d %H:%M:%S') (extract timestamp of first error and convert to standard format) git log --since=\"$error_time\" --until=\"$(date '+%Y-%m-%d %H:%M:%S')\" (show git commits since first error) awk -v since=\"$error_time\" '$1 \" \" $2 >= since {print \"Error:\", $0}' app.log (show errors since that time). Error-deployment correlation analysis.",
      "execution_time": "< 1 min",
      "requirements": ["git"],
      "warnings": null,
      "use_case": "Root cause analysis linking application errors to recent code deployments"
    },
    {
      "id": 247,
      "command": ["awk", "rsyslog"],
      "difficulty": 4,
      "rating": 4,
      "category": "log management",
      "tags": ["centralized logging", "syslog", "remote forwarding"],
      "task": "Parse and forward application logs to centralized syslog server",
      "solution": "tail -f app.log | awk '{if(/ERROR/) priority=\"3\"; else if(/WARN/) priority=\"4\"; else priority=\"6\"; gsub(/\"/, \"\", $0); system(\"logger -p local0.\" priority \" -t myapp \\\"\" $0 \"\\\"\")}' && echo \"Configure rsyslog: echo 'local0.* @@syslog-server:514' >> /etc/rsyslog.conf\"",
      "explanation": "tail -f app.log (follow application log) | awk '{if(/ERROR/) priority=\"3\"; else if(/WARN/) priority=\"4\"; else priority=\"6\"}' (map log levels to syslog priorities: ERROR=3, WARN=4, others=6) gsub(/\"/, \"\", $0) (clean quotes) system(\"logger -p local0.\" priority \" -t myapp \\\"\" $0 \"\\\"\") (send to syslog with facility local0 and tag myapp). Centralized logging integration.",
      "execution_time": "long-running",
      "requirements": ["rsyslog"],
      "warnings": "Configure rsyslog to forward to remote server",
      "use_case": "Enterprise log aggregation and centralized monitoring infrastructure"
    },
    {
      "id": 248,
      "command": ["awk", "prometheus"],
      "difficulty": 5,
      "rating": 5,
      "category": "log metrics",
      "tags": ["metrics extraction", "prometheus", "monitoring"],
      "task": "Extract metrics from logs and expose in Prometheus format",
      "solution": "awk 'BEGIN {print \"# HELP http_requests_total Total HTTP requests\"; print \"# TYPE http_requests_total counter\"} {status[$9]++; method[$6]++; total++} END {for(s in status) printf \"http_requests_total{status=\\\"%s\\\"} %d\\n\", s, status[s]; print \"# HELP http_requests_by_method_total Total requests by method\"; print \"# TYPE http_requests_by_method_total counter\"; for(m in method) printf \"http_requests_by_method_total{method=\\\"%s\\\"} %d\\n\", m, method[m]}' access.log > metrics.prom",
      "explanation": "awk 'BEGIN {print \"# HELP...\"; print \"# TYPE...\"}' (output Prometheus metric headers) {status[$9]++; method[$6]++; total++} (count by status code and HTTP method) END {for(s in status) printf \"http_requests_total{status=\\\"%s\\\"} %d\\n\", s, status[s]} (output status metrics) for(m in method) printf \"http_requests_by_method_total{method=\\\"%s\\\"} %d\\n\", m, method[m] (output method metrics) > metrics.prom. Prometheus-compatible metrics generation.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Custom application metrics for Prometheus monitoring and Grafana dashboards"
    },
    {
      "id": 249,
      "command": ["awk", "logstash"],
      "difficulty": 5,
      "rating": 5,
      "category": "log processing",
      "tags": ["elk stack", "structured processing", "pipeline"],
      "task": "Prepare logs for ELK stack ingestion with structured formatting",
      "solution": "awk '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7); printf \"{\\\"@timestamp\\\":\\\"%s\\\",\\\"client_ip\\\":\\\"%s\\\",\\\"method\\\":\\\"%s\\\",\\\"path\\\":\\\"%s\\\",\\\"status\\\":%d,\\\"bytes\\\":%d,\\\"user_agent\\\":\\\"%s\\\"}\\n\", $4, $1, $6, $7, $9, $10, substr($0, index($0, $12))}' access.log | head -5 && echo '# Logstash config:' && echo 'input { file { path => \"/path/to/logs/*.json\" codec => \"json\" } }'",
      "explanation": "awk '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7)}' (clean timestamp and path) printf \"{\\\"@timestamp\\\":\\\"%s\\\",\\\"client_ip\\\":\\\"%s\\\",\\\"method\\\":\\\"%s\\\",\\\"path\\\":\\\"%s\\\",\\\"status\\\":%d,\\\"bytes\\\":%d,\\\"user_agent\\\":\\\"%s\\\"}\\n\" (format as JSON with ELK-friendly field names) echo 'input { file...}' (show Logstash configuration). ELK stack data preparation pipeline.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Modern log analytics pipeline for Elasticsearch, Logstash, Kibana stack"
    },
    {
      "id": 250,
      "command": ["awk", "machine learning"],
      "difficulty": 5,
      "rating": 5,
      "category": "log analysis",
      "tags": ["anomaly detection", "pattern analysis", "ml preparation"],
      "task": "Extract features from logs for machine learning anomaly detection",
      "solution": "awk 'BEGIN {print \"timestamp,hour,status,bytes,response_time,user_agent_length,path_length\"} {gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; ua_len=length(substr($0, index($0, $12))); path_len=length($7); rt=$NF; printf \"%s,%d,%d,%d,%d,%d,%d\\n\", $4, hour, $9, $10, rt, ua_len, path_len}' access.log > ml_features.csv && echo \"# Use with: python -c 'import pandas as pd; from sklearn.ensemble import IsolationForest; df=pd.read_csv(\\\"ml_features.csv\\\"); model=IsolationForest(); anomalies=model.fit_predict(df.select_dtypes(include=[\\\"number\\\"])); print(\\\"Anomalies:\\\", sum(anomalies==-1))'\"",
      "explanation": "awk 'BEGIN {print \"timestamp,hour,status,bytes,response_time,user_agent_length,path_length\"}' (CSV header with feature names) {gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]} (extract timestamp and hour) ua_len=length(substr($0, index($0, $12))); path_len=length($7) (calculate string lengths as features) printf... (output CSV with numerical features) echo \"# Use with: python...\" (show ML usage example). Feature engineering for log anomaly detection.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Advanced log analytics with machine learning for automated anomaly detection"
    }
  ]
}
