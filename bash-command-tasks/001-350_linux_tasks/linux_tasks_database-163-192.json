{
  "tasks": [
    {
      "id": 163,
      "command": ["jq"],
      "difficulty": 2,
      "rating": 4,
      "category": "data analysis",
      "tags": ["json", "creation", "object"],
      "task": "Create a JSON object from command line with multiple fields",
      "solution": "echo '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}' | jq '.'",
      "explanation": "echo '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}' (create JSON string with escaped quotes) | jq '.' (parse and pretty-print JSON). The '.' operator is identity filter that formats JSON with proper indentation and validates syntax.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Generate JSON configuration or test data from command line"
    },
    {
      "id": 164,
      "command": ["jq"],
      "difficulty": 3,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "array", "construction"],
      "task": "Build JSON array from multiple input values",
      "solution": "echo -e 'apple\\norange\\nbanana' | jq -R -s 'split(\"\\n\") | map(select(length > 0))'",
      "explanation": "echo -e 'apple\\norange\\nbanana' (create multi-line text) | jq -R (read raw strings, not JSON) -s (slurp all input into single string) 'split(\"\\n\")' (split by newlines into array) | map(select(length > 0)) (filter out empty strings). Converts text lines to JSON array.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Convert text lists to JSON arrays for API consumption"
    },
    {
      "id": 165,
      "command": ["jq"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "deep navigation", "nested"],
      "task": "Navigate deeply nested JSON and extract specific values",
      "solution": "jq '.data.users[].profile.settings.preferences.theme' complex.json",
      "explanation": "jq '.data.users[]' (access data object, iterate through users array) '.profile.settings.preferences.theme' (navigate nested object hierarchy to extract theme values). Demonstrates deep object navigation with array iteration in complex JSON structures.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Extract configuration values from complex API responses"
    },
    {
      "id": 166,
      "command": ["jq"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "conditional", "complex filtering"],
      "task": "Filter JSON with multiple conditions and logical operators",
      "solution": "jq '.items[] | select(.price > 50 and .category == \"electronics\" and .inStock == true)' products.json",
      "explanation": "jq '.items[]' (iterate through items array) | select(.price > 50 and .category == \"electronics\" and .inStock == true) (filter with multiple AND conditions: numeric comparison, string equality, boolean check). Complex filtering with logical operators.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Complex product filtering for e-commerce applications"
    },
    {
      "id": 167,
      "command": ["jq"],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "transformation", "restructure"],
      "task": "Transform JSON structure and rename fields with calculations",
      "solution": "jq '.users | map({fullName: (.firstName + \" \" + .lastName), totalValue: (.salary * 12), department: .dept})' employees.json",
      "explanation": "jq '.users | map({...})' (transform each user object) {fullName: (.firstName + \" \" + .lastName) (concatenate strings), totalValue: (.salary * 12) (calculate annual salary), department: .dept} (rename field). Creates new object structure with computed fields.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Data transformation for reporting and analytics systems"
    },
    {
      "id": 168,
      "command": ["jq"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["json", "merging", "combining"],
      "task": "Merge multiple JSON files into single structure",
      "solution": "jq -s 'add' file1.json file2.json file3.json",
      "explanation": "jq -s (slurp mode - read all files into single array) 'add' (merge/combine all objects in array). For arrays, concatenates them; for objects, merges properties with later values overriding earlier ones.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Combine configuration files or merge API responses"
    },
    {
      "id": 169,
      "command": ["jq"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "validation", "schema"],
      "task": "Validate JSON structure and check for required fields",
      "solution": "jq 'if (.name and .email and .age) then \"Valid\" else \"Missing required fields\" end' user.json",
      "explanation": "jq 'if (.name and .email and .age)' (check if all required fields exist and are truthy) then \"Valid\" else \"Missing required fields\" end (conditional output based on validation). Simple schema validation using conditional logic.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Data validation before processing or storing user information"
    },
    {
      "id": 170,
      "command": ["jq"],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "aggregation", "advanced"],
      "task": "Calculate complex aggregations with grouping and statistics",
      "solution": "jq 'group_by(.department) | map({dept: .[0].department, count: length, avgSalary: (map(.salary) | add / length), totalBudget: (map(.salary) | add)})' employees.json",
      "explanation": "jq 'group_by(.department)' (group employees by department) | map({...}) (transform each group) dept: .[0].department (department name from first item), count: length (group size), avgSalary: (map(.salary) | add / length) (calculate average), totalBudget: (map(.salary) | add) (sum all salaries). Complex aggregation with multiple statistics.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Generate department statistics and budget reports from employee data"
    },
    {
      "id": 171,
      "command": ["jq"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["json", "arrays", "manipulation"],
      "task": "Sort JSON array by multiple fields with custom ordering",
      "solution": "jq 'sort_by(.priority, -.date, .name)' tasks.json",
      "explanation": "jq 'sort_by(.priority, -.date, .name)' (sort by multiple criteria in order: first by priority ascending, then by date descending using -, finally by name ascending). Multi-level sorting with mixed sort directions.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Sort task lists or data records by multiple criteria"
    },
    {
      "id": 172,
      "command": ["jq"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "flattening", "restructure"],
      "task": "Flatten nested JSON and create denormalized structure",
      "solution": "jq '.orders[] | {orderId: .id, customerName: .customer.name, customerEmail: .customer.email, itemName: .items[].name, itemPrice: .items[].price}' orders.json",
      "explanation": "jq '.orders[]' (iterate orders) | {...} (create flat structure) orderId: .id, customerName: .customer.name, customerEmail: .customer.email (extract nested customer data), itemName: .items[].name, itemPrice: .items[].price (flatten items array). Creates denormalized records from nested structure.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Prepare nested data for CSV export or database insertion"
    },
    {
      "id": 173,
      "command": ["jq", "curl"],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": ["json", "api", "processing"],
      "task": "Fetch API data and extract specific information with error handling",
      "solution": "curl -s https://api.github.com/users/octocat | jq 'if .message then \"Error: \" + .message else {name: .name, repos: .public_repos, followers: .followers} end'",
      "explanation": "curl -s https://api.github.com/users/octocat (fetch GitHub API data silently) | jq 'if .message then \"Error: \" + .message' (check for API error message) else {name: .name, repos: .public_repos, followers: .followers} end (extract specific fields if successful). API integration with error handling.",
      "execution_time": "< 1 min",
      "requirements": ["jq", "curl"],
      "warnings": null,
      "use_case": "API integration with error handling and data extraction"
    },
    {
      "id": 174,
      "command": ["jq"],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "time series", "analysis"],
      "task": "Analyze time series data and calculate trends",
      "solution": "jq '[.[] | select(.timestamp >= \"2024-01-01\")] | group_by(.date) | map({date: .[0].date, total: (map(.value) | add), avg: (map(.value) | add / length)})' timeseries.json",
      "explanation": "jq '[.[] | select(.timestamp >= \"2024-01-01\")]' (filter data from 2024 onwards) | group_by(.date) (group by date) | map({date: .[0].date, total: (map(.value) | add), avg: (map(.value) | add / length)}) (calculate daily totals and averages). Time series aggregation and analysis.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Time series analysis for metrics and business intelligence"
    },
    {
      "id": 175,
      "command": ["jq"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["json", "null handling", "cleanup"],
      "task": "Clean JSON data by removing null values and empty objects",
      "solution": "jq 'walk(if type == \"object\" then with_entries(select(.value != null and .value != \"\")) else . end)' messy.json",
      "explanation": "jq 'walk(...)' (recursively traverse all values) if type == \"object\" then with_entries(select(.value != null and .value != \"\")) (for objects, keep only entries with non-null, non-empty values) else . end (leave non-objects unchanged). Deep data cleaning with recursive traversal.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Data cleaning and preparation for analysis or storage"
    },
    {
      "id": 176,
      "command": ["awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "json", "conversion"],
      "task": "Convert CSV file to JSON format with proper field mapping",
      "solution": "awk -F',' 'NR==1{for(i=1;i<=NF;i++)h[i]=$i} NR>1{printf \"{%s}\\n\", substr((for(i=1;i<=NF;i++)printf \"\\\"%s\\\":\\\"%s\\\",\",h[i],$i),1,length()-1)}' data.csv",
      "explanation": "awk -F',' (comma delimiter) NR==1{for(i=1;i<=NF;i++)h[i]=$i} (store headers in array) NR>1{...} (for data rows, create JSON objects using headers as keys and current row values). Complex CSV to JSON conversion preserving field names.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Convert spreadsheet data to JSON for web applications"
    },
    {
      "id": 177,
      "command": ["jq"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["json", "csv", "conversion"],
      "task": "Convert JSON array to CSV with custom headers and formatting",
      "solution": "jq -r '(.[0] | keys_unsorted) as $keys | $keys, (.[] | [.[ $keys[] ]] | @csv)' data.json",
      "explanation": "jq -r (raw output without quotes) (.[0] | keys_unsorted) as $keys (get field names from first object, preserve order) | $keys, (.[] | [.[ $keys[] ]] | @csv) (print headers, then for each object extract values in key order and format as CSV). JSON to CSV conversion with proper headers.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Export JSON API data to CSV for spreadsheet analysis"
    },
    {
      "id": 178,
      "command": ["awk"],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": ["csv", "filtering", "conditional"],
      "task": "Filter CSV rows based on multiple column conditions",
      "solution": "awk -F',' '$3 > 1000 && $4 == \"active\" && $2 ~ /^[A-M]/ {print}' customers.csv",
      "explanation": "awk -F',' (comma delimiter) '$3 > 1000 && $4 == \"active\" && $2 ~ /^[A-M]/' (multiple conditions: column 3 > 1000, column 4 equals 'active', column 2 starts with A-M using regex) {print} (print matching rows). Multi-criteria CSV filtering.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Filter customer data for targeted marketing campaigns"
    },
    {
      "id": 179,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["csv", "pivoting", "aggregation"],
      "task": "Create pivot table from CSV data with sum aggregation",
      "solution": "awk -F',' 'NR>1 {sum[$2][$3] += $4} END {for(i in sum) {printf \"%s\", i; for(j in sum[i]) printf \",%s\", sum[i][j]; print \"\"}}' sales.csv",
      "explanation": "awk -F',' NR>1 {sum[$2][$3] += $4} (skip header, create 2D array where sum[row_category][column_category] accumulates values) END {for(i in sum) {...}} (output pivot table: for each row category, print column values). Creates pivot table with sum aggregation.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Create sales reports and business intelligence summaries"
    },
    {
      "id": 180,
      "command": ["awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "validation", "data quality"],
      "task": "Validate CSV data and report inconsistencies",
      "solution": "awk -F',' 'NR>1 {if(NF != 5) print \"Line \" NR \": Wrong number of fields\"; if($3 !~ /^[0-9]+$/) print \"Line \" NR \": Invalid number in column 3\"}' data.csv",
      "explanation": "awk -F',' NR>1 (skip header) {if(NF != 5) print \"Line \" NR \": Wrong number of fields\"} (check field count) if($3 !~ /^[0-9]+$/) print \"Line \" NR \": Invalid number in column 3\" (validate numeric field with regex). Data quality validation with specific error reporting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Data quality checks before importing into databases"
    },
    {
      "id": 181,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "statistics", "analysis"],
      "task": "Calculate comprehensive statistics for CSV numeric columns",
      "solution": "awk -F',' 'NR>1 {sum+=$3; if($3>max||NR==2) max=$3; if($3<min||NR==2) min=$3; count++} END {printf \"Count: %d, Sum: %.2f, Avg: %.2f, Min: %.2f, Max: %.2f\\n\", count, sum, sum/count, min, max}' data.csv",
      "explanation": "awk -F',' NR>1 {sum+=$3; ...count++} (accumulate sum, track min/max, count records) END {printf...} (calculate and display comprehensive statistics: count, sum, average, minimum, maximum). Statistical analysis of numeric column.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Generate statistical summaries for data analysis and reporting"
    },
    {
      "id": 182,
      "command": ["awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "deduplication", "unique"],
      "task": "Remove duplicate rows from CSV file based on specific columns",
      "solution": "awk -F',' '!seen[$1,$2]++' data.csv",
      "explanation": "awk -F',' '!seen[$1,$2]++' (use combination of columns 1 and 2 as key in seen array, print only if not seen before). The ++ operator increments after returning current value, so first occurrence returns 0 (false), ! makes it true for printing. Efficient deduplication.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Clean datasets by removing duplicate customer or transaction records"
    },
    {
      "id": 183,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["csv", "joining", "merge"],
      "task": "Join two CSV files on common field like SQL join",
      "solution": "awk -F',' 'FNR==NR {a[$1]=$2; next} $1 in a {print $0 \",\" a[$1]}' lookup.csv data.csv",
      "explanation": "awk -F',' FNR==NR {a[$1]=$2; next} (first file: store key-value pairs in array a, FNR==NR identifies first file) $1 in a {print $0 \",\" a[$1]} (second file: if key exists in lookup, print record with joined value). SQL-like inner join operation.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Enrich data by joining customer information with transaction data"
    },
    {
      "id": 184,
      "command": ["awk"],
      "difficulty": 3,
      "rating": 3,
      "category": "data analysis",
      "tags": ["csv", "transformation", "formatting"],
      "task": "Transform CSV columns with date formatting and calculations",
      "solution": "awk -F',' 'BEGIN{OFS=\",\"} NR>1 {gsub(/-/, \"/\", $2); $4 = $3 * 1.1; print}' sales.csv",
      "explanation": "awk -F',' BEGIN{OFS=\",\"} (set output field separator) NR>1 {gsub(/-/, \"/\", $2)} (skip header, replace hyphens with slashes in date field) $4 = $3 * 1.1 (calculate new value: 10% markup) print (output transformed record). Data transformation with formatting and calculations.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Data preparation with date standardization and price calculations"
    },
    {
      "id": 185,
      "command": ["cut", "sort", "uniq"],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": ["csv", "unique values", "column analysis"],
      "task": "Extract unique values from specific CSV column with counts",
      "solution": "cut -d',' -f3 data.csv | sort | uniq -c | sort -nr",
      "explanation": "cut -d',' -f3 data.csv (extract 3rd column using comma delimiter) | sort (sort values for uniq) | uniq -c (count unique occurrences) | sort -nr (sort by count, descending). Pipeline for categorical data analysis with frequency counts.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze categorical data distribution in CSV files"
    },
    {
      "id": 186,
      "command": ["awk", "sort"],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "ranking", "top n"],
      "task": "Find top N records in CSV based on numeric column with ties handling",
      "solution": "awk -F',' 'NR>1 {print $3 \",\" $0}' data.csv | sort -t',' -k1 -nr | head -10 | cut -d',' -f2-",
      "explanation": "awk -F',' NR>1 {print $3 \",\" $0} (prepend sort key to each record) | sort -t',' -k1 -nr (sort by first field numerically, descending) | head -10 (take top 10) | cut -d',' -f2- (remove prepended sort key). Top-N selection with proper numeric sorting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify top performers, best-selling products, or highest values"
    },
    {
      "id": 187,
      "command": ["jq", "awk"],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "csv", "workflow"],
      "task": "Complex workflow: CSV to JSON transformation with validation and enrichment",
      "solution": "awk -F',' 'NR==1{for(i=1;i<=NF;i++)h[i]=$i;next} {printf \"{\"; for(i=1;i<=NF;i++) printf \"\\\"%s\\\":\\\"%s\\\"%s\", h[i], $i, (i==NF?\"\":\";\"); print \"}\" }' data.csv | jq 'select(.email | test(\"@.*\\\\.\")) | . + {\"timestamp\": now, \"processed\": true}'",
      "explanation": "awk... (convert CSV to JSON with headers as keys) | jq 'select(.email | test(\"@.*\\\\.\"))' (validate email format with regex) | . + {\"timestamp\": now, \"processed\": true} (enrich with timestamp and processing flag). Complete ETL pipeline with conversion, validation, and enrichment.",
      "execution_time": "1-5 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "ETL pipeline for processing and validating business data"
    },
    {
      "id": 188,
      "command": ["jq"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "nested arrays", "complex"],
      "task": "Process nested JSON arrays and create flat summary report",
      "solution": "jq '.companies[] | {name: .name, totalEmployees: (.departments[] | .employees | length) | add, avgSalary: ([.departments[].employees[].salary] | add / length)}' companies.json",
      "explanation": "jq '.companies[]' (iterate companies) | {name: .name, totalEmployees: (.departments[] | .employees | length) | add (sum employee counts across departments), avgSalary: ([.departments[].employees[].salary] | add / length)} (flatten all salaries and calculate average). Complex nested data aggregation.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Create executive summaries from complex organizational data"
    },
    {
      "id": 189,
      "command": ["jq"],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "conditional logic", "advanced"],
      "task": "Implement complex business logic with conditional transformations",
      "solution": "jq '.customers[] | if .orderTotal > 1000 then . + {\"tier\": \"premium\", \"discount\": 0.15} elif .orderTotal > 500 then . + {\"tier\": \"gold\", \"discount\": 0.10} else . + {\"tier\": \"standard\", \"discount\": 0.05} end' customers.json",
      "explanation": "jq '.customers[]' (iterate customers) | if .orderTotal > 1000 then ... elif .orderTotal > 500 then ... else ... end (nested conditional logic) . + {\"tier\": \"...\", \"discount\": ...} (enrich object with tier and discount based on order total). Business rule implementation with tiered logic.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Customer tier assignment and dynamic pricing calculations"
    },
    {
      "id": 190,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "time series", "windowing"],
      "task": "Calculate moving averages and trends in time series CSV data",
      "solution": "awk -F',' 'NR>1 {values[NR-1]=$3; if(NR>4) {sum=0; for(i=NR-4;i<=NR-1;i++) sum+=values[i]; printf \"%s,%.2f\\n\", $1, sum/4}}' timeseries.csv",
      "explanation": "awk -F',' NR>1 {values[NR-1]=$3} (store values in array) if(NR>4) {sum=0; for(i=NR-4;i<=NR-1;i++) sum+=values[i]} (after 4 values, calculate sum of last 4) printf \"%s,%.2f\\n\", $1, sum/4 (output date and 4-period moving average). Time series analysis with sliding window.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Financial analysis and trend detection in time series data"
    },
    {
      "id": 191,
      "command": ["jq", "date"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["json", "timestamps", "date manipulation"],
      "task": "Process JSON with timestamp manipulation and date-based filtering",
      "solution": "jq --arg cutoff \"$(date -d '30 days ago' '+%Y-%m-%d')\" '.events[] | select(.date >= $cutoff) | . + {\"daysAgo\": ((now - (.date | strptime(\"%Y-%m-%d\") | mktime)) / 86400 | floor)}'",
      "explanation": "jq --arg cutoff \"$(date -d '30 days ago' '+%Y-%m-%d')\" (pass shell command result as jq variable) '.events[] | select(.date >= $cutoff)' (filter events from last 30 days) | . + {\"daysAgo\": ((now - (.date | strptime(\"%Y-%m-%d\") | mktime)) / 86400 | floor)} (add calculated field for days ago). Date arithmetic and filtering integration.",
      "execution_time": "< 1 min",
      "requirements": ["jq", "date"],
      "warnings": null,
      "use_case": "Event processing with relative date calculations and filtering"
    },
    {
      "id": 192,
      "command": ["awk", "jq"],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": ["csv", "json", "advanced pipeline"],
      "task": "Create advanced data processing pipeline: CSV analysis to JSON report",
      "solution": "awk -F',' 'NR>1 {dept[$2]++; sales[$2]+=$4} END {for(d in dept) printf \"{\\\"department\\\":\\\"%s\\\",\\\"employees\\\":%d,\\\"totalSales\\\":%.2f}\\n\", d, dept[d], sales[d]}' sales.csv | jq -s 'sort_by(-.totalSales) | {\"reportDate\": now, \"departments\": ., \"summary\": {\"totalDepts\": length, \"totalSales\": (map(.totalSales) | add)}}'",
      "explanation": "awk -F',' 'NR>1 {dept[$2]++; sales[$2]+=$4}' (skip header, count employees per department, sum sales per department) END {for(d in dept) printf...} (output JSON objects for each department) | jq -s 'sort_by(-.totalSales)' (slurp into array, sort by sales descending) | {\"reportDate\": now, \"departments\": ., \"summary\": {...}} (create final report with metadata and summary statistics). Complete business intelligence pipeline from CSV to formatted JSON report.",
      "execution_time": "< 1 min",
      "requirements": ["jq"],
      "warnings": null,
      "use_case": "Business intelligence pipeline for executive dashboard generation"
    }
  ]
}