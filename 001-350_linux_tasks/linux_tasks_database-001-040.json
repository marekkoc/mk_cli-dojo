{
  "tasks": [
    {
      "id": 1,
      "command": ["find"],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": ["size", "modification", "cleanup"],
      "task": "Find all files in home directory modified in last 7 days and larger than 10MB",
      "solution": "find ~ -type f -mtime -7 -size +10M",
      "explanation": "find ~ (search in home directory) -type f (files only, not directories) -mtime -7 (modified within last 7 days, negative value means 'less than N days ago') -size +10M (size greater than 10 megabytes)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify large files that were recently created or modified for cleanup"
    },
    {
      "id": 2,
      "command": ["find"],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": ["empty", "cleanup", "git"],
      "task": "Find and remove all empty directories in current project, excluding .git directories",
      "solution": "find . -type d -empty -not -path \"*/.git/*\" -delete",
      "explanation": "find . (current directory) -type d (directories only) -empty (completely empty directories) -not -path '*/.git/*' (exclude .git subdirectories) -delete (remove found directories). The -not operator negates the path condition.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Always test with -print first before using -delete",
      "use_case": "Clean up project structure by removing empty directories"
    },
    {
      "id": 3,
      "command": ["find", "python"],
      "difficulty": 4,
      "rating": 5,
      "category": "development",
      "tags": ["python", "syntax", "validation"],
      "task": "Find all Python files and check which ones have syntax errors",
      "solution": "find . -name \"*.py\" -exec python -m py_compile {} \\; -print 2>&1 | grep -B1 \"SyntaxError\"",
      "explanation": "find . -name '*.py' (locate Python files) -exec python -m py_compile {} \\; (compile each file to check syntax, {} is placeholder for filename) -print (show filename after execution) 2>&1 (redirect stderr to stdout) | grep -B1 'SyntaxError' (show syntax errors with 1 line before for context)",
      "execution_time": "1-5 min",
      "requirements": ["python"],
      "warnings": null,
      "use_case": "Validate Python code syntax before committing to repository"
    },
    {
      "id": 4,
      "command": ["find", "md5sum", "sort", "uniq"],
      "difficulty": 4,
      "rating": 4,
      "category": "file management",
      "tags": ["duplicates", "hash", "cleanup"],
      "task": "Find duplicate files in Downloads directory based on MD5 hash",
      "solution": "find ~/Downloads -type f -exec md5sum {} \\; | sort | uniq -d -w32",
      "explanation": "find ~/Downloads -type f (find files in Downloads) -exec md5sum {} \\; (calculate MD5 hash for each file) | sort (sort by hash values) | uniq -d -w32 (show only duplicates -d, comparing first 32 characters -w32 which is the MD5 hash length)",
      "execution_time": "1-5 min",
      "requirements": ["md5sum"],
      "warnings": null,
      "use_case": "Identify and remove duplicate downloaded files to save disk space"
    },
    {
      "id": 5,
      "command": ["find"],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": ["executable", "binary"],
      "task": "Find all executable files without extension in system directories",
      "solution": "find /usr/bin /usr/local/bin -type f -executable -not -name \"*.*\" 2>/dev/null",
      "explanation": "find /usr/bin /usr/local/bin (search in system binary directories) -type f (files only) -executable (files with execute permission) -not -name '*.*' (exclude files with extensions) 2>/dev/null (suppress permission denied errors)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Discover system binaries and commands available on the system"
    },
    {
      "id": 6,
      "command": ["find", "ls", "sort"],
      "difficulty": 4,
      "rating": 4,
      "category": "file management",
      "tags": ["size", "analysis", "largest"],
      "task": "Find the largest file in each subdirectory",
      "solution": "find . -type d -exec sh -c 'echo \"=== $1 ===\"; find \"$1\" -maxdepth 1 -type f -exec ls -lh {} \\; | sort -k5 -hr | head -1' _ {} \\;",
      "explanation": "find . -type d (find directories) -exec sh -c '...' _ {} \\; (execute shell command for each directory). Inside: echo directory name, find files with -maxdepth 1 (current level only), ls -lh (long format, human readable), sort -k5 -hr (sort by 5th column - size, human-numeric, reverse), head -1 (show largest)",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze disk usage by finding largest files in each directory"
    },
    {
      "id": 7,
      "command": ["find"],
      "difficulty": 3,
      "rating": 4,
      "category": "security",
      "tags": ["permissions", "security", "writable"],
      "task": "Find files with world-writable permissions (security risk)",
      "solution": "find . -type f -perm -o+w -ls",
      "explanation": "find . -type f (find files) -perm -o+w (permission test: 'other' users have write permission, - means 'at least these permissions') -ls (detailed listing showing permissions, owner, size, date). World-writable files are security risks as any user can modify them.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security audit to find files that could be modified by any user"
    },
    {
      "id": 8,
      "command": ["find", "tar", "xargs"],
      "difficulty": 4,
      "rating": 3,
      "category": "backup",
      "tags": ["logs", "archive", "compression"],
      "task": "Find log files older than 30 days and compress them into archive",
      "solution": "find . -name \"*.log\" -mtime +30 -print0 | xargs -0 tar -czf old_logs_$(date +%Y%m%d).tar.gz",
      "explanation": "find . -name '*.log' -mtime +30 (log files older than 30 days) -print0 (null-separated output for filenames with spaces) | xargs -0 (process null-separated input) tar -czf (create, gzip compress, filename) old_logs_$(date +%Y%m%d).tar.gz (timestamped archive name)",
      "execution_time": "1-5 min",
      "requirements": ["tar"],
      "warnings": null,
      "use_case": "Automated log rotation and archiving for disk space management"
    },
    {
      "id": 9,
      "command": ["find", "grep"],
      "difficulty": 3,
      "rating": 3,
      "category": "file management",
      "tags": ["special characters", "encoding", "spaces"],
      "task": "Find files with spaces or non-ASCII characters in their names",
      "solution": "find . -name \"* *\" -type f; find . -type f | grep -P '[^\\x00-\\x7F]'",
      "explanation": "Two commands: 1) find . -name '* *' -type f (files with spaces in names using wildcard pattern) 2) find . -type f | grep -P '[^\\x00-\\x7F]' (pipe filenames to grep with Perl regex -P matching non-ASCII characters outside \\x00-\\x7F range)",
      "execution_time": "< 1 min",
      "requirements": ["grep with -P option"],
      "warnings": null,
      "use_case": "Identify problematic filenames before transferring to different systems"
    },
    {
      "id": 10,
      "command": ["find", "inotifywait"],
      "difficulty": 5,
      "rating": 5,
      "category": "monitoring",
      "tags": ["real-time", "monitoring", "python"],
      "task": "Monitor directory for newly created Python files in real-time",
      "solution": "inotifywait -m -r -e create --format '%w%f' . | grep '\\.py$'",
      "explanation": "inotifywait -m (monitor continuously) -r (recursive) -e create (watch for file creation events) --format '%w%f' (output format: watch path + filename) . (current directory) | grep '\\.py$' (filter for files ending with .py using regex)",
      "execution_time": "long-running",
      "requirements": ["inotify-tools"],
      "warnings": null,
      "use_case": "Development environment monitoring for automatic code processing"
    },
    {
      "id": 11,
      "command": ["grep"],
      "difficulty": 2,
      "rating": 2,
      "category": "text processing",
      "tags": ["email", "regex", "validation"],
      "task": "Find all email addresses in text files",
      "solution": "grep -r -E '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' .",
      "explanation": "grep -r (recursive search) -E (extended regex) with email pattern: [a-zA-Z0-9._%+-]+ (username: letters, numbers, dots, underscores, percent, plus, hyphen) @ (literal at symbol) [a-zA-Z0-9.-]+ (domain: letters, numbers, dots, hyphens) \\. (literal dot) [a-zA-Z]{2,} (domain extension: 2+ letters)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract contact information from documents or logs"
    },
    {
      "id": 12,
      "command": ["grep", "sort", "uniq"],
      "difficulty": 3,
      "rating": 3,
      "category": "text processing",
      "tags": ["ip", "logs", "frequency"],
      "task": "Find most frequent IP addresses in log files",
      "solution": "grep -o -E '([0-9]{1,3}\\.){3}[0-9]{1,3}' *.log | sort | uniq -c | sort -nr | head -10",
      "explanation": "grep -o (only output matching parts) -E (extended regex) '([0-9]{1,3}\\.){3}[0-9]{1,3}' (IP pattern: 1-3 digits + dot, repeated 3 times, then 1-3 digits) *.log | sort | uniq -c (count occurrences) | sort -nr (numeric reverse sort) | head -10 (top 10)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze web server logs to identify top visitors or potential attacks"
    },
    {
      "id": 13,
      "command": ["grep"],
      "difficulty": 4,
      "rating": 4,
      "category": "text processing",
      "tags": ["multiline", "context", "code"],
      "task": "Find function definitions in Python files with 3 lines of context",
      "solution": "grep -r -n -A 3 -B 1 '^def ' --include='*.py' .",
      "explanation": "grep -r (recursive) -n (show line numbers) -A 3 (3 lines after match) -B 1 (1 line before match) '^def ' (lines starting with 'def ' - Python function definitions) --include='*.py' (only search Python files) . (current directory)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Code review and documentation generation from Python projects"
    },
    {
      "id": 14,
      "command": ["awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["csv", "sum", "column"],
      "task": "Calculate sum of values in specific column of CSV file",
      "solution": "awk -F',' '{sum += $3} END {print \"Total:\", sum}' data.csv",
      "explanation": "awk -F',' (field separator is comma for CSV) '{sum += $3}' (add 3rd column value to sum variable for each line) END {print 'Total:', sum} (after processing all lines, print the total sum)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quick financial calculations from CSV exports"
    },
    {
      "id": 15,
      "command": ["awk"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["csv", "groupby", "statistics"],
      "task": "Group CSV data by column and calculate average for each group",
      "solution": "awk -F',' '{sum[$1] += $2; count[$1]++} END {for (i in sum) print i, sum[i]/count[i]}' data.csv",
      "explanation": "awk -F',' (CSV delimiter) '{sum[$1] += $2; count[$1]++}' (use 1st column as array key, accumulate 2nd column values and count occurrences) END {for (i in sum) print i, sum[i]/count[i]} (iterate through groups, calculate and print averages)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze sales data by region or category with averages"
    },
    {
      "id": 16,
      "command": ["awk"],
      "difficulty": 2,
      "rating": 3,
      "category": "text processing",
      "tags": ["whitespace", "cleanup", "formatting"],
      "task": "Remove duplicate consecutive blank lines from text file",
      "solution": "awk '/^$/ {if (!blank) print; blank=1; next} {blank=0; print}' file.txt",
      "explanation": "awk with pattern-action: '/^$/' (empty line pattern) {if (!blank) print; blank=1; next} (if not already in blank state, print line and set blank flag, skip to next line) {blank=0; print} (for non-empty lines, reset blank flag and print). This preserves single blank lines but removes consecutive ones.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Clean up text files or code by removing excessive blank lines"
    },
    {
      "id": 17,
      "command": ["sed"],
      "difficulty": 2,
      "rating": 2,
      "category": "text processing",
      "tags": ["replacement", "email", "privacy"],
      "task": "Replace all email addresses with 'REDACTED' in text files",
      "solution": "sed -E 's/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/REDACTED/g' file.txt",
      "explanation": "sed -E (extended regex mode) 's/pattern/replacement/g' (substitute command with global flag). Pattern matches email addresses: [a-zA-Z0-9._%+-]+ (username part) @ [a-zA-Z0-9.-]+ (domain) \\. [a-zA-Z]{2,} (extension). Replaces all matches with 'REDACTED'.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Anonymize documents before sharing by removing email addresses"
    },
    {
      "id": 18,
      "command": ["sed"],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": ["extraction", "lines", "range"],
      "task": "Extract lines between two patterns from a file",
      "solution": "sed -n '/START_PATTERN/,/END_PATTERN/p' file.txt",
      "explanation": "sed -n (suppress default output) '/START_PATTERN/,/END_PATTERN/p' (address range from first pattern to second pattern, with print command). This extracts all lines between and including the START_PATTERN and END_PATTERN lines.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract specific sections from configuration files or logs"
    },
    {
      "id": 19,
      "command": ["sed"],
      "difficulty": 4,
      "rating": 5,
      "category": "text processing",
      "tags": ["json", "formatting", "pretty"],
      "task": "Pretty print JSON by adding proper indentation",
      "solution": "sed 's/,/,\\n/g; s/{/{\\n/g; s/}/\\n}/g' file.json | sed 's/^/  /' | sed '/^\\s*$/d'",
      "explanation": "Three sed commands piped: 1) s/,/,\\n/g; s/{/{\\n/g; s/}/\\n}/g (add newlines after commas, opening and closing braces) 2) s/^/  / (add 2-space indentation to each line) 3) /^\\s*$/d (delete empty lines). This creates basic JSON pretty-printing.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Format minified JSON for better readability during debugging"
    },
    {
      "id": 20,
      "command": ["find", "grep", "awk"],
      "difficulty": 5,
      "rating": 5,
      "category": "development",
      "tags": ["code analysis", "complexity", "python"],
      "task": "Find Python files and calculate average lines of code per function",
      "solution": "find . -name '*.py' -exec grep -c '^def ' {} \\; | paste <(find . -name '*.py') - | awk '{cmd=\"wc -l < \"$1; cmd | getline lines; close(cmd); if($2>0) print $1, lines/$2; else print $1, lines}' | awk '{sum+=$2; count++} END {print \"Average LOC per function:\", sum/count}'",
      "explanation": "Complex pipeline: 1) Count 'def' lines per file 2) paste filenames with counts 3) First awk: get line count with wc, calculate lines/functions ratio 4) Second awk: calculate overall average. Uses getline to execute shell commands from within awk.",
      "execution_time": "1-5 min",
      "requirements": ["python"],
      "warnings": null,
      "use_case": "Code quality analysis to identify overly complex functions"
    },
    {
      "id": 21,
      "command": ["sort", "uniq"],
      "difficulty": 2,
      "rating": 2,
      "category": "data analysis",
      "tags": ["duplicates", "count", "frequency"],
      "task": "Find and count duplicate lines in a file",
      "solution": "sort file.txt | uniq -d -c | sort -nr",
      "explanation": "sort file.txt (sort lines alphabetically - required for uniq) | uniq -d -c (show only duplicate lines -d with count -c) | sort -nr (sort by count numerically in reverse order to show most frequent first)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify repeated entries in data files or logs"
    },
    {
      "id": 22,
      "command": ["cut", "sort", "uniq"],
      "difficulty": 3,
      "rating": 3,
      "category": "data analysis",
      "tags": ["csv", "unique", "column"],
      "task": "Extract unique values from specific column in CSV file",
      "solution": "cut -d',' -f3 data.csv | sort | uniq",
      "explanation": "cut -d',' (delimiter is comma) -f3 (extract 3rd field/column) data.csv | sort (sort extracted values) | uniq (remove duplicate lines, showing only unique values). Cut extracts columns, sort prepares for uniq which requires sorted input.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze data categories or enumerate possible values in datasets"
    },
    {
      "id": 23,
      "command": ["tr", "sort", "uniq"],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": ["word count", "frequency", "analysis"],
      "task": "Find most frequently used words in text file",
      "solution": "tr -cs 'A-Za-z' '\\n' < file.txt | sort | uniq -c | sort -nr | head -10",
      "explanation": "tr -cs 'A-Za-z' '\\n' (translate: -c complement set, -s squeeze, replace non-letters with newlines, one word per line) < file.txt | sort | uniq -c (count word frequencies) | sort -nr (sort by count, descending) | head -10 (top 10 words)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Text analysis for content writing or document summarization"
    },
    {
      "id": 24,
      "command": ["wc", "find"],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": ["statistics", "code", "lines"],
      "task": "Count total lines of code in all Python files",
      "solution": "find . -name '*.py' -exec wc -l {} + | tail -1",
      "explanation": "find . -name '*.py' (find Python files) -exec wc -l {} + (count lines in batches using + instead of \\; for efficiency) | tail -1 (last line contains total count). The + operator passes multiple files to single wc command, and wc automatically provides a total.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Project size estimation and development progress tracking"
    },
    {
      "id": 25,
      "command": ["grep", "awk", "sort"],
      "difficulty": 4,
      "rating": 4,
      "category": "monitoring",
      "tags": ["logs", "errors", "analysis"],
      "task": "Analyze error patterns in log files and show top 5 error types",
      "solution": "grep -i error *.log | awk -F: '{print $3}' | sort | uniq -c | sort -nr | head -5",
      "explanation": "grep -i error *.log (case-insensitive search for 'error' in log files) | awk -F: '{print $3}' (split by colon, print 3rd field - typically the error message) | sort | uniq -c (count unique error types) | sort -nr (sort by frequency) | head -5 (top 5)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "System monitoring and troubleshooting by identifying common errors"
    },
    {
      "id": 26,
      "command": ["find", "dcmdump"],
      "difficulty": 2,
      "rating": 3,
      "category": "medical imaging",
      "tags": ["dicom", "metadata", "inspection"],
      "task": "Find all DICOM files and display their basic metadata",
      "solution": "find . -name \"*.dcm\" -o -name \"*.dicom\" | head -5 | xargs -I {} dcmdump +P 0008,0020 +P 0010,0010 +P 0008,0060 {}",
      "explanation": "find . -name '*.dcm' -o -name '*.dicom' (find DICOM files with common extensions, -o means OR) | head -5 (limit to 5 files for quick preview) | xargs -I {} (replace {} with each filename) dcmdump +P 0008,0020 +P 0010,0010 +P 0008,0060 {} (extract specific DICOM tags: study date, patient name, modality)",
      "execution_time": "< 1 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Quick inspection of DICOM study dates, patient names, and modalities"
    },
    {
      "id": 27,
      "command": ["find", "dcmdump", "grep"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "patient", "privacy"],
      "task": "Find DICOM files containing specific patient information",
      "solution": "find . -name \"*.dcm\" -exec dcmdump {} \\; | grep -B5 -A5 \"Patient.*John.*Doe\"",
      "explanation": "find . -name '*.dcm' -exec dcmdump {} \\; (dump DICOM metadata for each file) | grep -B5 -A5 'Patient.*John.*Doe' (search for patient name pattern with 5 lines before and after for context). The .* allows for flexible matching of patient name formats.",
      "execution_time": "1-5 min",
      "requirements": ["dcmtk"],
      "warnings": "Handle patient data according to HIPAA compliance",
      "use_case": "Locate specific patient studies for review or analysis"
    },
    {
      "id": 28,
      "command": ["find", "dcmdump", "awk"],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "modality", "statistics"],
      "task": "Count DICOM files by imaging modality (CT, MRI, X-Ray, etc.)",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0008,0060 {} \\; | grep \"Modality\" | awk '{print $3}' | sort | uniq -c | sort -nr",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0008,0060 {} \\; (extract modality tag 0008,0060 from each DICOM file) | grep 'Modality' (filter modality lines) | awk '{print $3}' (extract modality value) | sort | uniq -c (count occurrences) | sort -nr (sort by count)",
      "execution_time": "1-5 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Analyze medical imaging dataset composition for research planning"
    },
    {
      "id": 29,
      "command": ["find", "dcmconv"],
      "difficulty": 3,
      "rating": 3,
      "category": "medical imaging",
      "tags": ["dicom", "conversion", "format"],
      "task": "Convert all DICOM files to different transfer syntax",
      "solution": "find . -name \"*.dcm\" -exec dcmconv +ti {} {}.new \\; && find . -name \"*.dcm.new\" -exec sh -c 'mv \"$1\" \"${1%.new}\"' _ {} \\;",
      "explanation": "Two-step process: 1) find . -name '*.dcm' -exec dcmconv +ti {} {}.new \\; (convert each DICOM file to implicit transfer syntax, creating .new files) && 2) find . -name '*.dcm.new' -exec sh -c 'mv \"$1\" \"${1%.new}\"' _ {} \\; (rename .new files back to original names, ${1%.new} removes .new suffix)",
      "execution_time": "5+ min",
      "requirements": ["dcmtk"],
      "warnings": "Backup original files before conversion",
      "use_case": "Standardize DICOM transfer syntax for compatibility with specific systems"
    },
    {
      "id": 30,
      "command": ["find", "dcmdump", "grep", "cut"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["dicom", "series", "organization"],
      "task": "Extract and organize DICOM files by Series Instance UID",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'series=$(dcmdump +P 0020,000e \"$1\" | grep SeriesInstanceUID | cut -d\"[\" -f2 | cut -d\"]\" -f1); mkdir -p \"series_$series\" && cp \"$1\" \"series_$series/\"' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (execute shell script for each DICOM file). Script: extract SeriesInstanceUID using dcmdump, grep to find the line, cut to extract the value between brackets, create directory named series_$series, copy file to that directory. This organizes mixed DICOM files by series.",
      "execution_time": "5+ min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Organize mixed DICOM files into separate series directories for analysis"
    },
    {
      "id": 31,
      "command": ["find", "dcmdump", "grep"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "validation", "corruption"],
      "task": "Find corrupted or invalid DICOM files",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'dcmdump \"$1\" >/dev/null 2>&1 || echo \"Corrupted: $1\"' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (execute shell command for each DICOM file). Command: dcmdump '$1' >/dev/null 2>&1 (try to dump DICOM data, suppress all output) || echo 'Corrupted: $1' (if dcmdump fails, print filename as corrupted). Uses logical OR to execute echo only on failure.",
      "execution_time": "1-5 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Quality control to identify damaged DICOM files before processing"
    },
    {
      "id": 32,
      "command": ["find", "dcm2niix"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "nifti", "conversion"],
      "task": "Convert DICOM series to NIfTI format for neuroimaging analysis",
      "solution": "find . -type d -exec sh -c 'if ls \"$1\"/*.dcm >/dev/null 2>&1; then dcm2niix -o nifti_output \"$1\"; fi' _ {} \\;",
      "explanation": "find . -type d (find directories) -exec sh -c '...' _ {} \\; (execute shell script for each directory). Script: if ls '$1'/*.dcm >/dev/null 2>&1 (check if directory contains .dcm files, suppress output) then dcm2niix -o nifti_output '$1' (convert DICOM series to NIfTI format in output directory). Only processes directories with DICOM files.",
      "execution_time": "5+ min",
      "requirements": ["dcm2niix"],
      "warnings": null,
      "use_case": "Prepare DICOM data for neuroimaging analysis tools like FSL or SPM"
    },
    {
      "id": 33,
      "command": ["find", "dcmdump", "sort"],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "temporal", "sorting"],
      "task": "Sort DICOM files by acquisition date and time",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'echo \"$(dcmdump +P 0008,0022 +P 0008,0032 \"$1\" | grep -E \"AcquisitionDate|AcquisitionTime\" | tr \"\\n\" \" \") $1\"' _ {} \\; | sort",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (process each DICOM file). Script: dcmdump +P 0008,0022 +P 0008,0032 (extract acquisition date and time tags) | grep -E 'AcquisitionDate|AcquisitionTime' (filter relevant lines) | tr '\\n' ' ' (join date and time on one line) then echo with filename | sort (sort by date/time)",
      "execution_time": "1-5 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Temporal analysis of medical imaging studies or longitudinal research"
    },
    {
      "id": 34,
      "command": ["find", "dcmodify"],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["dicom", "anonymization", "privacy"],
      "task": "Anonymize DICOM files by removing patient identifying information",
      "solution": "find . -name \"*.dcm\" -exec dcmodify -gin -nb -ea \"(0010,0010)=ANONYMOUS\" -ea \"(0010,0020)=ANON$(date +%s)\" {} \\;",
      "explanation": "find . -name '*.dcm' -exec dcmodify (DICOM modification tool) -gin (generate new instance UIDs) -nb (no backup) -ea '(0010,0010)=ANONYMOUS' (replace patient name with ANONYMOUS) -ea '(0010,0020)=ANON$(date +%s)' (replace patient ID with ANON + timestamp) {} \\; (process each file). Creates anonymized versions in-place.",
      "execution_time": "5+ min",
      "requirements": ["dcmtk"],
      "warnings": "Ensure compliance with medical data protection regulations",
      "use_case": "Prepare medical imaging data for research sharing while protecting patient privacy"
    },
    {
      "id": 35,
      "command": ["find", "dcmdump", "awk"],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "slice", "thickness"],
      "task": "Calculate average slice thickness across all DICOM images",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0018,0050 {} \\; | grep \"SliceThickness\" | awk -F\"[\" '{print $2}' | awk -F\"]\" '{sum+=$1; count++} END {if(count>0) print \"Average slice thickness:\", sum/count \"mm\"}'",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0018,0050 {} \\; (extract slice thickness tag) | grep 'SliceThickness' (filter relevant lines) | awk -F'[' '{print $2}' (split by '[', get value part) | awk -F']' '{sum+=$1; count++} END {...} (split by ']', accumulate values, calculate average)",
      "execution_time": "< 1 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Quality assessment of imaging protocols for research standardization"
    },
    {
      "id": 36,
      "command": ["find", "dcmdump", "grep", "wc"],
      "difficulty": 3,
      "rating": 3,
      "category": "medical imaging",
      "tags": ["dicom", "contrast", "enhancement"],
      "task": "Count DICOM images with contrast enhancement",
      "solution": "find . -name \"*.dcm\" -exec dcmdump {} \\; | grep -i \"contrast\" | wc -l",
      "explanation": "find . -name '*.dcm' -exec dcmdump {} \\; (dump all DICOM metadata) | grep -i 'contrast' (case-insensitive search for contrast-related information) | wc -l (count matching lines). This searches for any mention of contrast in DICOM metadata fields.",
      "execution_time": "1-5 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Analyze dataset composition for contrast-enhanced vs non-enhanced studies"
    },
    {
      "id": 37,
      "command": ["find", "dcmdump"],
      "difficulty": 2,
      "rating": 3,
      "category": "medical imaging",
      "tags": ["dicom", "manufacturer", "equipment"],
      "task": "List all imaging equipment manufacturers in DICOM dataset",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0008,0070 {} \\; | grep \"Manufacturer\" | sort | uniq",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0008,0070 {} \\; (extract manufacturer tag 0008,0070 from each DICOM file) | grep 'Manufacturer' (filter manufacturer lines) | sort | uniq (sort and remove duplicates to get unique manufacturer list)",
      "execution_time": "< 1 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Equipment inventory and vendor analysis for medical imaging department"
    },
    {
      "id": 38,
      "command": ["find", "dcmj2pnm"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "preview", "conversion"],
      "task": "Generate PNG preview images from DICOM files for quick review",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'dcmj2pnm \"$1\" \"${1%.dcm}.png\" --write-png' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (process each DICOM file). Script: dcmj2pnm '$1' '${1%.dcm}.png' --write-png (convert DICOM to PNG format, ${1%.dcm} removes .dcm extension and adds .png). Creates PNG preview for each DICOM image.",
      "execution_time": "5+ min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Create visual previews of medical images for quick dataset review"
    },
    {
      "id": 39,
      "command": ["find", "dcmdump", "grep"],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": ["dicom", "body part", "anatomy"],
      "task": "Find DICOM images of specific body parts or anatomical regions",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'if dcmdump \"$1\" | grep -i \"BodyPartExamined.*CHEST\\|StudyDescription.*chest\"; then echo \"$1\"; fi' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (process each DICOM file). Script: if dcmdump '$1' | grep -i 'BodyPartExamined.*CHEST\\|StudyDescription.*chest' (search for chest in body part or study description fields, case-insensitive) then echo '$1' (print filename if match found). Uses alternation with \\| for multiple patterns.",
      "execution_time": "1-5 min",
      "requirements": ["dcmtk"],
      "warnings": null,
      "use_case": "Filter medical imaging datasets by anatomical region for targeted analysis"
    },
    {
      "id": 40,
      "command": ["find", "dcmdump", "awk"],
      "difficulty": 5,
      "rating": 5,
      "category": "medical imaging",
      "tags": ["dicom", "dose", "radiation"],
      "task": "Calculate total radiation dose across all CT DICOM studies",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0018,9345 {} \\; | grep \"CTDIvol\" | awk -F\"[\" '{print $2}' | awk -F\"]\" '{sum+=$1; count++} END {if(count>0) print \"Total CTDIvol:\", sum \"mGy across\", count \"images\"}'",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0018,9345 {} \\; (extract CTDIvol tag - radiation dose index) | grep 'CTDIvol' (filter dose lines) | awk -F'[' '{print $2}' (extract value between brackets) | awk -F']' '{sum+=$1; count++} END {...} (accumulate dose values and count, print total). CTDIvol is CT dose index volume in mGy.",
      "execution_time": "1-5 min",
      "requirements": ["dcmtk"],
      "warnings": "Radiation dose analysis requires medical physics expertise",
      "use_case": "Radiation dose monitoring and optimization for patient safety compliance"
    }
  ]
}