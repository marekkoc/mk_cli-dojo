{
  "tasks": [
    {
      "id": 1,
      "command": [
        "find"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "size",
        "modification",
        "cleanup"
      ],
      "task": "Find all files in home directory modified in last 7 days and larger than 10MB",
      "solution": "find ~ -type f -mtime -7 -size +10M",
      "explanation": "find ~ (search in home directory) -type f (files only, not directories) -mtime -7 (modified within last 7 days, negative value means 'less than N days ago') -size +10M (size greater than 10 megabytes)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify large files that were recently created or modified for cleanup"
    },
    {
      "id": 2,
      "command": [
        "find"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "empty",
        "cleanup",
        "git"
      ],
      "task": "Find and remove all empty directories in current project, excluding .git directories",
      "solution": "find . -type d -empty -not -path \"*/.git/*\" -delete",
      "explanation": "find . (current directory) -type d (directories only) -empty (completely empty directories) -not -path '*/.git/*' (exclude .git subdirectories) -delete (remove found directories). The -not operator negates the path condition.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Always test with -print first before using -delete",
      "use_case": "Clean up project structure by removing empty directories"
    },
    {
      "id": 3,
      "command": [
        "find",
        "python"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "development",
      "tags": [
        "python",
        "syntax",
        "validation"
      ],
      "task": "Find all Python files and check which ones have syntax errors",
      "solution": "find . -name \"*.py\" -exec python -m py_compile {} \\; -print 2>&1 | grep -B1 \"SyntaxError\"",
      "explanation": "find . -name '*.py' (locate Python files) -exec python -m py_compile {} \\; (compile each file to check syntax, {} is placeholder for filename) -print (show filename after execution) 2>&1 (redirect stderr to stdout) | grep -B1 'SyntaxError' (show syntax errors with 1 line before for context)",
      "execution_time": "1-5 min",
      "requirements": [
        "python"
      ],
      "warnings": null,
      "use_case": "Validate Python code syntax before committing to repository"
    },
    {
      "id": 4,
      "command": [
        "find",
        "md5sum",
        "sort",
        "uniq"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "file management",
      "tags": [
        "duplicates",
        "hash",
        "cleanup"
      ],
      "task": "Find duplicate files in Downloads directory based on MD5 hash",
      "solution": "find ~/Downloads -type f -exec md5sum {} \\; | sort | uniq -d -w32",
      "explanation": "find ~/Downloads -type f (find files in Downloads) -exec md5sum {} \\; (calculate MD5 hash for each file) | sort (sort by hash values) | uniq -d -w32 (show only duplicates -d, comparing first 32 characters -w32 which is the MD5 hash length)",
      "execution_time": "1-5 min",
      "requirements": [
        "md5sum"
      ],
      "warnings": null,
      "use_case": "Identify and remove duplicate downloaded files to save disk space"
    },
    {
      "id": 5,
      "command": [
        "find"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "executable",
        "binary"
      ],
      "task": "Find all executable files without extension in system directories",
      "solution": "find /usr/bin /usr/local/bin -type f -executable -not -name \"*.*\" 2>/dev/null",
      "explanation": "find /usr/bin /usr/local/bin (search in system binary directories) -type f (files only) -executable (files with execute permission) -not -name '*.*' (exclude files with extensions) 2>/dev/null (suppress permission denied errors)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Discover system binaries and commands available on the system"
    },
    {
      "id": 6,
      "command": [
        "find",
        "ls",
        "sort"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "file management",
      "tags": [
        "size",
        "analysis",
        "largest"
      ],
      "task": "Find the largest file in each subdirectory",
      "solution": "find . -type d -exec sh -c 'echo \"=== $1 ===\"; find \"$1\" -maxdepth 1 -type f -exec ls -lh {} \\; | sort -k5 -hr | head -1' _ {} \\;",
      "explanation": "find . -type d (find directories) -exec sh -c '...' _ {} \\; (execute shell command for each directory). Inside: echo directory name, find files with -maxdepth 1 (current level only), ls -lh (long format, human readable), sort -k5 -hr (sort by 5th column - size, human-numeric, reverse), head -1 (show largest)",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze disk usage by finding largest files in each directory"
    },
    {
      "id": 7,
      "command": [
        "find"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "security",
      "tags": [
        "permissions",
        "security",
        "writable"
      ],
      "task": "Find files with world-writable permissions (security risk)",
      "solution": "find . -type f -perm -o+w -ls",
      "explanation": "find . -type f (find files) -perm -o+w (permission test: 'other' users have write permission, - means 'at least these permissions') -ls (detailed listing showing permissions, owner, size, date). World-writable files are security risks as any user can modify them.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security audit to find files that could be modified by any user"
    },
    {
      "id": 8,
      "command": [
        "find",
        "tar",
        "xargs"
      ],
      "difficulty": 4,
      "rating": 3,
      "category": "backup",
      "tags": [
        "logs",
        "archive",
        "compression"
      ],
      "task": "Find log files older than 30 days and compress them into archive",
      "solution": "find . -name \"*.log\" -mtime +30 -print0 | xargs -0 tar -czf old_logs_$(date +%Y%m%d).tar.gz",
      "explanation": "find . -name '*.log' -mtime +30 (log files older than 30 days) -print0 (null-separated output for filenames with spaces) | xargs -0 (process null-separated input) tar -czf (create, gzip compress, filename) old_logs_$(date +%Y%m%d).tar.gz (timestamped archive name)",
      "execution_time": "1-5 min",
      "requirements": [
        "tar"
      ],
      "warnings": null,
      "use_case": "Automated log rotation and archiving for disk space management"
    },
    {
      "id": 9,
      "command": [
        "find",
        "grep"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "file management",
      "tags": [
        "special characters",
        "encoding",
        "spaces"
      ],
      "task": "Find files with spaces or non-ASCII characters in their names",
      "solution": "find . -name \"* *\" -type f; find . -type f | grep -P '[^\\x00-\\x7F]'",
      "explanation": "Two commands: 1) find . -name '* *' -type f (files with spaces in names using wildcard pattern) 2) find . -type f | grep -P '[^\\x00-\\x7F]' (pipe filenames to grep with Perl regex -P matching non-ASCII characters outside \\x00-\\x7F range)",
      "execution_time": "< 1 min",
      "requirements": [
        "grep with -P option"
      ],
      "warnings": null,
      "use_case": "Identify problematic filenames before transferring to different systems"
    },
    {
      "id": 10,
      "command": [
        "find",
        "inotifywait"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "monitoring",
      "tags": [
        "real-time",
        "monitoring",
        "python"
      ],
      "task": "Monitor directory for newly created Python files in real-time",
      "solution": "inotifywait -m -r -e create --format '%w%f' . | grep '\\.py$'",
      "explanation": "inotifywait -m (monitor continuously) -r (recursive) -e create (watch for file creation events) --format '%w%f' (output format: watch path + filename) . (current directory) | grep '\\.py$' (filter for files ending with .py using regex)",
      "execution_time": "long-running",
      "requirements": [
        "inotify-tools"
      ],
      "warnings": null,
      "use_case": "Development environment monitoring for automatic code processing"
    },
    {
      "id": 11,
      "command": [
        "grep"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "text processing",
      "tags": [
        "email",
        "regex",
        "validation"
      ],
      "task": "Find all email addresses in text files",
      "solution": "grep -r -E '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' .",
      "explanation": "grep -r (recursive search) -E (extended regex) with email pattern: [a-zA-Z0-9._%+-]+ (username: letters, numbers, dots, underscores, percent, plus, hyphen) @ (literal at symbol) [a-zA-Z0-9.-]+ (domain: letters, numbers, dots, hyphens) \\. (literal dot) [a-zA-Z]{2,} (domain extension: 2+ letters)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract contact information from documents or logs"
    },
    {
      "id": 12,
      "command": [
        "grep",
        "sort",
        "uniq"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "text processing",
      "tags": [
        "ip",
        "logs",
        "frequency"
      ],
      "task": "Find most frequent IP addresses in log files",
      "solution": "grep -o -E '([0-9]{1,3}\\.){3}[0-9]{1,3}' *.log | sort | uniq -c | sort -nr | head -10",
      "explanation": "grep -o (only output matching parts) -E (extended regex) '([0-9]{1,3}\\.){3}[0-9]{1,3}' (IP pattern: 1-3 digits + dot, repeated 3 times, then 1-3 digits) *.log | sort | uniq -c (count occurrences) | sort -nr (numeric reverse sort) | head -10 (top 10)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze web server logs to identify top visitors or potential attacks"
    },
    {
      "id": 13,
      "command": [
        "grep"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "text processing",
      "tags": [
        "multiline",
        "context",
        "code"
      ],
      "task": "Find function definitions in Python files with 3 lines of context",
      "solution": "grep -r -n -A 3 -B 1 '^def ' --include='*.py' .",
      "explanation": "grep -r (recursive) -n (show line numbers) -A 3 (3 lines after match) -B 1 (1 line before match) '^def ' (lines starting with 'def ' - Python function definitions) --include='*.py' (only search Python files) . (current directory)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Code review and documentation generation from Python projects"
    },
    {
      "id": 14,
      "command": [
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "csv",
        "sum",
        "column"
      ],
      "task": "Calculate sum of values in specific column of CSV file",
      "solution": "awk -F',' '{sum += $3} END {print \"Total:\", sum}' data.csv",
      "explanation": "awk -F',' (field separator is comma for CSV) '{sum += $3}' (add 3rd column value to sum variable for each line) END {print 'Total:', sum} (after processing all lines, print the total sum)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quick financial calculations from CSV exports"
    },
    {
      "id": 15,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "csv",
        "groupby",
        "statistics"
      ],
      "task": "Group CSV data by column and calculate average for each group",
      "solution": "awk -F',' '{sum[$1] += $2; count[$1]++} END {for (i in sum) print i, sum[i]/count[i]}' data.csv",
      "explanation": "awk -F',' (CSV delimiter) '{sum[$1] += $2; count[$1]++}' (use 1st column as array key, accumulate 2nd column values and count occurrences) END {for (i in sum) print i, sum[i]/count[i]} (iterate through groups, calculate and print averages)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze sales data by region or category with averages"
    },
    {
      "id": 16,
      "command": [
        "awk"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "text processing",
      "tags": [
        "whitespace",
        "cleanup",
        "formatting"
      ],
      "task": "Remove duplicate consecutive blank lines from text file",
      "solution": "awk '/^$/ {if (!blank) print; blank=1; next} {blank=0; print}' file.txt",
      "explanation": "awk with pattern-action: '/^$/' (empty line pattern) {if (!blank) print; blank=1; next} (if not already in blank state, print line and set blank flag, skip to next line) {blank=0; print} (for non-empty lines, reset blank flag and print). This preserves single blank lines but removes consecutive ones.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Clean up text files or code by removing excessive blank lines"
    },
    {
      "id": 17,
      "command": [
        "sed"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "text processing",
      "tags": [
        "replacement",
        "email",
        "privacy"
      ],
      "task": "Replace all email addresses with 'REDACTED' in text files",
      "solution": "sed -E 's/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}/REDACTED/g' file.txt",
      "explanation": "sed -E (extended regex mode) 's/pattern/replacement/g' (substitute command with global flag). Pattern matches email addresses: [a-zA-Z0-9._%+-]+ (username part) @ [a-zA-Z0-9.-]+ (domain) \\. [a-zA-Z]{2,} (extension). Replaces all matches with 'REDACTED'.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Anonymize documents before sharing by removing email addresses"
    },
    {
      "id": 18,
      "command": [
        "sed"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": [
        "extraction",
        "lines",
        "range"
      ],
      "task": "Extract lines between two patterns from a file",
      "solution": "sed -n '/START_PATTERN/,/END_PATTERN/p' file.txt",
      "explanation": "sed -n (suppress default output) '/START_PATTERN/,/END_PATTERN/p' (address range from first pattern to second pattern, with print command). This extracts all lines between and including the START_PATTERN and END_PATTERN lines.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract specific sections from configuration files or logs"
    },
    {
      "id": 19,
      "command": [
        "sed"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "text processing",
      "tags": [
        "json",
        "formatting",
        "pretty"
      ],
      "task": "Pretty print JSON by adding proper indentation",
      "solution": "sed 's/,/,\\n/g; s/{/{\\n/g; s/}/\\n}/g' file.json | sed 's/^/  /' | sed '/^\\s*$/d'",
      "explanation": "Three sed commands piped: 1) s/,/,\\n/g; s/{/{\\n/g; s/}/\\n}/g (add newlines after commas, opening and closing braces) 2) s/^/  / (add 2-space indentation to each line) 3) /^\\s*$/d (delete empty lines). This creates basic JSON pretty-printing.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Format minified JSON for better readability during debugging"
    },
    {
      "id": 20,
      "command": [
        "find",
        "grep",
        "awk"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "development",
      "tags": [
        "code analysis",
        "complexity",
        "python"
      ],
      "task": "Find Python files and calculate average lines of code per function",
      "solution": "find . -name '*.py' -exec grep -c '^def ' {} \\; | paste <(find . -name '*.py') - | awk '{cmd=\"wc -l < \"$1; cmd | getline lines; close(cmd); if($2>0) print $1, lines/$2; else print $1, lines}' | awk '{sum+=$2; count++} END {print \"Average LOC per function:\", sum/count}'",
      "explanation": "Complex pipeline: 1) Count 'def' lines per file 2) paste filenames with counts 3) First awk: get line count with wc, calculate lines/functions ratio 4) Second awk: calculate overall average. Uses getline to execute shell commands from within awk.",
      "execution_time": "1-5 min",
      "requirements": [
        "python"
      ],
      "warnings": null,
      "use_case": "Code quality analysis to identify overly complex functions"
    },
    {
      "id": 21,
      "command": [
        "sort",
        "uniq"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "data analysis",
      "tags": [
        "duplicates",
        "count",
        "frequency"
      ],
      "task": "Find and count duplicate lines in a file",
      "solution": "sort file.txt | uniq -d -c | sort -nr",
      "explanation": "sort file.txt (sort lines alphabetically - required for uniq) | uniq -d -c (show only duplicate lines -d with count -c) | sort -nr (sort by count numerically in reverse order to show most frequent first)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify repeated entries in data files or logs"
    },
    {
      "id": 22,
      "command": [
        "cut",
        "sort",
        "uniq"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "data analysis",
      "tags": [
        "csv",
        "unique",
        "column"
      ],
      "task": "Extract unique values from specific column in CSV file",
      "solution": "cut -d',' -f3 data.csv | sort | uniq",
      "explanation": "cut -d',' (delimiter is comma) -f3 (extract 3rd field/column) data.csv | sort (sort extracted values) | uniq (remove duplicate lines, showing only unique values). Cut extracts columns, sort prepares for uniq which requires sorted input.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze data categories or enumerate possible values in datasets"
    },
    {
      "id": 23,
      "command": [
        "tr",
        "sort",
        "uniq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": [
        "word count",
        "frequency",
        "analysis"
      ],
      "task": "Find most frequently used words in text file",
      "solution": "tr -cs 'A-Za-z' '\\n' < file.txt | sort | uniq -c | sort -nr | head -10",
      "explanation": "tr -cs 'A-Za-z' '\\n' (translate: -c complement set, -s squeeze, replace non-letters with newlines, one word per line) < file.txt | sort | uniq -c (count word frequencies) | sort -nr (sort by count, descending) | head -10 (top 10 words)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Text analysis for content writing or document summarization"
    },
    {
      "id": 24,
      "command": [
        "wc",
        "find"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": [
        "statistics",
        "code",
        "lines"
      ],
      "task": "Count total lines of code in all Python files",
      "solution": "find . -name '*.py' -exec wc -l {} + | tail -1",
      "explanation": "find . -name '*.py' (find Python files) -exec wc -l {} + (count lines in batches using + instead of \\; for efficiency) | tail -1 (last line contains total count). The + operator passes multiple files to single wc command, and wc automatically provides a total.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Project size estimation and development progress tracking"
    },
    {
      "id": 25,
      "command": [
        "grep",
        "awk",
        "sort"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "logs",
        "errors",
        "analysis"
      ],
      "task": "Analyze error patterns in log files and show top 5 error types",
      "solution": "grep -i error *.log | awk -F: '{print $3}' | sort | uniq -c | sort -nr | head -5",
      "explanation": "grep -i error *.log (case-insensitive search for 'error' in log files) | awk -F: '{print $3}' (split by colon, print 3rd field - typically the error message) | sort | uniq -c (count unique error types) | sort -nr (sort by frequency) | head -5 (top 5)",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "System monitoring and troubleshooting by identifying common errors"
    },
    {
      "id": 26,
      "command": [
        "find",
        "dcmdump"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "metadata",
        "inspection"
      ],
      "task": "Find all DICOM files and display their basic metadata",
      "solution": "find . -name \"*.dcm\" -o -name \"*.dicom\" | head -5 | xargs -I {} dcmdump +P 0008,0020 +P 0010,0010 +P 0008,0060 {}",
      "explanation": "find . -name '*.dcm' -o -name '*.dicom' (find DICOM files with common extensions, -o means OR) | head -5 (limit to 5 files for quick preview) | xargs -I {} (replace {} with each filename) dcmdump +P 0008,0020 +P 0010,0010 +P 0008,0060 {} (extract specific DICOM tags: study date, patient name, modality)",
      "execution_time": "< 1 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Quick inspection of DICOM study dates, patient names, and modalities"
    },
    {
      "id": 27,
      "command": [
        "find",
        "dcmdump",
        "grep"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "patient",
        "privacy"
      ],
      "task": "Find DICOM files containing specific patient information",
      "solution": "find . -name \"*.dcm\" -exec dcmdump {} \\; | grep -B5 -A5 \"Patient.*John.*Doe\"",
      "explanation": "find . -name '*.dcm' -exec dcmdump {} \\; (dump DICOM metadata for each file) | grep -B5 -A5 'Patient.*John.*Doe' (search for patient name pattern with 5 lines before and after for context). The .* allows for flexible matching of patient name formats.",
      "execution_time": "1-5 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": "Handle patient data according to HIPAA compliance",
      "use_case": "Locate specific patient studies for review or analysis"
    },
    {
      "id": 28,
      "command": [
        "find",
        "dcmdump",
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "modality",
        "statistics"
      ],
      "task": "Count DICOM files by imaging modality (CT, MRI, X-Ray, etc.)",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0008,0060 {} \\; | grep \"Modality\" | awk '{print $3}' | sort | uniq -c | sort -nr",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0008,0060 {} \\; (extract modality tag 0008,0060 from each DICOM file) | grep 'Modality' (filter modality lines) | awk '{print $3}' (extract modality value) | sort | uniq -c (count occurrences) | sort -nr (sort by count)",
      "execution_time": "1-5 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Analyze medical imaging dataset composition for research planning"
    },
    {
      "id": 29,
      "command": [
        "find",
        "dcmconv"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "conversion",
        "format"
      ],
      "task": "Convert all DICOM files to different transfer syntax",
      "solution": "find . -name \"*.dcm\" -exec dcmconv +ti {} {}.new \\; && find . -name \"*.dcm.new\" -exec sh -c 'mv \"$1\" \"${1%.new}\"' _ {} \\;",
      "explanation": "Two-step process: 1) find . -name '*.dcm' -exec dcmconv +ti {} {}.new \\; (convert each DICOM file to implicit transfer syntax, creating .new files) && 2) find . -name '*.dcm.new' -exec sh -c 'mv \"$1\" \"${1%.new}\"' _ {} \\; (rename .new files back to original names, ${1%.new} removes .new suffix)",
      "execution_time": "5+ min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": "Backup original files before conversion",
      "use_case": "Standardize DICOM transfer syntax for compatibility with specific systems"
    },
    {
      "id": 30,
      "command": [
        "find",
        "dcmdump",
        "grep",
        "cut"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "series",
        "organization"
      ],
      "task": "Extract and organize DICOM files by Series Instance UID",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'series=$(dcmdump +P 0020,000e \"$1\" | grep SeriesInstanceUID | cut -d\"[\" -f2 | cut -d\"]\" -f1); mkdir -p \"series_$series\" && cp \"$1\" \"series_$series/\"' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (execute shell script for each DICOM file). Script: extract SeriesInstanceUID using dcmdump, grep to find the line, cut to extract the value between brackets, create directory named series_$series, copy file to that directory. This organizes mixed DICOM files by series.",
      "execution_time": "5+ min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Organize mixed DICOM files into separate series directories for analysis"
    },
    {
      "id": 31,
      "command": [
        "find",
        "dcmdump",
        "grep"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "validation",
        "corruption"
      ],
      "task": "Find corrupted or invalid DICOM files",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'dcmdump \"$1\" >/dev/null 2>&1 || echo \"Corrupted: $1\"' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (execute shell command for each DICOM file). Command: dcmdump '$1' >/dev/null 2>&1 (try to dump DICOM data, suppress all output) || echo 'Corrupted: $1' (if dcmdump fails, print filename as corrupted). Uses logical OR to execute echo only on failure.",
      "execution_time": "1-5 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Quality control to identify damaged DICOM files before processing"
    },
    {
      "id": 32,
      "command": [
        "find",
        "dcm2niix"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "nifti",
        "conversion"
      ],
      "task": "Convert DICOM series to NIfTI format for neuroimaging analysis",
      "solution": "find . -type d -exec sh -c 'if ls \"$1\"/*.dcm >/dev/null 2>&1; then dcm2niix -o nifti_output \"$1\"; fi' _ {} \\;",
      "explanation": "find . -type d (find directories) -exec sh -c '...' _ {} \\; (execute shell script for each directory). Script: if ls '$1'/*.dcm >/dev/null 2>&1 (check if directory contains .dcm files, suppress output) then dcm2niix -o nifti_output '$1' (convert DICOM series to NIfTI format in output directory). Only processes directories with DICOM files.",
      "execution_time": "5+ min",
      "requirements": [
        "dcm2niix"
      ],
      "warnings": null,
      "use_case": "Prepare DICOM data for neuroimaging analysis tools like FSL or SPM"
    },
    {
      "id": 33,
      "command": [
        "find",
        "dcmdump",
        "sort"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "temporal",
        "sorting"
      ],
      "task": "Sort DICOM files by acquisition date and time",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'echo \"$(dcmdump +P 0008,0022 +P 0008,0032 \"$1\" | grep -E \"AcquisitionDate|AcquisitionTime\" | tr \"\\n\" \" \") $1\"' _ {} \\; | sort",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (process each DICOM file). Script: dcmdump +P 0008,0022 +P 0008,0032 (extract acquisition date and time tags) | grep -E 'AcquisitionDate|AcquisitionTime' (filter relevant lines) | tr '\\n' ' ' (join date and time on one line) then echo with filename | sort (sort by date/time)",
      "execution_time": "1-5 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Temporal analysis of medical imaging studies or longitudinal research"
    },
    {
      "id": 34,
      "command": [
        "find",
        "dcmodify"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "anonymization",
        "privacy"
      ],
      "task": "Anonymize DICOM files by removing patient identifying information",
      "solution": "find . -name \"*.dcm\" -exec dcmodify -gin -nb -ea \"(0010,0010)=ANONYMOUS\" -ea \"(0010,0020)=ANON$(date +%s)\" {} \\;",
      "explanation": "find . -name '*.dcm' -exec dcmodify (DICOM modification tool) -gin (generate new instance UIDs) -nb (no backup) -ea '(0010,0010)=ANONYMOUS' (replace patient name with ANONYMOUS) -ea '(0010,0020)=ANON$(date +%s)' (replace patient ID with ANON + timestamp) {} \\; (process each file). Creates anonymized versions in-place.",
      "execution_time": "5+ min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": "Ensure compliance with medical data protection regulations",
      "use_case": "Prepare medical imaging data for research sharing while protecting patient privacy"
    },
    {
      "id": 35,
      "command": [
        "find",
        "dcmdump",
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "slice",
        "thickness"
      ],
      "task": "Calculate average slice thickness across all DICOM images",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0018,0050 {} \\; | grep \"SliceThickness\" | awk -F\"[\" '{print $2}' | awk -F\"]\" '{sum+=$1; count++} END {if(count>0) print \"Average slice thickness:\", sum/count \"mm\"}'",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0018,0050 {} \\; (extract slice thickness tag) | grep 'SliceThickness' (filter relevant lines) | awk -F'[' '{print $2}' (split by '[', get value part) | awk -F']' '{sum+=$1; count++} END {...} (split by ']', accumulate values, calculate average)",
      "execution_time": "< 1 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Quality assessment of imaging protocols for research standardization"
    },
    {
      "id": 36,
      "command": [
        "find",
        "dcmdump",
        "grep",
        "wc"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "contrast",
        "enhancement"
      ],
      "task": "Count DICOM images with contrast enhancement",
      "solution": "find . -name \"*.dcm\" -exec dcmdump {} \\; | grep -i \"contrast\" | wc -l",
      "explanation": "find . -name '*.dcm' -exec dcmdump {} \\; (dump all DICOM metadata) | grep -i 'contrast' (case-insensitive search for contrast-related information) | wc -l (count matching lines). This searches for any mention of contrast in DICOM metadata fields.",
      "execution_time": "1-5 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Analyze dataset composition for contrast-enhanced vs non-enhanced studies"
    },
    {
      "id": 37,
      "command": [
        "find",
        "dcmdump"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "manufacturer",
        "equipment"
      ],
      "task": "List all imaging equipment manufacturers in DICOM dataset",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0008,0070 {} \\; | grep \"Manufacturer\" | sort | uniq",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0008,0070 {} \\; (extract manufacturer tag 0008,0070 from each DICOM file) | grep 'Manufacturer' (filter manufacturer lines) | sort | uniq (sort and remove duplicates to get unique manufacturer list)",
      "execution_time": "< 1 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Equipment inventory and vendor analysis for medical imaging department"
    },
    {
      "id": 38,
      "command": [
        "find",
        "dcmj2pnm"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "preview",
        "conversion"
      ],
      "task": "Generate PNG preview images from DICOM files for quick review",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'dcmj2pnm \"$1\" \"${1%.dcm}.png\" --write-png' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (process each DICOM file). Script: dcmj2pnm '$1' '${1%.dcm}.png' --write-png (convert DICOM to PNG format, ${1%.dcm} removes .dcm extension and adds .png). Creates PNG preview for each DICOM image.",
      "execution_time": "5+ min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Create visual previews of medical images for quick dataset review"
    },
    {
      "id": 39,
      "command": [
        "find",
        "dcmdump",
        "grep"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "body part",
        "anatomy"
      ],
      "task": "Find DICOM images of specific body parts or anatomical regions",
      "solution": "find . -name \"*.dcm\" -exec sh -c 'if dcmdump \"$1\" | grep -i \"BodyPartExamined.*CHEST\\|StudyDescription.*chest\"; then echo \"$1\"; fi' _ {} \\;",
      "explanation": "find . -name '*.dcm' -exec sh -c '...' _ {} \\; (process each DICOM file). Script: if dcmdump '$1' | grep -i 'BodyPartExamined.*CHEST\\|StudyDescription.*chest' (search for chest in body part or study description fields, case-insensitive) then echo '$1' (print filename if match found). Uses alternation with \\| for multiple patterns.",
      "execution_time": "1-5 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": null,
      "use_case": "Filter medical imaging datasets by anatomical region for targeted analysis"
    },
    {
      "id": 40,
      "command": [
        "find",
        "dcmdump",
        "awk"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "medical imaging",
      "tags": [
        "dicom",
        "dose",
        "radiation"
      ],
      "task": "Calculate total radiation dose across all CT DICOM studies",
      "solution": "find . -name \"*.dcm\" -exec dcmdump +P 0018,9345 {} \\; | grep \"CTDIvol\" | awk -F\"[\" '{print $2}' | awk -F\"]\" '{sum+=$1; count++} END {if(count>0) print \"Total CTDIvol:\", sum \"mGy across\", count \"images\"}'",
      "explanation": "find . -name '*.dcm' -exec dcmdump +P 0018,9345 {} \\; (extract CTDIvol tag - radiation dose index) | grep 'CTDIvol' (filter dose lines) | awk -F'[' '{print $2}' (extract value between brackets) | awk -F']' '{sum+=$1; count++} END {...} (accumulate dose values and count, print total). CTDIvol is CT dose index volume in mGy.",
      "execution_time": "1-5 min",
      "requirements": [
        "dcmtk"
      ],
      "warnings": "Radiation dose analysis requires medical physics expertise",
      "use_case": "Radiation dose monitoring and optimization for patient safety compliance"
    },
    {
      "id": 41,
      "command": [
        "jq"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": [
        "json",
        "parsing",
        "extraction"
      ],
      "task": "Extract all email addresses from a JSON file containing user data",
      "solution": "jq -r '.users[].email' data.json",
      "explanation": "jq -r (raw output without quotes) '.users[]' (iterate through all items in users array) '.email' (extract email field from each user object). The [] operator flattens the array to process each element individually.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Extract contact information from API responses or user databases"
    },
    {
      "id": 42,
      "command": [
        "jq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "json",
        "filtering",
        "conditional"
      ],
      "task": "Filter JSON array to show only items where price is greater than 100",
      "solution": "jq '.products[] | select(.price > 100)' catalog.json",
      "explanation": "jq '.products[]' (iterate through products array) | select(.price > 100) (filter items where price field is greater than 100). The select() function acts as a conditional filter, only passing through objects that meet the criteria.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Filter product catalogs or datasets based on numeric criteria"
    },
    {
      "id": 43,
      "command": [
        "jq"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "groupby",
        "statistics"
      ],
      "task": "Group JSON data by category and calculate average price for each group",
      "solution": "jq 'group_by(.category) | map({category: .[0].category, avg_price: (map(.price) | add / length)})' products.json",
      "explanation": "jq 'group_by(.category)' (group array items by category field) | map({...}) (transform each group into new object) .[0].category (get category from first item in group) map(.price) | add / length (extract prices, sum them, divide by count for average). Complex aggregation with grouping.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Analyze e-commerce data or create summary statistics from JSON datasets"
    },
    {
      "id": 44,
      "command": [
        "jq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "json",
        "nested",
        "flattening"
      ],
      "task": "Flatten nested JSON structure and extract specific nested fields",
      "solution": "jq '.orders[] | {order_id: .id, customer: .customer.name, total: .billing.total}' orders.json",
      "explanation": "jq '.orders[]' (iterate through orders array) | {...} (create new object with specified fields) .customer.name (access nested field using dot notation) .billing.total (access deeply nested field). This flattens complex nested structures into simpler objects.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Simplify complex JSON structures for analysis or reporting"
    },
    {
      "id": 45,
      "command": [
        "jq",
        "curl"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "web",
      "tags": [
        "api",
        "json",
        "monitoring"
      ],
      "task": "Monitor API endpoint and extract error responses in real-time",
      "solution": "while true; do curl -s https://api.example.com/status | jq 'select(.status != \"ok\") | {time: now, error: .error}'; sleep 10; done",
      "explanation": "while true; do ... done (infinite loop) curl -s (silent mode) | jq 'select(.status != \"ok\")' (filter only non-ok responses) | {...} (create object with current timestamp using now and error field) sleep 10 (wait 10 seconds between checks). Continuous API monitoring with error detection.",
      "execution_time": "long-running",
      "requirements": [
        "jq",
        "curl"
      ],
      "warnings": null,
      "use_case": "API monitoring and alerting for production systems"
    },
    {
      "id": 46,
      "command": [
        "rsync"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "backup",
      "tags": [
        "sync",
        "backup",
        "incremental"
      ],
      "task": "Synchronize local directory with remote server, showing progress",
      "solution": "rsync -avz --progress /local/path/ user@server:/remote/path/",
      "explanation": "rsync -a (archive mode: recursive, preserve permissions, times, etc.) -v (verbose output) -z (compress during transfer) --progress (show transfer progress) /local/path/ (source with trailing slash for contents) user@server:/remote/path/ (SSH destination). Efficient incremental sync.",
      "execution_time": "5+ min",
      "requirements": [
        "rsync",
        "ssh"
      ],
      "warnings": null,
      "use_case": "Regular backup of project files or data to remote server"
    },
    {
      "id": 47,
      "command": [
        "rsync"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": [
        "sync",
        "exclude",
        "selective"
      ],
      "task": "Sync directory while excluding temporary files and maintaining deletions",
      "solution": "rsync -avz --delete --exclude='*.tmp' --exclude='*.log' --exclude='.git/' src/ dest/",
      "explanation": "rsync -avz (archive, verbose, compress) --delete (remove files from destination that don't exist in source) --exclude='pattern' (exclude files matching pattern, multiple excludes possible) src/ dest/ (sync source contents to destination). Selective sync with cleanup.",
      "execution_time": "1-5 min",
      "requirements": [
        "rsync"
      ],
      "warnings": "Use --delete carefully as it removes files from destination",
      "use_case": "Clean synchronization of code projects excluding temporary files"
    },
    {
      "id": 48,
      "command": [
        "rsync"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "backup",
      "tags": [
        "sync",
        "bandwidth",
        "resume"
      ],
      "task": "Resume interrupted large file transfer with bandwidth limiting",
      "solution": "rsync -avz --partial --progress --bwlimit=1000 large_dataset/ user@server:/backup/",
      "explanation": "rsync -avz (standard options) --partial (keep partially transferred files for resuming) --progress (show detailed progress) --bwlimit=1000 (limit bandwidth to 1000 KB/s) large_dataset/ (source) user@server:/backup/ (remote destination). Resumable transfers with traffic control.",
      "execution_time": "long-running",
      "requirements": [
        "rsync",
        "ssh"
      ],
      "warnings": null,
      "use_case": "Transfer large datasets over limited bandwidth connections"
    },
    {
      "id": 49,
      "command": [
        "tar"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "backup",
      "tags": [
        "archive",
        "compression",
        "backup"
      ],
      "task": "Create compressed archive of directory with current date in filename",
      "solution": "tar -czf backup_$(date +%Y%m%d).tar.gz /path/to/directory",
      "explanation": "tar -c (create archive) -z (gzip compression) -f (specify filename) backup_$(date +%Y%m%d).tar.gz (filename with command substitution for date in YYYYMMDD format) /path/to/directory (directory to archive). Creates timestamped backups.",
      "execution_time": "1-5 min",
      "requirements": [
        "tar"
      ],
      "warnings": null,
      "use_case": "Automated daily backups with timestamped archive names"
    },
    {
      "id": 50,
      "command": [
        "tar"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "backup",
      "tags": [
        "archive",
        "exclude",
        "selective"
      ],
      "task": "Create archive excluding specific file patterns and showing progress",
      "solution": "tar -czf archive.tar.gz --exclude='*.log' --exclude='node_modules' --verbose project/",
      "explanation": "tar -czf archive.tar.gz (create compressed archive) --exclude='pattern' (exclude files matching glob pattern, can use multiple times) --verbose (show files being processed) project/ (source directory). Selective archiving with visual feedback.",
      "execution_time": "1-5 min",
      "requirements": [
        "tar"
      ],
      "warnings": null,
      "use_case": "Archive projects while excluding unnecessary files like logs and dependencies"
    },
    {
      "id": 51,
      "command": [
        "tar",
        "find"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "backup",
      "tags": [
        "archive",
        "incremental",
        "modified"
      ],
      "task": "Create incremental backup of files modified in last 7 days",
      "solution": "find /path/to/backup -type f -mtime -7 | tar -czf incremental_$(date +%Y%m%d).tar.gz -T -",
      "explanation": "find /path/to/backup -type f -mtime -7 (find files modified within 7 days) | tar -czf incremental_$(date +%Y%m%d).tar.gz (create timestamped compressed archive) -T - (read file list from stdin using dash). Efficient incremental backups using file modification times.",
      "execution_time": "1-5 min",
      "requirements": [
        "tar",
        "find"
      ],
      "warnings": null,
      "use_case": "Efficient incremental backups focusing only on recently changed files"
    },
    {
      "id": 52,
      "command": [
        "curl"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "web",
      "tags": [
        "api",
        "json",
        "authentication"
      ],
      "task": "Make authenticated API request and save response to file",
      "solution": "curl -H \"Authorization: Bearer $TOKEN\" -H \"Content-Type: application/json\" https://api.example.com/data -o response.json",
      "explanation": "curl -H 'Authorization: Bearer $TOKEN' (add auth header with environment variable) -H 'Content-Type: application/json' (specify content type header) https://api.example.com/data (API endpoint) -o response.json (save output to file). Standard authenticated API request pattern.",
      "execution_time": "< 1 min",
      "requirements": [
        "curl"
      ],
      "warnings": null,
      "use_case": "Fetch data from authenticated APIs for analysis or integration"
    },
    {
      "id": 53,
      "command": [
        "curl",
        "jq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "web",
      "tags": [
        "api",
        "monitoring",
        "health"
      ],
      "task": "Check API health status and alert if service is down",
      "solution": "response=$(curl -s -w \"%{http_code}\" https://api.example.com/health); if [[ $response != *\"200\" ]]; then echo \"API DOWN: $response\"; fi",
      "explanation": "response=$(curl -s -w '%{http_code}' url) (capture response with HTTP status code, -s for silent, -w for write-out format) if [[ $response != *'200' ]] (check if response doesn't contain '200') then echo 'API DOWN: $response' (alert with status). Health check with conditional alerting.",
      "execution_time": "< 1 min",
      "requirements": [
        "curl"
      ],
      "warnings": null,
      "use_case": "Automated health checks for web services and APIs"
    },
    {
      "id": 54,
      "command": [
        "curl"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": [
        "download",
        "parallel",
        "batch"
      ],
      "task": "Download multiple files in parallel with retry logic",
      "solution": "cat urls.txt | xargs -n 1 -P 5 -I {} sh -c 'curl -L --retry 3 --retry-delay 2 -O {}'",
      "explanation": "cat urls.txt (read URL list) | xargs -n 1 (one URL per command) -P 5 (run 5 processes in parallel) -I {} (placeholder for URL) sh -c 'curl -L (follow redirects) --retry 3 (retry 3 times on failure) --retry-delay 2 (wait 2 seconds between retries) -O {} (download with original filename)'. Robust parallel downloading.",
      "execution_time": "5+ min",
      "requirements": [
        "curl",
        "xargs"
      ],
      "warnings": null,
      "use_case": "Batch download of datasets or media files with fault tolerance"
    },
    {
      "id": 55,
      "command": [
        "parallel"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "automation",
      "tags": [
        "parallel",
        "processing",
        "efficiency"
      ],
      "task": "Process multiple files in parallel using all CPU cores",
      "solution": "parallel -j+0 'process_file {}' ::: *.txt",
      "explanation": "parallel -j+0 (use all available CPU cores, +0 means number of cores) 'process_file {}' (command template with {} placeholder for filename) ::: *.txt (input sources - all .txt files). GNU parallel automatically distributes work across cores for maximum efficiency.",
      "execution_time": "1-5 min",
      "requirements": [
        "parallel"
      ],
      "warnings": null,
      "use_case": "Speed up data processing by utilizing all available CPU cores"
    },
    {
      "id": 56,
      "command": [
        "parallel",
        "convert"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "multimedia",
      "tags": [
        "parallel",
        "images",
        "batch"
      ],
      "task": "Resize all images in directory using parallel processing",
      "solution": "parallel -j+0 'convert {} -resize 800x600 resized_{}' ::: *.jpg",
      "explanation": "parallel -j+0 (all CPU cores) 'convert {} -resize 800x600 resized_{}' (ImageMagick convert command: resize to 800x600 pixels, prefix output with 'resized_') ::: *.jpg (process all JPEG files). Parallel image processing for significant speed improvement over sequential processing.",
      "execution_time": "5+ min",
      "requirements": [
        "parallel",
        "imagemagick"
      ],
      "warnings": null,
      "use_case": "Batch image processing for web optimization or dataset preparation"
    },
    {
      "id": 57,
      "command": [
        "parallel",
        "gzip"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": [
        "parallel",
        "compression",
        "logs"
      ],
      "task": "Compress log files in parallel while preserving directory structure",
      "solution": "find /var/log -name '*.log' -print0 | parallel -0 -j+0 'gzip {}'",
      "explanation": "find /var/log -name '*.log' -print0 (find log files, output null-separated for filenames with spaces) | parallel -0 (read null-separated input) -j+0 (all cores) 'gzip {}' (compress each file in place). Parallel compression maintains directory structure and handles special characters in filenames.",
      "execution_time": "1-5 min",
      "requirements": [
        "parallel",
        "gzip"
      ],
      "warnings": "Ensure log files are not actively being written to",
      "use_case": "Efficient log rotation and compression for system maintenance"
    },
    {
      "id": 58,
      "command": [
        "watch"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "monitoring",
        "real-time",
        "disk"
      ],
      "task": "Monitor disk usage in real-time with color highlighting",
      "solution": "watch -c -n 2 'df -h | grep -E \"(Filesystem|/dev/)\"'",
      "explanation": "watch -c (enable color output) -n 2 (refresh every 2 seconds) 'df -h (human-readable disk usage) | grep -E '(Filesystem|/dev/)' (show header and only real filesystems, exclude tmpfs/proc)'. Real-time monitoring with filtered output and color highlighting for easy reading.",
      "execution_time": "long-running",
      "requirements": [
        "watch"
      ],
      "warnings": null,
      "use_case": "Real-time monitoring of disk space during large file operations"
    },
    {
      "id": 59,
      "command": [
        "watch",
        "ps"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "monitoring",
        "processes",
        "memory"
      ],
      "task": "Monitor memory usage of specific process in real-time",
      "solution": "watch -n 1 'ps aux | grep python | grep -v grep | sort -k4 -nr'",
      "explanation": "watch -n 1 (refresh every second) 'ps aux (all processes with detailed info) | grep python (filter Python processes) | grep -v grep (exclude grep itself) | sort -k4 -nr (sort by 4th column - memory usage, numeric reverse)'. Real-time process monitoring with automatic filtering and sorting.",
      "execution_time": "long-running",
      "requirements": [
        "watch",
        "ps"
      ],
      "warnings": null,
      "use_case": "Track memory consumption of running applications for performance tuning"
    },
    {
      "id": 60,
      "command": [
        "git",
        "grep"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "version control",
      "tags": [
        "git",
        "search",
        "history"
      ],
      "task": "Search for specific text pattern across entire Git history",
      "solution": "git log -S \"search_pattern\" --source --all --oneline",
      "explanation": "git log -S 'search_pattern' (pickaxe search - find commits that add or remove the specified string) --source (show which ref/branch) --all (search all branches and tags) --oneline (compact output format). Searches the actual content changes, not just commit messages.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Track when specific code patterns or functions were introduced or removed"
    },
    {
      "id": 61,
      "command": [
        "git",
        "find"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "version control",
      "tags": [
        "git",
        "cleanup",
        "large files"
      ],
      "task": "Find and remove large files from Git history to reduce repository size",
      "solution": "git rev-list --objects --all | git cat-file --batch-check='%(objecttype) %(objectname) %(objectsize) %(rest)' | grep '^blob' | sort -k3 -nr | head -10",
      "explanation": "git rev-list --objects --all (list all objects in repository) | git cat-file --batch-check='...' (get object info: type, hash, size, name) | grep '^blob' (filter to files only) | sort -k3 -nr (sort by size column, descending) | head -10 (show top 10 largest). Identifies large files for repository cleanup.",
      "execution_time": "1-5 min",
      "requirements": [
        "git"
      ],
      "warnings": "Rewriting Git history affects all collaborators",
      "use_case": "Repository maintenance and size optimization for faster clones"
    },
    {
      "id": 62,
      "command": [
        "git",
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "version control",
      "tags": [
        "git",
        "statistics",
        "contributors"
      ],
      "task": "Generate contributor statistics showing lines added/removed per author",
      "solution": "git log --numstat --pretty=\"%ae\" | awk '/^[0-9]/ {add+=$1; del+=$2} /^[a-zA-Z]/ {print; if(add+del>0) print \"  +\" add \" -\" del; add=del=0}'",
      "explanation": "git log --numstat (show added/deleted lines per file) --pretty='%ae' (show author email) | awk with pattern-action: '/^[0-9]/' (lines starting with numbers - file stats) accumulate additions/deletions '/^[a-zA-Z]/' (author email lines) print stats and reset counters. Generates detailed contributor statistics.",
      "execution_time": "< 1 min",
      "requirements": [
        "git",
        "awk"
      ],
      "warnings": null,
      "use_case": "Project analysis and contributor recognition for team management"
    },
    {
      "id": 63,
      "command": [
        "ssh"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "ssh",
        "tunneling",
        "port forwarding"
      ],
      "task": "Create SSH tunnel to access remote database through bastion host",
      "solution": "ssh -L 5432:database.internal:5432 user@bastion.example.com",
      "explanation": "ssh -L 5432:database.internal:5432 (local port forwarding: bind local port 5432 to database.internal:5432 through SSH connection) user@bastion.example.com (SSH to bastion host). Creates secure tunnel allowing local database connections through jump server.",
      "execution_time": "long-running",
      "requirements": [
        "ssh"
      ],
      "warnings": null,
      "use_case": "Secure access to internal databases through jump servers"
    },
    {
      "id": 64,
      "command": [
        "ssh",
        "find"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "ssh",
        "remote",
        "cleanup"
      ],
      "task": "Execute complex command on multiple remote servers simultaneously",
      "solution": "for server in server1 server2 server3; do ssh $server 'find /tmp -mtime +7 -delete' & done; wait",
      "explanation": "for server in server1 server2 server3 (iterate through server list) do ssh $server 'command' & (execute SSH command in background with &) done; wait (wait for all background jobs to complete). Parallel execution across multiple servers with synchronization.",
      "execution_time": "1-5 min",
      "requirements": [
        "ssh"
      ],
      "warnings": "Ensure SSH keys are properly configured for passwordless access",
      "use_case": "Automated maintenance tasks across multiple servers"
    },
    {
      "id": 65,
      "command": [
        "cron",
        "find"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "automation",
      "tags": [
        "cron",
        "cleanup",
        "scheduling"
      ],
      "task": "Schedule automated cleanup of old log files every night at 2 AM",
      "solution": "echo '0 2 * * * find /var/log -name \"*.log\" -mtime +30 -delete' | crontab -",
      "explanation": "echo '0 2 * * * command' (cron format: minute hour day month weekday, 0 2 * * * = 2:00 AM daily) find /var/log -name '*.log' -mtime +30 -delete (find log files older than 30 days and delete) | crontab - (install crontab from stdin). Automated log cleanup scheduling.",
      "execution_time": "< 1 min",
      "requirements": [
        "cron"
      ],
      "warnings": "Test commands manually before scheduling",
      "use_case": "Automated system maintenance and log rotation"
    },
    {
      "id": 66,
      "command": [
        "cron",
        "rsync"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "automation",
      "tags": [
        "cron",
        "backup",
        "scheduling"
      ],
      "task": "Schedule daily incremental backups with email notifications",
      "solution": "echo '0 3 * * * rsync -avz /data/ backup@server:/backups/ && echo \"Backup completed\" | mail -s \"Daily Backup Status\" admin@company.com' | crontab -",
      "explanation": "echo '0 3 * * * ...' (3:00 AM daily) rsync -avz /data/ backup@server:/backups/ (sync data to backup server) && (if successful) echo 'Backup completed' | mail -s 'Daily Backup Status' admin@company.com (send success notification) | crontab - (install cron job). Automated backup with notification.",
      "execution_time": "< 1 min",
      "requirements": [
        "cron",
        "rsync",
        "mail"
      ],
      "warnings": null,
      "use_case": "Automated data backup with monitoring and alerting"
    },
    {
      "id": 67,
      "command": [
        "diff"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "text processing",
      "tags": [
        "diff",
        "comparison",
        "changes"
      ],
      "task": "Compare two configuration files and show only the differences",
      "solution": "diff -u config.old config.new",
      "explanation": "diff -u (unified format showing context around changes with + and - prefixes) config.old config.new (compare old and new versions). Unified format is more readable than default format and shows context lines around changes for better understanding.",
      "execution_time": "< 1 min",
      "requirements": [
        "diff"
      ],
      "warnings": null,
      "use_case": "Configuration management and change tracking"
    },
    {
      "id": 68,
      "command": [
        "diff",
        "find"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "file management",
      "tags": [
        "diff",
        "directory",
        "sync"
      ],
      "task": "Find files that exist in one directory but not another",
      "solution": "diff <(find dir1 -type f | sort) <(find dir2 -type f | sort)",
      "explanation": "diff <(find dir1 -type f | sort) <(find dir2 -type f | sort) uses process substitution <() to create temporary files from command output. find dir1 -type f | sort (get sorted list of files in dir1) compared with same for dir2. Shows files unique to each directory.",
      "execution_time": "< 1 min",
      "requirements": [
        "diff",
        "find"
      ],
      "warnings": null,
      "use_case": "Directory synchronization verification and missing file detection"
    },
    {
      "id": 69,
      "command": [
        "imagemagick"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "multimedia",
      "tags": [
        "images",
        "batch",
        "conversion"
      ],
      "task": "Batch convert images to different format with quality optimization",
      "solution": "for img in *.jpg; do convert \"$img\" -quality 85 -resize 1920x1080> \"${img%.jpg}.webp\"; done",
      "explanation": "for img in *.jpg (iterate through JPEG files) convert '$img' (ImageMagick convert tool) -quality 85 (85% quality for good compression/quality balance) -resize 1920x1080> (resize only if larger, > prevents upscaling) '${img%.jpg}.webp' (remove .jpg extension, add .webp). Efficient batch conversion to modern format.",
      "execution_time": "5+ min",
      "requirements": [
        "imagemagick"
      ],
      "warnings": null,
      "use_case": "Web optimization by converting images to modern formats"
    },
    {
      "id": 70,
      "command": [
        "imagemagick",
        "find"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "multimedia",
      "tags": [
        "images",
        "metadata",
        "analysis"
      ],
      "task": "Extract and analyze image metadata from photo collection",
      "solution": "find . -name \"*.jpg\" -exec identify -verbose {} \\; | grep -E \"(Geometry|Filesize|Date)\"",
      "explanation": "find . -name '*.jpg' -exec identify -verbose {} \\; (run ImageMagick identify with verbose output on each JPEG file) | grep -E '(Geometry|Filesize|Date)' (filter for specific metadata: dimensions, file size, and date information using extended regex with alternation). Extracts key image properties.",
      "execution_time": "1-5 min",
      "requirements": [
        "imagemagick"
      ],
      "warnings": null,
      "use_case": "Photo collection analysis and metadata extraction for organization"
    },
    {
      "id": 71,
      "command": [
        "ffmpeg"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "multimedia",
      "tags": [
        "video",
        "conversion",
        "compression"
      ],
      "task": "Batch convert video files to web-optimized format",
      "solution": "for video in *.mp4; do ffmpeg -i \"$video\" -c:v libx264 -crf 23 -c:a aac \"web_${video}\"; done",
      "explanation": "for video in *.mp4 (iterate through MP4 files) ffmpeg -i '$video' (input file) -c:v libx264 (video codec: H.264) -crf 23 (constant rate factor: 23 is good quality/size balance) -c:a aac (audio codec: AAC) 'web_${video}' (output with 'web_' prefix). Optimizes videos for web streaming.",
      "execution_time": "long-running",
      "requirements": [
        "ffmpeg"
      ],
      "warnings": "Video processing is CPU intensive and time consuming",
      "use_case": "Prepare video content for web streaming with optimal file sizes"
    },
    {
      "id": 72,
      "command": [
        "ffmpeg",
        "find"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "multimedia",
      "tags": [
        "video",
        "analysis",
        "duration"
      ],
      "task": "Calculate total duration of all video files in directory tree",
      "solution": "find . -name \"*.mp4\" -exec ffprobe -v quiet -show_entries format=duration -of csv=p=0 {} \\; | awk '{sum+=$1} END {printf \"Total: %.2f hours\\n\", sum/3600}'",
      "explanation": "find . -name '*.mp4' -exec ffprobe (FFmpeg probe tool) -v quiet (suppress verbose output) -show_entries format=duration (show only duration) -of csv=p=0 (output as CSV without headers) {} \\; | awk '{sum+=$1}' (accumulate durations) END {printf ...} (convert seconds to hours). Analyzes media collection duration.",
      "execution_time": "1-5 min",
      "requirements": [
        "ffmpeg",
        "awk"
      ],
      "warnings": null,
      "use_case": "Media collection analysis and storage planning"
    },
    {
      "id": 73,
      "command": [
        "ps",
        "grep",
        "awk"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "processes",
        "memory",
        "monitoring"
      ],
      "task": "Monitor processes consuming more than 10% CPU and log them",
      "solution": "ps aux | awk '$3 > 10 {print strftime(\"%Y-%m-%d %H:%M:%S\"), $2, $3\"%\", $11}' >> high_cpu.log",
      "explanation": "ps aux (show all processes with detailed info) | awk '$3 > 10' (filter processes where 3rd column - CPU% > 10) '{print strftime('%Y-%m-%d %H:%M:%S') (format current timestamp) $2 (PID) $3'%' (CPU percentage) $11 (command)} >> high_cpu.log (append to log file). Monitors and logs high CPU usage with timestamps.",
      "execution_time": "< 1 min",
      "requirements": [
        "ps",
        "awk"
      ],
      "warnings": null,
      "use_case": "Performance monitoring and troubleshooting high CPU usage"
    },
    {
      "id": 74,
      "command": [
        "netstat",
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "network",
      "tags": [
        "network",
        "connections",
        "analysis"
      ],
      "task": "Analyze network connections and show top connecting IP addresses",
      "solution": "netstat -tn | awk '/^tcp/ {print $5}' | cut -d: -f1 | sort | uniq -c | sort -nr | head -10",
      "explanation": "netstat -tn (show TCP connections, numeric addresses) | awk '/^tcp/' (filter TCP connections) '{print $5}' (extract remote address column) | cut -d: -f1 (extract IP part before port) | sort | uniq -c (count occurrences) | sort -nr (sort by count, descending) | head -10 (top 10). Network connection analysis.",
      "execution_time": "< 1 min",
      "requirements": [
        "netstat",
        "awk"
      ],
      "warnings": null,
      "use_case": "Network security analysis and identifying connection patterns"
    },
    {
      "id": 75,
      "command": [
        "date",
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "automation",
      "tags": [
        "date",
        "calculation",
        "scheduling"
      ],
      "task": "Calculate business days between two dates excluding weekends",
      "solution": "start=$(date -d \"2024-01-01\" +%s); end=$(date -d \"2024-12-31\" +%s); echo \"scale=0; (($end - $start) / 86400) * 5/7\" | bc",
      "explanation": "start=$(date -d '2024-01-01' +%s) (convert start date to Unix timestamp) end=$(date -d '2024-12-31' +%s) (convert end date to timestamp) echo 'scale=0; (($end - $start) / 86400) * 5/7' | bc (calculate: difference in seconds / 86400 seconds per day * 5/7 for weekdays only, using bc calculator with scale=0 for integer result). Business day calculation.",
      "execution_time": "< 1 min",
      "requirements": [
        "date",
        "bc"
      ],
      "warnings": null,
      "use_case": "Project planning and deadline calculations for business scenarios"
    },
    {
      "id": 76,
      "command": [
        "ls"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "file management",
      "tags": [
        "listing",
        "permissions",
        "details"
      ],
      "task": "List files with detailed information including permissions, owner, size and modification time",
      "solution": "ls -la",
      "explanation": "ls -l (long format showing permissions, owner, group, size, date) -a (include hidden files starting with dot). The long format displays: file type/permissions, link count, owner, group, size in bytes, modification date/time, filename.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Basic file system inspection and permissions checking"
    },
    {
      "id": 77,
      "command": [
        "ls"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "sorting",
        "size",
        "human-readable"
      ],
      "task": "List files sorted by size in human-readable format, largest first",
      "solution": "ls -lhS",
      "explanation": "ls -l (long format) -h (human-readable sizes with K, M, G suffixes) -S (sort by size, largest first). Combines detailed listing with size sorting and human-friendly size display for easy identification of large files.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify largest files in directory for cleanup or analysis"
    },
    {
      "id": 78,
      "command": [
        "ls"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "time",
        "sorting",
        "modification"
      ],
      "task": "List files sorted by modification time, newest first",
      "solution": "ls -lt",
      "explanation": "ls -l (long format) -t (sort by modification time, newest first). Shows files with detailed information ordered by when they were last modified, useful for tracking recent activity or changes.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Track recently modified files for debugging or monitoring"
    },
    {
      "id": 79,
      "command": [
        "ls"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "recursive",
        "tree",
        "subdirectories"
      ],
      "task": "Display directory tree structure with file details recursively",
      "solution": "ls -laR",
      "explanation": "ls -l (long format) -a (include hidden files) -R (recursive - descend into subdirectories). Lists all files and directories in the entire directory tree with full details. Each subdirectory is labeled with its path.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Can produce very long output in large directory trees",
      "use_case": "Comprehensive directory structure analysis"
    },
    {
      "id": 80,
      "command": [
        "ls"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "hidden",
        "filtering",
        "patterns"
      ],
      "task": "List only hidden files (starting with dot) in current directory",
      "solution": "ls -ld .*",
      "explanation": "ls -l (long format) -d (list directories themselves, not their contents) .* (glob pattern matching all files starting with dot - hidden files). The -d flag prevents descending into hidden directories like .git/.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Configuration file management and system file inspection"
    },
    {
      "id": 81,
      "command": [
        "ls"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "directories",
        "type",
        "filtering"
      ],
      "task": "List only directories in current location",
      "solution": "ls -d */",
      "explanation": "ls -d (list directory names only, don't show contents) */ (glob pattern matching all items ending with slash - directories only). The trailing slash in the pattern ensures only directories are matched.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Directory navigation and structure overview"
    },
    {
      "id": 82,
      "command": [
        "ls"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "colors",
        "file types",
        "classification"
      ],
      "task": "List files with color coding and file type indicators",
      "solution": "ls -laF --color=always",
      "explanation": "ls -l (long format) -a (include hidden) -F (append type indicator: / for directories, * for executables, @ for links, etc.) --color=always (force color output even when piped). Visual enhancement for file type identification.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Enhanced visual file browsing and type identification"
    },
    {
      "id": 83,
      "command": [
        "ls"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "file management",
      "tags": [
        "inode",
        "links",
        "filesystem"
      ],
      "task": "Display files with inode numbers and link count information",
      "solution": "ls -li",
      "explanation": "ls -l (long format) -i (show inode numbers - unique filesystem identifiers). Inode numbers help identify hard links (same inode = same file) and understand filesystem structure. Link count shows how many names point to the same inode.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Filesystem analysis and hard link detection"
    },
    {
      "id": 84,
      "command": [
        "ls",
        "grep"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "file management",
      "tags": [
        "filtering",
        "executable",
        "permissions"
      ],
      "task": "List only executable files in current directory",
      "solution": "ls -la | grep '^-..x'",
      "explanation": "ls -la (long format with hidden files) | grep '^-..x' (filter lines starting with - (regular file) followed by any two characters, then x (execute permission)). Regex matches files with execute permission for owner.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security audit and script identification"
    },
    {
      "id": 85,
      "command": [
        "ls",
        "awk"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "file management",
      "tags": [
        "size calculation",
        "statistics",
        "analysis"
      ],
      "task": "Calculate total size of all files in directory using ls output",
      "solution": "ls -la | awk '{sum += $5} END {print \"Total size:\", sum, \"bytes\"}'",
      "explanation": "ls -la (detailed listing) | awk '{sum += $5}' (accumulate 5th column - file size in bytes) END {print 'Total size:', sum, 'bytes'} (after processing all lines, print total). Alternative to du command using ls output.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Directory size analysis without using du command"
    },
    {
      "id": 86,
      "command": [
        "ls"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "file management",
      "tags": [
        "one column",
        "scripting",
        "parsing"
      ],
      "task": "List files in single column format for easy script processing",
      "solution": "ls -1",
      "explanation": "ls -1 (force one filename per line output). Useful for scripting when you need to process filenames in loops or pipes without worrying about spaces or formatting. Default when output is not to terminal.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Shell scripting and automated file processing"
    },
    {
      "id": 87,
      "command": [
        "ls"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "file management",
      "tags": [
        "access time",
        "atime",
        "usage"
      ],
      "task": "Display files with access time instead of modification time",
      "solution": "ls -lau",
      "explanation": "ls -l (long format) -a (include hidden files) -u (use access time instead of modification time for sorting and display). Shows when files were last read/accessed, useful for identifying unused files.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Track file access patterns for security or optimization"
    },
    {
      "id": 88,
      "command": [
        "ls",
        "tail"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "newest",
        "recent",
        "monitoring"
      ],
      "task": "Show the 5 most recently modified files in directory",
      "solution": "ls -lt | head -6 | tail -5",
      "explanation": "ls -lt (long format sorted by time, newest first) | head -6 (take first 6 lines including header) | tail -5 (take last 5 lines excluding header). Pipeline removes the header line and shows only the 5 most recent files.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quick identification of recent activity in directories"
    },
    {
      "id": 89,
      "command": [
        "ls",
        "wc"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "count",
        "statistics",
        "inventory"
      ],
      "task": "Count total number of files and directories in current location",
      "solution": "ls -1 | wc -l",
      "explanation": "ls -1 (one item per line) | wc -l (count lines). Simple way to count directory contents. The -1 flag ensures each file/directory is on its own line for accurate counting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Directory inventory and file count statistics"
    },
    {
      "id": 90,
      "command": [
        "ls"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "file management",
      "tags": [
        "reverse",
        "oldest",
        "chronological"
      ],
      "task": "List files in reverse chronological order (oldest first) with full details",
      "solution": "ls -latr",
      "explanation": "ls -l (long format) -a (include hidden) -t (sort by time) -r (reverse order). Combines multiple options to show oldest files first, useful for finding old files or understanding historical file creation patterns.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Historical file analysis and finding oldest system files"
    },
    {
      "id": 91,
      "command": [
        "ln"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "symbolic link",
        "soft link",
        "shortcuts"
      ],
      "task": "Create a symbolic link to a file in different directory",
      "solution": "ln -s /path/to/original/file.txt /path/to/link_name",
      "explanation": "ln -s (create symbolic link, not hard link) /path/to/original/file.txt (target file - can be absolute or relative path) /path/to/link_name (link name/location). Symbolic links can cross filesystem boundaries and point to non-existent files.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Create shortcuts to frequently accessed files or programs"
    },
    {
      "id": 92,
      "command": [
        "ln",
        "ls"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "hard link",
        "inode",
        "filesystem"
      ],
      "task": "Create hard link and verify both files share same inode",
      "solution": "ln original.txt hardlink.txt && ls -li original.txt hardlink.txt",
      "explanation": "ln original.txt hardlink.txt (create hard link - no -s flag) && ls -li (if successful, show inode numbers with -i). Hard links share the same inode number, proving they're the same file with different names.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Hard links only work within same filesystem",
      "use_case": "File backup without duplicating data on same filesystem"
    },
    {
      "id": 93,
      "command": [
        "ln",
        "find"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "file management",
      "tags": [
        "broken links",
        "cleanup",
        "maintenance"
      ],
      "task": "Find and list all broken symbolic links in directory tree",
      "solution": "find . -type l ! -exec test -e {} \\; -print",
      "explanation": "find . -type l (find symbolic links) ! -exec test -e {} \\; (negation ! of test -e which checks if target exists) -print (print broken links). The test command returns false for broken links, ! inverts it to true.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "System maintenance and cleanup of broken symbolic links"
    },
    {
      "id": 94,
      "command": [
        "readlink"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "symbolic link",
        "target",
        "resolution"
      ],
      "task": "Display the target of a symbolic link with full path resolution",
      "solution": "readlink -f symbolic_link_name",
      "explanation": "readlink -f (follow all symbolic links recursively and resolve to absolute path) symbolic_link_name. The -f flag resolves the entire chain of links to show the final target with full canonical path.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Trace symbolic link chains to find actual file location"
    },
    {
      "id": 95,
      "command": [
        "file"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "file type",
        "identification",
        "magic"
      ],
      "task": "Determine file type and format of unknown file",
      "solution": "file unknown_file.dat",
      "explanation": "file unknown_file.dat (analyze file content using magic numbers and patterns to determine type). Examines file headers, content patterns, and metadata to identify format regardless of filename extension. Uses /usr/share/magic database.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify file formats when extension is missing or incorrect"
    },
    {
      "id": 96,
      "command": [
        "stat"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "metadata",
        "timestamps",
        "permissions"
      ],
      "task": "Display comprehensive file metadata including all timestamps",
      "solution": "stat filename.txt",
      "explanation": "stat filename.txt (show detailed file statistics including size, permissions, timestamps, inode, device, links). Displays access time, modify time, change time, birth time (if supported), plus filesystem-specific information.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Forensic analysis and detailed file information gathering"
    },
    {
      "id": 97,
      "command": [
        "file",
        "find"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "batch",
        "file types",
        "analysis"
      ],
      "task": "Identify file types for all files in directory recursively",
      "solution": "find . -type f -exec file {} \\;",
      "explanation": "find . -type f (find all regular files recursively) -exec file {} \\; (execute file command on each found file). Provides comprehensive file type analysis for entire directory trees, useful for content discovery.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Content analysis and file type inventory of unknown directories"
    },
    {
      "id": 98,
      "command": [
        "xdg-open",
        "file"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "default program",
        "associations",
        "desktop"
      ],
      "task": "Check default application for file type and open file with it",
      "solution": "file document.pdf && xdg-open document.pdf",
      "explanation": "file document.pdf (identify file type) && xdg-open document.pdf (if identification succeeds, open with default application). xdg-open uses desktop environment's file associations to launch appropriate program.",
      "execution_time": "< 1 min",
      "requirements": [
        "xdg-utils"
      ],
      "warnings": "Requires desktop environment",
      "use_case": "Verify and use system file associations for opening files"
    },
    {
      "id": 99,
      "command": [
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": [
        "field extraction",
        "columns",
        "delimiter"
      ],
      "task": "Extract specific columns from space-separated file and print them",
      "solution": "awk '{print $1, $3}' data.txt",
      "explanation": "awk '{print $1, $3}' (print first and third field from each line) data.txt. AWK automatically splits lines into fields using whitespace. $1, $3 extracts specific columns, comma adds space between them in output.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract specific data columns from structured text files"
    },
    {
      "id": 100,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "text processing",
      "tags": [
        "conditional",
        "filtering",
        "logic"
      ],
      "task": "Print lines where column 2 is greater than column 3",
      "solution": "awk '$2 > $3 {print}' numbers.txt",
      "explanation": "awk '$2 > $3 {print}' (if second field > third field, then print the line) numbers.txt. AWK pattern-action syntax: condition before braces determines when action executes. Print is default action when omitted.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Conditional data filtering and analysis in structured files"
    },
    {
      "id": 101,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "text processing",
      "tags": [
        "line numbers",
        "formatting",
        "output"
      ],
      "task": "Add line numbers to text file with custom formatting",
      "solution": "awk '{printf \"%3d: %s\\n\", NR, $0}' file.txt",
      "explanation": "awk '{printf '%3d: %s\\n', NR, $0}' (format output with printf) '%3d:' (3-digit right-aligned number) NR (built-in variable: Number of Records/line number) $0 (entire line) file.txt. Creates formatted line numbers.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Add formatted line numbers for code review or documentation"
    },
    {
      "id": 102,
      "command": [
        "awk"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "patterns",
        "ranges",
        "processing"
      ],
      "task": "Process lines between two patterns and perform calculations",
      "solution": "awk '/START/,/END/ {sum += $2} END {print \"Sum between patterns:\", sum}' data.txt",
      "explanation": "awk '/START/,/END/' (range pattern: process lines from START to END pattern) '{sum += $2}' (accumulate second field) END {print 'Sum between patterns:', sum} (after processing, print total). Range patterns enable section-specific processing.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract and analyze data from specific sections of log files"
    },
    {
      "id": 103,
      "command": [
        "chmod"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "security",
      "tags": [
        "permissions",
        "security",
        "access"
      ],
      "task": "Make a script executable for owner, readable for group and others",
      "solution": "chmod 744 script.sh",
      "explanation": "chmod 744 (octal permission notation: 7=rwx for owner, 4=r-- for group, 4=r-- for others) script.sh. Permission bits: read(4) + write(2) + execute(1). Owner gets full access (7), group and others get read-only (4).",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Set proper permissions for shell scripts and executables"
    },
    {
      "id": 104,
      "command": [
        "chmod"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "security",
      "tags": [
        "recursive",
        "directories",
        "permissions"
      ],
      "task": "Recursively change permissions for all files and directories",
      "solution": "chmod -R 755 /path/to/directory",
      "explanation": "chmod -R (recursive - apply to directory and all contents) 755 (rwxr-xr-x: owner full access, group and others read+execute) /path/to/directory. Standard web server permissions: directories need execute to be traversable.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Be careful with recursive permission changes",
      "use_case": "Set standard web directory permissions for Apache/Nginx"
    },
    {
      "id": 105,
      "command": [
        "chown"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "security",
      "tags": [
        "ownership",
        "user",
        "group"
      ],
      "task": "Change file owner to specific user and group",
      "solution": "chown user:group filename.txt",
      "explanation": "chown user:group (change owner to 'user' and group to 'group', colon separates user from group) filename.txt. Can use just user (keeps current group) or :group (keeps current user). Requires appropriate privileges.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": "Requires appropriate permissions to change ownership",
      "use_case": "Transfer file ownership after copying or creating files as root"
    },
    {
      "id": 106,
      "command": [
        "chown",
        "find"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "security",
      "tags": [
        "batch",
        "ownership",
        "recursive"
      ],
      "task": "Change ownership of all files belonging to specific user",
      "solution": "find /path -user olduser -exec chown newuser:newgroup {} \\;",
      "explanation": "find /path -user olduser (find all files owned by olduser) -exec chown newuser:newgroup {} \\; (change ownership to newuser:newgroup for each found file). Useful for user account migration or bulk ownership changes.",
      "execution_time": "1-5 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": "Test on small directory first, can affect system files",
      "use_case": "User account migration and bulk ownership changes"
    },
    {
      "id": 107,
      "command": [
        "useradd"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "user management",
        "account",
        "creation"
      ],
      "task": "Create new user with home directory and specific shell",
      "solution": "useradd -m -s /bin/bash newuser",
      "explanation": "useradd -m (create home directory) -s /bin/bash (set default shell to bash) newuser. Creates user account with home directory, sets shell, and adds entry to /etc/passwd. Additional setup like password setting needed separately.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": null,
      "use_case": "System administration and user account provisioning"
    },
    {
      "id": 108,
      "command": [
        "groupadd",
        "usermod"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "group management",
        "user groups",
        "permissions"
      ],
      "task": "Create new group and add existing user to it",
      "solution": "groupadd developers && usermod -aG developers username",
      "explanation": "groupadd developers (create new group named 'developers') && usermod -aG developers username (if successful, add username to developers group with -a append and -G supplementary group). Enables role-based access control.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": null,
      "use_case": "Role-based access control and permission management"
    },
    {
      "id": 109,
      "command": [
        "id"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "system admin",
      "tags": [
        "user info",
        "groups",
        "identity"
      ],
      "task": "Display current user ID and all group memberships",
      "solution": "id",
      "explanation": "id (show user and group IDs for current user). Displays UID (user ID), GID (primary group ID), and all supplementary groups with both numeric IDs and names. Essential for troubleshooting permission issues.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Verify user identity and group memberships for troubleshooting"
    },
    {
      "id": 110,
      "command": [
        "groups",
        "getent"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "group info",
        "membership",
        "system"
      ],
      "task": "List all groups and find members of specific group",
      "solution": "getent group | grep groupname",
      "explanation": "getent group (get entries from group database - includes local and network groups) | grep groupname (filter for specific group). Shows group info including GID and member list. More comprehensive than /etc/group.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "System audit and group membership verification"
    },
    {
      "id": 111,
      "command": [
        "jobs"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "background",
        "processes",
        "job control"
      ],
      "task": "List all active background jobs with their status",
      "solution": "jobs -l",
      "explanation": "jobs -l (list jobs with process IDs). Shows background jobs started in current shell session with job number, status (Running, Stopped, Done), and PID. Jobs are shell-specific background processes.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Monitor and manage background processes in shell session"
    },
    {
      "id": 112,
      "command": [
        "jobs",
        "bg",
        "fg"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "job control",
        "foreground",
        "background"
      ],
      "task": "Move stopped job to background and then bring it to foreground",
      "solution": "jobs && bg %1 && fg %1",
      "explanation": "jobs (list current jobs to see job numbers) && bg %1 (move job #1 to background, continuing execution) && fg %1 (bring job #1 to foreground). %N refers to job number. Demonstrates job control workflow.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Interactive job management during development and system tasks"
    },
    {
      "id": 113,
      "command": [
        "kill"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "process",
        "termination",
        "signals"
      ],
      "task": "Gracefully terminate process by PID using TERM signal",
      "solution": "kill -TERM 1234",
      "explanation": "kill -TERM 1234 (send TERM signal to process ID 1234). TERM (15) is default signal for graceful shutdown - allows process to clean up and save data. Alternative to KILL (-9) which forces immediate termination.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Verify PID before killing to avoid terminating wrong process",
      "use_case": "Clean shutdown of unresponsive applications"
    },
    {
      "id": 114,
      "command": [
        "kill",
        "ps"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "process",
        "kill by name",
        "pattern"
      ],
      "task": "Find and kill all processes matching specific name pattern",
      "solution": "ps aux | grep process_name | grep -v grep | awk '{print $2}' | xargs kill",
      "explanation": "ps aux (list all processes) | grep process_name (filter by name) | grep -v grep (exclude grep itself) | awk '{print $2}' (extract PID column) | xargs kill (pass PIDs to kill command). Pipeline approach to kill by name.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Double-check process list before killing",
      "use_case": "Clean up multiple instances of stuck applications"
    },
    {
      "id": 115,
      "command": [
        "killall"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "process",
        "kill by name",
        "bulk"
      ],
      "task": "Kill all processes with specific name using killall",
      "solution": "killall firefox",
      "explanation": "killall firefox (terminate all processes with exact name 'firefox'). More direct than ps/grep pipeline but less flexible. Sends TERM signal by default. Use killall -9 for forceful termination if needed.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Will kill ALL processes with that name",
      "use_case": "Quick termination of all instances of specific application"
    },
    {
      "id": 116,
      "command": [
        "ps"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "monitoring",
      "tags": [
        "processes",
        "system",
        "running"
      ],
      "task": "Display all running processes with detailed information",
      "solution": "ps aux",
      "explanation": "ps aux (a = all users' processes, u = user-oriented format with owner info, x = include processes without controlling terminal). Shows PID, CPU%, memory%, start time, command. Standard process listing format.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "System monitoring and process analysis"
    },
    {
      "id": 117,
      "command": [
        "ps",
        "sort"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "memory",
        "cpu",
        "resource usage"
      ],
      "task": "Show top 10 processes consuming most memory",
      "solution": "ps aux --sort=-%mem | head -11",
      "explanation": "ps aux (detailed process list) --sort=-%mem (sort by memory usage, - prefix for descending order) | head -11 (show first 11 lines including header). Identifies memory-intensive processes for optimization or troubleshooting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify memory-intensive processes for optimization"
    },
    {
      "id": 118,
      "command": [
        "ps",
        "grep"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "filtering",
        "specific process",
        "search"
      ],
      "task": "Find specific process and display its details",
      "solution": "ps aux | grep process_name",
      "explanation": "ps aux (list all processes with details) | grep process_name (filter lines containing process_name). Simple way to check if specific application is running and see its resource usage, PID, and other details.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Check if specific application is running and get its details"
    },
    {
      "id": 119,
      "command": [
        "md5sum"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "security",
      "tags": [
        "checksum",
        "integrity",
        "verification"
      ],
      "task": "Calculate MD5 checksum of file for integrity verification",
      "solution": "md5sum important_file.iso",
      "explanation": "md5sum important_file.iso (calculate MD5 hash of file). Produces 32-character hexadecimal hash that changes if file is modified. Compare with known good checksum to verify file integrity after downloads or transfers.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Verify file integrity after download or transfer"
    },
    {
      "id": 120,
      "command": [
        "md5sum"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "security",
      "tags": [
        "batch",
        "checksum",
        "verification"
      ],
      "task": "Create checksum file for all files in directory and verify later",
      "solution": "md5sum *.txt > checksums.md5 && md5sum -c checksums.md5",
      "explanation": "md5sum *.txt > checksums.md5 (calculate checksums for all .txt files, save to file) && md5sum -c checksums.md5 (verify all checksums in file). Two-phase process: create reference checksums, then verify integrity.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Backup verification and file integrity monitoring"
    },
    {
      "id": 121,
      "command": [
        "sha256sum",
        "md5sum"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "security",
      "tags": [
        "multiple hash",
        "security",
        "comparison"
      ],
      "task": "Compare MD5 and SHA256 checksums of same file",
      "solution": "md5sum file.dat && sha256sum file.dat",
      "explanation": "md5sum file.dat (calculate MD5 hash) && sha256sum file.dat (calculate SHA256 hash). SHA256 is more secure than MD5 but slower. Using both provides stronger verification and compatibility with different systems.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Enhanced security verification with multiple hash algorithms"
    },
    {
      "id": 122,
      "command": [
        "zip"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "backup",
      "tags": [
        "compression",
        "archive",
        "zip"
      ],
      "task": "Create password-protected ZIP archive of directory",
      "solution": "zip -r -P password archive.zip directory/",
      "explanation": "zip -r (recursive - include subdirectories) -P password (set password for encryption) archive.zip (output filename) directory/ (source directory). Creates encrypted ZIP compatible with most systems.",
      "execution_time": "1-5 min",
      "requirements": [
        "zip"
      ],
      "warnings": "Password will be visible in command history",
      "use_case": "Secure file sharing and backup with password protection"
    },
    {
      "id": 123,
      "command": [
        "unzip"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "backup",
      "tags": [
        "decompression",
        "extraction",
        "zip"
      ],
      "task": "Extract ZIP archive and list contents without extracting",
      "solution": "unzip -l archive.zip && unzip archive.zip",
      "explanation": "unzip -l archive.zip (list contents without extracting - safe preview) && unzip archive.zip (if list looks good, extract all files). Two-step process ensures you know what will be extracted before doing it.",
      "execution_time": "1-5 min",
      "requirements": [
        "unzip"
      ],
      "warnings": null,
      "use_case": "Safe archive inspection before extraction"
    },
    {
      "id": 124,
      "command": [
        "gzip",
        "gunzip"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "backup",
      "tags": [
        "compression",
        "gzip",
        "space saving"
      ],
      "task": "Compress large file and then decompress it",
      "solution": "gzip large_file.txt && gunzip large_file.txt.gz",
      "explanation": "gzip large_file.txt (compress file, replaces original with .gz version) && gunzip large_file.txt.gz (decompress back to original). gzip provides good compression for text files and is widely supported.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Space-efficient storage of large text files and logs"
    },
    {
      "id": 125,
      "command": [
        "tar",
        "gzip"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": [
        "archive",
        "compression",
        "backup"
      ],
      "task": "Create compressed tar archive with progress indication",
      "solution": "tar -czf backup.tar.gz --verbose directory/",
      "explanation": "tar -c (create) -z (gzip compression) -f backup.tar.gz (filename) --verbose (show files being processed) directory/ (source). Combines archiving and compression in one step with visual feedback.",
      "execution_time": "5+ min",
      "requirements": null,
      "warnings": null,
      "use_case": "Professional backup creation with compression and progress tracking"
    },
    {
      "id": 126,
      "command": [
        "df"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "monitoring",
      "tags": [
        "disk space",
        "filesystem",
        "usage"
      ],
      "task": "Display available disk space in human-readable format",
      "solution": "df -h",
      "explanation": "df -h (disk free with human-readable sizes using K, M, G, T suffixes). Shows filesystem, total size, used space, available space, usage percentage, and mount point for all mounted filesystems.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Regular system monitoring and disk space management"
    },
    {
      "id": 127,
      "command": [
        "df"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "disk usage",
        "percentage",
        "alerts"
      ],
      "task": "Show disk usage percentage and highlight filesystems over 80% full",
      "solution": "df -h | awk '$5 > 80 {print \"WARNING: \" $0}'",
      "explanation": "df -h (disk usage in human format) | awk '$5 > 80' (check if 5th column - usage percentage > 80%) '{print 'WARNING: ' $0}' (print warning with full line). Automated monitoring for disk space alerts.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Automated disk space monitoring and alerting"
    },
    {
      "id": 128,
      "command": [
        "du"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "directory size",
        "disk usage",
        "analysis"
      ],
      "task": "Show size of each subdirectory in current location",
      "solution": "du -sh */",
      "explanation": "du -s (summarize - don't show subdirectories) -h (human-readable sizes) */ (all directories in current location). Provides size summary for each top-level directory without recursive details.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify which directories consume most disk space"
    },
    {
      "id": 129,
      "command": [
        "du",
        "sort"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "largest directories",
        "sorting",
        "analysis"
      ],
      "task": "Find and display 10 largest directories recursively",
      "solution": "du -h | sort -hr | head -10",
      "explanation": "du -h (disk usage with human-readable sizes for all directories) | sort -hr (sort human-numeric reverse - largest first) | head -10 (show top 10). Identifies space-consuming directories for cleanup planning.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Disk cleanup planning and storage optimization"
    },
    {
      "id": 130,
      "command": [
        "du",
        "find"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "monitoring",
      "tags": [
        "large files",
        "cleanup",
        "analysis"
      ],
      "task": "Find directories larger than 1GB and show their contents size",
      "solution": "find . -type d -exec du -sh {} \\; | grep -E '^[0-9.]+G'",
      "explanation": "find . -type d (find directories) -exec du -sh {} \\; (get size summary for each directory) | grep -E '^[0-9.]+G' (filter for sizes in gigabytes using regex). Identifies large directories for targeted cleanup.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify large directories for cleanup or archiving"
    },
    {
      "id": 131,
      "command": [
        "fdisk"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "partitions",
        "disk",
        "information"
      ],
      "task": "List all disk partitions and their sizes",
      "solution": "fdisk -l",
      "explanation": "fdisk -l (list all disk partitions and partition tables). Shows disk devices, partition types, sizes, and filesystem information. Requires root privileges to access all disks. Essential for disk management and troubleshooting.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": "Read-only operation but requires root access",
      "use_case": "Disk analysis and partition management planning"
    },
    {
      "id": 132,
      "command": [
        "lsblk"
      ],
      "difficulty": 2,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "block devices",
        "tree",
        "partitions"
      ],
      "task": "Display block devices in tree format with filesystem information",
      "solution": "lsblk -f",
      "explanation": "lsblk -f (list block devices with filesystem information). Shows storage devices in tree format including device names, mount points, filesystem types, and UUIDs. The -f flag adds filesystem details like type, label, and UUID for each partition.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Visual overview of storage devices and mount points"
    },
    {
      "id": 133,
      "command": [
        "parted"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "partition info",
        "disk geometry",
        "advanced"
      ],
      "task": "Display detailed partition information including partition table type",
      "solution": "parted -l",
      "explanation": "parted -l (list all storage devices and their partition tables). Shows detailed information including disk size, partition table type (GPT/MBR), partition boundaries, and filesystem types. More detailed than fdisk and supports GPT partition tables.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": "Read-only operation when using -l flag",
      "use_case": "Advanced disk analysis and partition table inspection"
    },
    {
      "id": 134,
      "command": [
        "blkid"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "uuid",
        "filesystem",
        "identification"
      ],
      "task": "Display UUID and filesystem type for all partitions",
      "solution": "blkid",
      "explanation": "blkid (block device identification). Shows UUID (Universally Unique Identifier), filesystem type, and labels for all available block devices. Essential for creating fstab entries and identifying storage devices reliably.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Filesystem identification for fstab configuration"
    },
    {
      "id": 135,
      "command": [
        "mount"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "mounting",
        "filesystem",
        "access"
      ],
      "task": "Mount USB drive to specific directory and verify mount",
      "solution": "mount /dev/sdb1 /mnt/usb && mount | grep sdb1",
      "explanation": "mount /dev/sdb1 /mnt/usb (mount device /dev/sdb1 to /mnt/usb directory) && mount | grep sdb1 (if successful, verify mount by showing mounted filesystems containing sdb1). Makes filesystem accessible through specified directory.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": "Ensure mount point directory exists and USB device path is correct",
      "use_case": "Access external storage devices and removable media"
    },
    {
      "id": 136,
      "command": [
        "umount"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "system admin",
      "tags": [
        "unmounting",
        "safe removal",
        "filesystem"
      ],
      "task": "Safely unmount filesystem and verify it's no longer mounted",
      "solution": "umount /mnt/usb && mount | grep usb",
      "explanation": "umount /mnt/usb (unmount filesystem from /mnt/usb mount point) && mount | grep usb (verify unmounting by checking if usb appears in mounted filesystems - should return nothing). Safe way to disconnect storage devices.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": "Ensure no processes are using the mounted filesystem",
      "use_case": "Safe removal of external storage devices"
    },
    {
      "id": 137,
      "command": [
        "mount",
        "grep"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "mounted filesystems",
        "system info",
        "monitoring"
      ],
      "task": "Display only currently mounted filesystems excluding virtual ones",
      "solution": "mount | grep -E '^/dev'",
      "explanation": "mount (show all mounted filesystems) | grep -E '^/dev' (filter lines starting with /dev using extended regex). Excludes virtual filesystems like proc, sys, tmpfs and shows only physical storage devices currently mounted.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quick overview of physically mounted storage devices"
    },
    {
      "id": 138,
      "command": [
        "mount",
        "fstab"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "persistent mount",
        "fstab",
        "configuration"
      ],
      "task": "Add permanent mount entry to fstab and test mount",
      "solution": "echo '/dev/sdb1 /mnt/data ext4 defaults 0 2' >> /etc/fstab && mount -a",
      "explanation": "echo '/dev/sdb1 /mnt/data ext4 defaults 0 2' >> /etc/fstab (append mount entry: device, mount point, filesystem type, options, dump, fsck order) && mount -a (mount all filesystems in fstab). Creates persistent mount that survives reboots.",
      "execution_time": "< 1 min",
      "requirements": [
        "sudo privileges"
      ],
      "warnings": "Backup /etc/fstab before editing, incorrect entries can prevent boot",
      "use_case": "Configure automatic mounting of storage devices at boot"
    },
    {
      "id": 139,
      "command": [
        "top"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "real-time",
        "processes",
        "system load"
      ],
      "task": "Monitor system in real-time and sort processes by memory usage",
      "solution": "top -o %MEM",
      "explanation": "top -o %MEM (start top with processes sorted by memory usage percentage). Provides real-time view of running processes, system load, CPU usage, memory usage. Interactive tool - use 'M' to sort by memory, 'P' by CPU, 'q' to quit.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Press 'q' to quit top",
      "use_case": "Real-time system performance monitoring and troubleshooting"
    },
    {
      "id": 140,
      "command": [
        "htop"
      ],
      "difficulty": 2,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "interactive",
        "processes",
        "system monitor"
      ],
      "task": "Use interactive process viewer with color coding and tree view",
      "solution": "htop",
      "explanation": "htop (enhanced interactive process viewer). Improved version of top with color coding, mouse support, horizontal scrolling, tree view of processes. Use F5 for tree view, F6 to sort, F9 to kill processes, F10 to quit.",
      "execution_time": "long-running",
      "requirements": [
        "htop"
      ],
      "warnings": "Use F10 or 'q' to quit",
      "use_case": "Enhanced system monitoring with better visualization than top"
    },
    {
      "id": 141,
      "command": [
        "nohup"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "background",
        "persistent",
        "logout"
      ],
      "task": "Run command that continues after logout and redirect output",
      "solution": "nohup python long_script.py > output.log 2>&1 &",
      "explanation": "nohup python long_script.py (run command immune to hangup signals) > output.log (redirect stdout to log file) 2>&1 (redirect stderr to same file as stdout) & (run in background). Process continues even after terminal closes or SSH disconnects.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": null,
      "use_case": "Run long-running processes that survive session termination"
    },
    {
      "id": 142,
      "command": [
        "screen"
      ],
      "difficulty": 3,
      "rating": 5,
      "category": "system admin",
      "tags": [
        "terminal multiplexer",
        "sessions",
        "detach"
      ],
      "task": "Create detachable terminal session and reconnect to it",
      "solution": "screen -S mysession && screen -r mysession",
      "explanation": "screen -S mysession (create new screen session named 'mysession') && screen -r mysession (reattach to existing session). Terminal multiplexer allowing detachable sessions that persist across network disconnections. Use Ctrl+A, D to detach.",
      "execution_time": "long-running",
      "requirements": [
        "screen"
      ],
      "warnings": "Use Ctrl+A, D to detach from screen session",
      "use_case": "Manage persistent terminal sessions for remote work"
    },
    {
      "id": 143,
      "command": [
        "tmux"
      ],
      "difficulty": 3,
      "rating": 5,
      "category": "system admin",
      "tags": [
        "terminal multiplexer",
        "panes",
        "windows"
      ],
      "task": "Create tmux session with multiple panes and windows",
      "solution": "tmux new-session -s work && tmux split-window -h && tmux new-window",
      "explanation": "tmux new-session -s work (create session named 'work') && tmux split-window -h (split current window horizontally) && tmux new-window (create new window). Advanced terminal multiplexer with windows, panes, and session management.",
      "execution_time": "long-running",
      "requirements": [
        "tmux"
      ],
      "warnings": "Use Ctrl+B to access tmux commands",
      "use_case": "Advanced terminal session management with multiple workspaces"
    },
    {
      "id": 144,
      "command": [
        "crontab"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "automation",
      "tags": [
        "scheduling",
        "automation",
        "maintenance"
      ],
      "task": "Schedule script to run every day at 3 AM and list current cron jobs",
      "solution": "crontab -e # Add: 0 3 * * * /path/to/script.sh && crontab -l",
      "explanation": "crontab -e (edit user's cron table) # Add: 0 3 * * * /path/to/script.sh (cron format: minute hour day month weekday command) && crontab -l (list current cron jobs). Persistent task scheduling that survives reboots.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": "Test scripts manually before scheduling",
      "use_case": "Automated system maintenance and backup scheduling"
    },
    {
      "id": 145,
      "command": [
        "at"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "automation",
      "tags": [
        "one-time",
        "scheduling",
        "delayed execution"
      ],
      "task": "Schedule one-time command to run at specific time",
      "solution": "echo 'backup.sh' | at 23:30 && atq",
      "explanation": "echo 'backup.sh' | at 23:30 (schedule backup.sh to run once at 23:30 today) && atq (list pending at jobs). Alternative to cron for one-time scheduled execution. Uses natural time formats like 'tomorrow', 'next week'.",
      "execution_time": "< 1 min",
      "requirements": [
        "at daemon"
      ],
      "warnings": null,
      "use_case": "Schedule one-time tasks for specific times"
    },
    {
      "id": 146,
      "command": [
        "uptime"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "monitoring",
      "tags": [
        "system load",
        "uptime",
        "performance"
      ],
      "task": "Display system uptime and current load averages",
      "solution": "uptime",
      "explanation": "uptime (show system uptime and load averages). Displays current time, how long system has been running, number of users, and load averages for 1, 5, and 15 minutes. Load averages indicate system utilization.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quick system health check and load assessment"
    },
    {
      "id": 147,
      "command": [
        "free"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "monitoring",
      "tags": [
        "memory",
        "ram",
        "usage"
      ],
      "task": "Display memory usage in human-readable format",
      "solution": "free -h",
      "explanation": "free -h (show memory usage with human-readable units like GB, MB). Displays total, used, free, shared, buffer/cache, and available memory. Available memory is the key metric for determining if system needs more RAM.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Monitor RAM usage and available memory"
    },
    {
      "id": 148,
      "command": [
        "iostat"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "disk io",
        "performance",
        "statistics"
      ],
      "task": "Monitor disk I/O statistics in real-time",
      "solution": "iostat -x 2",
      "explanation": "iostat -x 2 (extended I/O statistics updated every 2 seconds). Shows per-device statistics including read/write operations per second, bandwidth, average wait time, and utilization percentage. Essential for diagnosing storage performance issues.",
      "execution_time": "long-running",
      "requirements": [
        "sysstat package"
      ],
      "warnings": "Press Ctrl+C to stop monitoring",
      "use_case": "Diagnose disk performance issues and I/O bottlenecks"
    },
    {
      "id": 149,
      "command": [
        "lsof"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "open files",
        "processes",
        "debugging"
      ],
      "task": "List all files opened by specific process and network connections",
      "solution": "lsof -p 1234 && lsof -i",
      "explanation": "lsof -p 1234 (list open files for process ID 1234) && lsof -i (list network connections). Shows file descriptors, network sockets, shared libraries, and other resources used by processes. Powerful debugging tool for file access issues.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Debug file access issues and network connection problems"
    },
    {
      "id": 150,
      "command": [
        "which",
        "whereis"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "system admin",
      "tags": [
        "command location",
        "path",
        "binary"
      ],
      "task": "Find location of command binary and related files",
      "solution": "which python && whereis python",
      "explanation": "which python (find path to python executable in PATH) && whereis python (find python binary, source, and manual pages). which shows the actual executable that would run, whereis provides comprehensive location information including documentation.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Locate command binaries for scripting and PATH troubleshooting"
    },
    {
      "id": 151,
      "command": [
        "cat"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "text processing",
      "tags": [
        "file content",
        "display",
        "concatenate"
      ],
      "task": "Display the contents of a file with line numbers",
      "solution": "cat -n filename.txt",
      "explanation": "cat -n filename.txt (concatenate and display file with line numbers). The -n flag adds line numbers to each line of output, making it easier to reference specific lines for debugging or analysis. Useful alternative to 'nl' command.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quick file inspection with line numbers for debugging"
    },
    {
      "id": 152,
      "command": [
        "cat"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "text processing",
      "tags": [
        "concatenate",
        "merge",
        "files"
      ],
      "task": "Concatenate multiple files into a single output file",
      "solution": "cat file1.txt file2.txt file3.txt > combined.txt",
      "explanation": "cat file1.txt file2.txt file3.txt (concatenate multiple files in order) > combined.txt (redirect combined output to new file). Files are joined sequentially without any separators. Preserves original file order and content.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Merge log files or combine data files for analysis"
    },
    {
      "id": 153,
      "command": [
        "cat"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": [
        "here document",
        "creation",
        "scripting"
      ],
      "task": "Create a multi-line file using cat with here document",
      "solution": "cat > config.txt << EOF\nserver=localhost\nport=8080\ndebug=true\nEOF",
      "explanation": "cat > config.txt (redirect cat output to file) << EOF (here document delimiter - read until EOF marker) \\nserver=localhost\\nport=8080\\ndebug=true\\nEOF (multi-line content ending with delimiter). Creates files with embedded content in scripts.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Create configuration files or scripts from command line"
    },
    {
      "id": 154,
      "command": [
        "cat"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "text processing",
      "tags": [
        "whitespace",
        "tabs",
        "formatting"
      ],
      "task": "Display file content showing tabs as ^I and line endings",
      "solution": "cat -A filename.txt",
      "explanation": "cat -A filename.txt (show all non-printing characters). Displays tabs as ^I, line endings as $, and other control characters visibly. The -A flag combines -vET options for comprehensive whitespace visualization. Essential for debugging formatting issues.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Debug formatting issues and invisible characters in text files"
    },
    {
      "id": 155,
      "command": [
        "touch"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "file management",
      "tags": [
        "file creation",
        "empty file",
        "new"
      ],
      "task": "Create multiple empty files at once",
      "solution": "touch file1.txt file2.txt file3.txt",
      "explanation": "touch file1.txt file2.txt file3.txt (create multiple empty files in one command). If files don't exist, touch creates them with zero size and current timestamp. If files exist, only timestamps are updated without changing content.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Quickly create placeholder files for development or testing"
    },
    {
      "id": 156,
      "command": [
        "touch"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "timestamp",
        "modification time",
        "access time"
      ],
      "task": "Update file timestamp to current time without changing content",
      "solution": "touch existing_file.txt",
      "explanation": "touch existing_file.txt (update access and modification times to current time). Does not modify file content, only metadata timestamps. Useful for triggering build systems or backup tools that rely on modification times for change detection.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Force file modification time update for build systems or backups"
    },
    {
      "id": 157,
      "command": [
        "touch"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "specific timestamp",
        "date",
        "time manipulation"
      ],
      "task": "Create file with specific timestamp from the past",
      "solution": "touch -t 202301011200 old_file.txt",
      "explanation": "touch -t 202301011200 old_file.txt (set specific timestamp using -t flag). Time format is [[CC]YY]MMDDhhmm[.ss] - here 202301011200 means January 1, 2023, 12:00. Creates file with historical timestamp for testing date-based operations.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Create test files with historical timestamps for testing date-based operations"
    },
    {
      "id": 158,
      "command": [
        "tail"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "text processing",
      "tags": [
        "end of file",
        "last lines",
        "monitoring"
      ],
      "task": "Display the last 20 lines of a log file",
      "solution": "tail -n 20 logfile.log",
      "explanation": "tail -n 20 logfile.log (show last 20 lines of file). Default is 10 lines, -n specifies exact number. Useful for checking recent log entries without reading entire file. More efficient than cat for large files when only end matters.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Check recent entries in log files for debugging"
    },
    {
      "id": 159,
      "command": [
        "tail"
      ],
      "difficulty": 2,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "real-time",
        "follow",
        "live monitoring"
      ],
      "task": "Monitor log file in real-time showing new lines as they appear",
      "solution": "tail -f application.log",
      "explanation": "tail -f application.log (follow file as it grows, showing new lines in real-time). The -f flag keeps tail running and displays new content as it's appended. Essential for live log monitoring during application development and troubleshooting.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Press Ctrl+C to stop monitoring",
      "use_case": "Real-time monitoring of application logs during development or troubleshooting"
    },
    {
      "id": 160,
      "command": [
        "tail",
        "grep"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "filtered monitoring",
        "error tracking",
        "real-time"
      ],
      "task": "Monitor log file in real-time and show only error messages",
      "solution": "tail -f app.log | grep -i error",
      "explanation": "tail -f app.log (follow log file in real-time) | grep -i error (filter for lines containing 'error' case-insensitively). Combines real-time monitoring with filtering to show only relevant error messages. Pipeline allows focused monitoring of specific events.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": null,
      "use_case": "Real-time error monitoring and alerting in production systems"
    },
    {
      "id": 161,
      "command": [
        "head"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "text processing",
      "tags": [
        "beginning of file",
        "first lines",
        "preview"
      ],
      "task": "Display the first 15 lines of a large data file",
      "solution": "head -n 15 data.csv",
      "explanation": "head -n 15 data.csv (show first 15 lines of file). Default is 10 lines, -n specifies exact number. Useful for previewing file structure, headers, and sample data before processing large files. More efficient than cat for quick inspection.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Preview file structure and headers before processing large datasets"
    },
    {
      "id": 162,
      "command": [
        "head",
        "tail"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "text processing",
      "tags": [
        "extract range",
        "specific lines",
        "text extraction"
      ],
      "task": "Extract lines 10-20 from a file using head and tail combination",
      "solution": "head -n 20 file.txt | tail -n 11",
      "explanation": "head -n 20 file.txt (get first 20 lines) | tail -n 11 (from those 20 lines, take the last 11). This extracts lines 10-20 (11 lines total). Pipeline technique for extracting specific line ranges without sed or awk.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Extract specific line ranges from large files for analysis or debugging"
    },
    {
      "id": 163,
      "command": [
        "jq"
      ],
      "difficulty": 2,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "json",
        "creation",
        "object"
      ],
      "task": "Create a JSON object from command line with multiple fields",
      "solution": "echo '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}' | jq '.'",
      "explanation": "echo '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}' (create JSON string with escaped quotes) | jq '.' (parse and pretty-print JSON). The '.' operator is identity filter that formats JSON with proper indentation and validates syntax.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Generate JSON configuration or test data from command line"
    },
    {
      "id": 164,
      "command": [
        "jq"
      ],
      "difficulty": 3,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "array",
        "construction"
      ],
      "task": "Build JSON array from multiple input values",
      "solution": "echo -e 'apple\\norange\\nbanana' | jq -R -s 'split(\"\\n\") | map(select(length > 0))'",
      "explanation": "echo -e 'apple\\norange\\nbanana' (create multi-line text) | jq -R (read raw strings, not JSON) -s (slurp all input into single string) 'split(\"\\n\")' (split by newlines into array) | map(select(length > 0)) (filter out empty strings). Converts text lines to JSON array.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Convert text lists to JSON arrays for API consumption"
    },
    {
      "id": 165,
      "command": [
        "jq"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "deep navigation",
        "nested"
      ],
      "task": "Navigate deeply nested JSON and extract specific values",
      "solution": "jq '.data.users[].profile.settings.preferences.theme' complex.json",
      "explanation": "jq '.data.users[]' (access data object, iterate through users array) '.profile.settings.preferences.theme' (navigate nested object hierarchy to extract theme values). Demonstrates deep object navigation with array iteration in complex JSON structures.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Extract configuration values from complex API responses"
    },
    {
      "id": 166,
      "command": [
        "jq"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "conditional",
        "complex filtering"
      ],
      "task": "Filter JSON with multiple conditions and logical operators",
      "solution": "jq '.items[] | select(.price > 50 and .category == \"electronics\" and .inStock == true)' products.json",
      "explanation": "jq '.items[]' (iterate through items array) | select(.price > 50 and .category == \"electronics\" and .inStock == true) (filter with multiple AND conditions: numeric comparison, string equality, boolean check). Complex filtering with logical operators.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Complex product filtering for e-commerce applications"
    },
    {
      "id": 167,
      "command": [
        "jq"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "transformation",
        "restructure"
      ],
      "task": "Transform JSON structure and rename fields with calculations",
      "solution": "jq '.users | map({fullName: (.firstName + \" \" + .lastName), totalValue: (.salary * 12), department: .dept})' employees.json",
      "explanation": "jq '.users | map({...})' (transform each user object) {fullName: (.firstName + \" \" + .lastName) (concatenate strings), totalValue: (.salary * 12) (calculate annual salary), department: .dept} (rename field). Creates new object structure with computed fields.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Data transformation for reporting and analytics systems"
    },
    {
      "id": 168,
      "command": [
        "jq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "json",
        "merging",
        "combining"
      ],
      "task": "Merge multiple JSON files into single structure",
      "solution": "jq -s 'add' file1.json file2.json file3.json",
      "explanation": "jq -s (slurp mode - read all files into single array) 'add' (merge/combine all objects in array). For arrays, concatenates them; for objects, merges properties with later values overriding earlier ones.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Combine configuration files or merge API responses"
    },
    {
      "id": 169,
      "command": [
        "jq"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "validation",
        "schema"
      ],
      "task": "Validate JSON structure and check for required fields",
      "solution": "jq 'if (.name and .email and .age) then \"Valid\" else \"Missing required fields\" end' user.json",
      "explanation": "jq 'if (.name and .email and .age)' (check if all required fields exist and are truthy) then \"Valid\" else \"Missing required fields\" end (conditional output based on validation). Simple schema validation using conditional logic.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Data validation before processing or storing user information"
    },
    {
      "id": 170,
      "command": [
        "jq"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "aggregation",
        "advanced"
      ],
      "task": "Calculate complex aggregations with grouping and statistics",
      "solution": "jq 'group_by(.department) | map({dept: .[0].department, count: length, avgSalary: (map(.salary) | add / length), totalBudget: (map(.salary) | add)})' employees.json",
      "explanation": "jq 'group_by(.department)' (group employees by department) | map({...}) (transform each group) dept: .[0].department (department name from first item), count: length (group size), avgSalary: (map(.salary) | add / length) (calculate average), totalBudget: (map(.salary) | add) (sum all salaries). Complex aggregation with multiple statistics.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Generate department statistics and budget reports from employee data"
    },
    {
      "id": 171,
      "command": [
        "jq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "json",
        "arrays",
        "manipulation"
      ],
      "task": "Sort JSON array by multiple fields with custom ordering",
      "solution": "jq 'sort_by(.priority, -.date, .name)' tasks.json",
      "explanation": "jq 'sort_by(.priority, -.date, .name)' (sort by multiple criteria in order: first by priority ascending, then by date descending using -, finally by name ascending). Multi-level sorting with mixed sort directions.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Sort task lists or data records by multiple criteria"
    },
    {
      "id": 172,
      "command": [
        "jq"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "flattening",
        "restructure"
      ],
      "task": "Flatten nested JSON and create denormalized structure",
      "solution": "jq '.orders[] | {orderId: .id, customerName: .customer.name, customerEmail: .customer.email, itemName: .items[].name, itemPrice: .items[].price}' orders.json",
      "explanation": "jq '.orders[]' (iterate orders) | {...} (create flat structure) orderId: .id, customerName: .customer.name, customerEmail: .customer.email (extract nested customer data), itemName: .items[].name, itemPrice: .items[].price (flatten items array). Creates denormalized records from nested structure.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Prepare nested data for CSV export or database insertion"
    },
    {
      "id": 173,
      "command": [
        "jq",
        "curl"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": [
        "json",
        "api",
        "processing"
      ],
      "task": "Fetch API data and extract specific information with error handling",
      "solution": "curl -s https://api.github.com/users/octocat | jq 'if .message then \"Error: \" + .message else {name: .name, repos: .public_repos, followers: .followers} end'",
      "explanation": "curl -s https://api.github.com/users/octocat (fetch GitHub API data silently) | jq 'if .message then \"Error: \" + .message' (check for API error message) else {name: .name, repos: .public_repos, followers: .followers} end (extract specific fields if successful). API integration with error handling.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq",
        "curl"
      ],
      "warnings": null,
      "use_case": "API integration with error handling and data extraction"
    },
    {
      "id": 174,
      "command": [
        "jq"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "time series",
        "analysis"
      ],
      "task": "Analyze time series data and calculate trends",
      "solution": "jq '[.[] | select(.timestamp >= \"2024-01-01\")] | group_by(.date) | map({date: .[0].date, total: (map(.value) | add), avg: (map(.value) | add / length)})' timeseries.json",
      "explanation": "jq '[.[] | select(.timestamp >= \"2024-01-01\")]' (filter data from 2024 onwards) | group_by(.date) (group by date) | map({date: .[0].date, total: (map(.value) | add), avg: (map(.value) | add / length)}) (calculate daily totals and averages). Time series aggregation and analysis.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Time series analysis for metrics and business intelligence"
    },
    {
      "id": 175,
      "command": [
        "jq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "json",
        "null handling",
        "cleanup"
      ],
      "task": "Clean JSON data by removing null values and empty objects",
      "solution": "jq 'walk(if type == \"object\" then with_entries(select(.value != null and .value != \"\")) else . end)' messy.json",
      "explanation": "jq 'walk(...)' (recursively traverse all values) if type == \"object\" then with_entries(select(.value != null and .value != \"\")) (for objects, keep only entries with non-null, non-empty values) else . end (leave non-objects unchanged). Deep data cleaning with recursive traversal.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Data cleaning and preparation for analysis or storage"
    },
    {
      "id": 176,
      "command": [
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "csv",
        "json",
        "conversion"
      ],
      "task": "Convert CSV file to JSON format with proper field mapping",
      "solution": "awk -F',' 'NR==1{for(i=1;i<=NF;i++)h[i]=$i} NR>1{printf \"{%s}\\n\", substr((for(i=1;i<=NF;i++)printf \"\\\"%s\\\":\\\"%s\\\",\",h[i],$i),1,length()-1)}' data.csv",
      "explanation": "awk -F',' (comma delimiter) NR==1{for(i=1;i<=NF;i++)h[i]=$i} (store headers in array) NR>1{...} (for data rows, create JSON objects using headers as keys and current row values). Complex CSV to JSON conversion preserving field names.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Convert spreadsheet data to JSON for web applications"
    },
    {
      "id": 177,
      "command": [
        "jq"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "json",
        "csv",
        "conversion"
      ],
      "task": "Convert JSON array to CSV with custom headers and formatting",
      "solution": "jq -r '(.[0] | keys_unsorted) as $keys | $keys, (.[] | [.[ $keys[] ]] | @csv)' data.json",
      "explanation": "jq -r (raw output without quotes) (.[0] | keys_unsorted) as $keys (get field names from first object, preserve order) | $keys, (.[] | [.[ $keys[] ]] | @csv) (print headers, then for each object extract values in key order and format as CSV). JSON to CSV conversion with proper headers.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Export JSON API data to CSV for spreadsheet analysis"
    },
    {
      "id": 178,
      "command": [
        "awk"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": [
        "csv",
        "filtering",
        "conditional"
      ],
      "task": "Filter CSV rows based on multiple column conditions",
      "solution": "awk -F',' '$3 > 1000 && $4 == \"active\" && $2 ~ /^[A-M]/ {print}' customers.csv",
      "explanation": "awk -F',' (comma delimiter) '$3 > 1000 && $4 == \"active\" && $2 ~ /^[A-M]/' (multiple conditions: column 3 > 1000, column 4 equals 'active', column 2 starts with A-M using regex) {print} (print matching rows). Multi-criteria CSV filtering.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Filter customer data for targeted marketing campaigns"
    },
    {
      "id": 179,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "csv",
        "pivoting",
        "aggregation"
      ],
      "task": "Create pivot table from CSV data with sum aggregation",
      "solution": "awk -F',' 'NR>1 {sum[$2][$3] += $4} END {for(i in sum) {printf \"%s\", i; for(j in sum[i]) printf \",%s\", sum[i][j]; print \"\"}}' sales.csv",
      "explanation": "awk -F',' NR>1 {sum[$2][$3] += $4} (skip header, create 2D array where sum[row_category][column_category] accumulates values) END {for(i in sum) {...}} (output pivot table: for each row category, print column values). Creates pivot table with sum aggregation.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Create sales reports and business intelligence summaries"
    },
    {
      "id": 180,
      "command": [
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "csv",
        "validation",
        "data quality"
      ],
      "task": "Validate CSV data and report inconsistencies",
      "solution": "awk -F',' 'NR>1 {if(NF != 5) print \"Line \" NR \": Wrong number of fields\"; if($3 !~ /^[0-9]+$/) print \"Line \" NR \": Invalid number in column 3\"}' data.csv",
      "explanation": "awk -F',' NR>1 (skip header) {if(NF != 5) print \"Line \" NR \": Wrong number of fields\"} (check field count) if($3 !~ /^[0-9]+$/) print \"Line \" NR \": Invalid number in column 3\" (validate numeric field with regex). Data quality validation with specific error reporting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Data quality checks before importing into databases"
    },
    {
      "id": 181,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "csv",
        "statistics",
        "analysis"
      ],
      "task": "Calculate comprehensive statistics for CSV numeric columns",
      "solution": "awk -F',' 'NR>1 {sum+=$3; if($3>max||NR==2) max=$3; if($3<min||NR==2) min=$3; count++} END {printf \"Count: %d, Sum: %.2f, Avg: %.2f, Min: %.2f, Max: %.2f\\n\", count, sum, sum/count, min, max}' data.csv",
      "explanation": "awk -F',' NR>1 {sum+=$3; ...count++} (accumulate sum, track min/max, count records) END {printf...} (calculate and display comprehensive statistics: count, sum, average, minimum, maximum). Statistical analysis of numeric column.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Generate statistical summaries for data analysis and reporting"
    },
    {
      "id": 182,
      "command": [
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "csv",
        "deduplication",
        "unique"
      ],
      "task": "Remove duplicate rows from CSV file based on specific columns",
      "solution": "awk -F',' '!seen[$1,$2]++' data.csv",
      "explanation": "awk -F',' '!seen[$1,$2]++' (use combination of columns 1 and 2 as key in seen array, print only if not seen before). The ++ operator increments after returning current value, so first occurrence returns 0 (false), ! makes it true for printing. Efficient deduplication.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Clean datasets by removing duplicate customer or transaction records"
    },
    {
      "id": 183,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "csv",
        "joining",
        "merge"
      ],
      "task": "Join two CSV files on common field like SQL join",
      "solution": "awk -F',' 'FNR==NR {a[$1]=$2; next} $1 in a {print $0 \",\" a[$1]}' lookup.csv data.csv",
      "explanation": "awk -F',' FNR==NR {a[$1]=$2; next} (first file: store key-value pairs in array a, FNR==NR identifies first file) $1 in a {print $0 \",\" a[$1]} (second file: if key exists in lookup, print record with joined value). SQL-like inner join operation.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Enrich data by joining customer information with transaction data"
    },
    {
      "id": 184,
      "command": [
        "awk"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "data analysis",
      "tags": [
        "csv",
        "transformation",
        "formatting"
      ],
      "task": "Transform CSV columns with date formatting and calculations",
      "solution": "awk -F',' 'BEGIN{OFS=\",\"} NR>1 {gsub(/-/, \"/\", $2); $4 = $3 * 1.1; print}' sales.csv",
      "explanation": "awk -F',' BEGIN{OFS=\",\"} (set output field separator) NR>1 {gsub(/-/, \"/\", $2)} (skip header, replace hyphens with slashes in date field) $4 = $3 * 1.1 (calculate new value: 10% markup) print (output transformed record). Data transformation with formatting and calculations.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Data preparation with date standardization and price calculations"
    },
    {
      "id": 185,
      "command": [
        "cut",
        "sort",
        "uniq"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "data analysis",
      "tags": [
        "csv",
        "unique values",
        "column analysis"
      ],
      "task": "Extract unique values from specific CSV column with counts",
      "solution": "cut -d',' -f3 data.csv | sort | uniq -c | sort -nr",
      "explanation": "cut -d',' -f3 data.csv (extract 3rd column using comma delimiter) | sort (sort values for uniq) | uniq -c (count unique occurrences) | sort -nr (sort by count, descending). Pipeline for categorical data analysis with frequency counts.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Analyze categorical data distribution in CSV files"
    },
    {
      "id": 186,
      "command": [
        "awk",
        "sort"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "csv",
        "ranking",
        "top n"
      ],
      "task": "Find top N records in CSV based on numeric column with ties handling",
      "solution": "awk -F',' 'NR>1 {print $3 \",\" $0}' data.csv | sort -t',' -k1 -nr | head -10 | cut -d',' -f2-",
      "explanation": "awk -F',' NR>1 {print $3 \",\" $0} (prepend sort key to each record) | sort -t',' -k1 -nr (sort by first field numerically, descending) | head -10 (take top 10) | cut -d',' -f2- (remove prepended sort key). Top-N selection with proper numeric sorting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Identify top performers, best-selling products, or highest values"
    },
    {
      "id": 187,
      "command": [
        "jq",
        "awk"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "csv",
        "workflow"
      ],
      "task": "Complex workflow: CSV to JSON transformation with validation and enrichment",
      "solution": "awk -F',' 'NR==1{for(i=1;i<=NF;i++)h[i]=$i;next} {printf \"{\"; for(i=1;i<=NF;i++) printf \"\\\"%s\\\":\\\"%s\\\"%s\", h[i], $i, (i==NF?\"\":\";\"); print \"}\" }' data.csv | jq 'select(.email | test(\"@.*\\\\.\")) | . + {\"timestamp\": now, \"processed\": true}'",
      "explanation": "awk... (convert CSV to JSON with headers as keys) | jq 'select(.email | test(\"@.*\\\\.\"))' (validate email format with regex) | . + {\"timestamp\": now, \"processed\": true} (enrich with timestamp and processing flag). Complete ETL pipeline with conversion, validation, and enrichment.",
      "execution_time": "1-5 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "ETL pipeline for processing and validating business data"
    },
    {
      "id": 188,
      "command": [
        "jq"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "nested arrays",
        "complex"
      ],
      "task": "Process nested JSON arrays and create flat summary report",
      "solution": "jq '.companies[] | {name: .name, totalEmployees: (.departments[] | .employees | length) | add, avgSalary: ([.departments[].employees[].salary] | add / length)}' companies.json",
      "explanation": "jq '.companies[]' (iterate companies) | {name: .name, totalEmployees: (.departments[] | .employees | length) | add (sum employee counts across departments), avgSalary: ([.departments[].employees[].salary] | add / length)} (flatten all salaries and calculate average). Complex nested data aggregation.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Create executive summaries from complex organizational data"
    },
    {
      "id": 189,
      "command": [
        "jq"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "conditional logic",
        "advanced"
      ],
      "task": "Implement complex business logic with conditional transformations",
      "solution": "jq '.customers[] | if .orderTotal > 1000 then . + {\"tier\": \"premium\", \"discount\": 0.15} elif .orderTotal > 500 then . + {\"tier\": \"gold\", \"discount\": 0.10} else . + {\"tier\": \"standard\", \"discount\": 0.05} end' customers.json",
      "explanation": "jq '.customers[]' (iterate customers) | if .orderTotal > 1000 then ... elif .orderTotal > 500 then ... else ... end (nested conditional logic) . + {\"tier\": \"...\", \"discount\": ...} (enrich object with tier and discount based on order total). Business rule implementation with tiered logic.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Customer tier assignment and dynamic pricing calculations"
    },
    {
      "id": 190,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "csv",
        "time series",
        "windowing"
      ],
      "task": "Calculate moving averages and trends in time series CSV data",
      "solution": "awk -F',' 'NR>1 {values[NR-1]=$3; if(NR>4) {sum=0; for(i=NR-4;i<=NR-1;i++) sum+=values[i]; printf \"%s,%.2f\\n\", $1, sum/4}}' timeseries.csv",
      "explanation": "awk -F',' NR>1 {values[NR-1]=$3} (store values in array) if(NR>4) {sum=0; for(i=NR-4;i<=NR-1;i++) sum+=values[i]} (after 4 values, calculate sum of last 4) printf \"%s,%.2f\\n\", $1, sum/4 (output date and 4-period moving average). Time series analysis with sliding window.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Financial analysis and trend detection in time series data"
    },
    {
      "id": 191,
      "command": [
        "jq",
        "date"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "json",
        "timestamps",
        "date manipulation"
      ],
      "task": "Process JSON with timestamp manipulation and date-based filtering",
      "solution": "jq --arg cutoff \"$(date -d '30 days ago' '+%Y-%m-%d')\" '.events[] | select(.date >= $cutoff) | . + {\"daysAgo\": ((now - (.date | strptime(\"%Y-%m-%d\") | mktime)) / 86400 | floor)}'",
      "explanation": "jq --arg cutoff \"$(date -d '30 days ago' '+%Y-%m-%d')\" (pass shell command result as jq variable) '.events[] | select(.date >= $cutoff)' (filter events from last 30 days) | . + {\"daysAgo\": ((now - (.date | strptime(\"%Y-%m-%d\") | mktime)) / 86400 | floor)} (add calculated field for days ago). Date arithmetic and filtering integration.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq",
        "date"
      ],
      "warnings": null,
      "use_case": "Event processing with relative date calculations and filtering"
    },
    {
      "id": 192,
      "command": [
        "awk",
        "jq"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "csv",
        "json",
        "advanced pipeline"
      ],
      "task": "Create advanced data processing pipeline: CSV analysis to JSON report",
      "solution": "awk -F',' 'NR>1 {dept[$2]++; sales[$2]+=$4} END {for(d in dept) printf \"{\\\"department\\\":\\\"%s\\\",\\\"employees\\\":%d,\\\"totalSales\\\":%.2f}\\n\", d, dept[d], sales[d]}' sales.csv | jq -s 'sort_by(-.totalSales) | {\"reportDate\": now, \"departments\": ., \"summary\": {\"totalDepts\": length, \"totalSales\": (map(.totalSales) | add)}}'",
      "explanation": "awk -F',' 'NR>1 {dept[$2]++; sales[$2]+=$4}' (skip header, count employees per department, sum sales per department) END {for(d in dept) printf...} (output JSON objects for each department) | jq -s 'sort_by(-.totalSales)' (slurp into array, sort by sales descending) | {\"reportDate\": now, \"departments\": ., \"summary\": {...}} (create final report with metadata and summary statistics). Complete business intelligence pipeline from CSV to formatted JSON report.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Business intelligence pipeline for executive dashboard generation"
    },
    {
      "id": 193,
      "command": [
        "for"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "scripting",
      "tags": [
        "for loop",
        "basic",
        "iteration"
      ],
      "task": "Print numbers from 1 to 10 using for loop",
      "solution": "for i in {1..10}; do echo $i; done",
      "explanation": "for i in {1..10} (brace expansion creates sequence 1, 2, 3...10) do echo $i (print current number) done (end loop). Bash brace expansion {start..end} generates numeric sequences efficiently.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Basic iteration for numbering, indexing, or simple counting operations"
    },
    {
      "id": 194,
      "command": [
        "for"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "for loop",
        "files",
        "batch processing"
      ],
      "task": "Rename all .txt files by adding current date prefix",
      "solution": "for file in *.txt; do mv \"$file\" \"$(date +%Y%m%d)_$file\"; done",
      "explanation": "for file in *.txt (iterate through all .txt files in current directory) do mv \"$file\" \"$(date +%Y%m%d)_$file\" (rename each file with YYYYMMDD prefix using command substitution) done. Quotes protect filenames with spaces.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Batch file renaming with timestamps for organization or backup purposes"
    },
    {
      "id": 195,
      "command": [
        "for",
        "mkdir"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": [
        "for loop",
        "directory creation",
        "structure"
      ],
      "task": "Create project directory structure for months of the year",
      "solution": "for month in Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec; do mkdir -p \"2024_$month\"; done",
      "explanation": "for month in Jan Feb Mar... (iterate through month abbreviations) do mkdir -p \"2024_$month\" (create directory with year prefix, -p creates parent dirs if needed) done. Creates organized directory structure for project planning.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Project organization, creating systematic directory structures for planning"
    },
    {
      "id": 196,
      "command": [
        "for",
        "cp"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": [
        "for loop",
        "backup",
        "multiple copies"
      ],
      "task": "Create multiple numbered backups of important file",
      "solution": "for i in {1..5}; do cp important.txt \"backup_${i}_$(date +%H%M%S).txt\"; sleep 1; done",
      "explanation": "for i in {1..5} (create 5 backups) do cp important.txt \"backup_${i}_$(date +%H%M%S).txt\" (copy with numbered and timestamped filename) sleep 1 (wait 1 second for different timestamps) done. Creates multiple backup versions with unique names.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Create multiple backup versions before risky operations or for version control"
    },
    {
      "id": 197,
      "command": [
        "for",
        "find"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "for loop",
        "file processing",
        "compression"
      ],
      "task": "Find and compress all log files in multiple directories",
      "solution": "for dir in /var/log /home/user/logs /opt/app/logs; do find \"$dir\" -name \"*.log\" -exec gzip {} \\; 2>/dev/null; done",
      "explanation": "for dir in /var/log /home/user/logs /opt/app/logs (iterate through multiple log directories) do find \"$dir\" -name \"*.log\" -exec gzip {} \\; (find and compress log files) 2>/dev/null (suppress error messages) done. Systematic log compression across directories.",
      "execution_time": "1-5 min",
      "requirements": [
        "gzip"
      ],
      "warnings": "Ensure log files are not actively being written to",
      "use_case": "System maintenance and log rotation across multiple applications"
    },
    {
      "id": 198,
      "command": [
        "for",
        "curl"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": [
        "for loop",
        "api",
        "monitoring"
      ],
      "task": "Check multiple website endpoints and report status",
      "solution": "for url in https://api1.com/health https://api2.com/status https://api3.com/ping; do echo -n \"$url: \"; curl -s -o /dev/null -w \"%{http_code}\" \"$url\" || echo \"FAILED\"; echo; done",
      "explanation": "for url in https://api1.com/health... (iterate through API endpoints) do echo -n \"$url: \" (print URL without newline) curl -s -o /dev/null -w \"%{http_code}\" \"$url\" (silent request, discard output, show only HTTP status) || echo \"FAILED\" (if curl fails) echo (add newline). Systematic endpoint monitoring.",
      "execution_time": "< 1 min",
      "requirements": [
        "curl"
      ],
      "warnings": null,
      "use_case": "API health monitoring and service availability checking"
    },
    {
      "id": 199,
      "command": [
        "for",
        "ssh"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "system admin",
      "tags": [
        "for loop",
        "remote",
        "deployment"
      ],
      "task": "Deploy script to multiple servers and execute remotely",
      "solution": "for server in web1.com web2.com web3.com; do echo \"Deploying to $server\"; scp deploy.sh \"$server:/tmp/\" && ssh \"$server\" 'chmod +x /tmp/deploy.sh && /tmp/deploy.sh'; done",
      "explanation": "for server in web1.com web2.com web3.com (iterate through server list) do echo \"Deploying to $server\" (show progress) scp deploy.sh \"$server:/tmp/\" (copy script to server) && ssh \"$server\" 'chmod +x /tmp/deploy.sh && /tmp/deploy.sh' (if copy succeeds, make executable and run). Automated deployment workflow.",
      "execution_time": "1-5 min",
      "requirements": [
        "ssh",
        "scp"
      ],
      "warnings": "Ensure SSH keys are configured for passwordless access",
      "use_case": "Automated deployment and configuration management across server fleet"
    },
    {
      "id": 200,
      "command": [
        "for",
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "for loop",
        "csv processing",
        "reports"
      ],
      "task": "Process multiple CSV files and generate summary report",
      "solution": "for file in *.csv; do echo \"Processing $file:\"; awk -F',' 'NR>1 {sum+=$3; count++} END {printf \"Records: %d, Total: %.2f, Average: %.2f\\n\", count, sum, sum/count}' \"$file\"; done",
      "explanation": "for file in *.csv (iterate through CSV files) do echo \"Processing $file:\" (show current file) awk -F',' 'NR>1 {sum+=$3; count++} END {...}' (skip header, sum column 3, calculate statistics) \"$file\" done. Batch CSV analysis with individual file reporting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Batch data analysis for financial reports or statistical summaries"
    },
    {
      "id": 201,
      "command": [
        "while"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "while loop",
        "file monitoring",
        "real-time"
      ],
      "task": "Monitor file size and alert when it exceeds limit",
      "solution": "while true; do size=$(stat -f%z file.log 2>/dev/null || stat -c%s file.log 2>/dev/null); if [ \"$size\" -gt 1048576 ]; then echo \"File size exceeded 1MB: $size bytes\"; break; fi; sleep 5; done",
      "explanation": "while true (infinite loop) do size=$(stat -f%z file.log 2>/dev/null || stat -c%s file.log 2>/dev/null) (get file size, try macOS then Linux format) if [ \"$size\" -gt 1048576 ] (if size > 1MB) then echo... break (alert and exit loop) sleep 5 (wait 5 seconds) done. Continuous file size monitoring.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Press Ctrl+C to stop if size limit not reached",
      "use_case": "Monitor log file growth and trigger alerts or rotation when limits exceeded"
    },
    {
      "id": 202,
      "command": [
        "while",
        "read"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "data processing",
      "tags": [
        "while loop",
        "file reading",
        "line processing"
      ],
      "task": "Read file line by line and process each line with custom logic",
      "solution": "while IFS= read -r line; do echo \"Processing: $line\"; echo \"$line\" | tr '[:lower:]' '[:upper:]' >> output.txt; done < input.txt",
      "explanation": "while IFS= read -r line (read line preserving whitespace, -r prevents backslash interpretation) do echo \"Processing: $line\" (show progress) echo \"$line\" | tr '[:lower:]' '[:upper:]' >> output.txt (convert to uppercase, append to output) done < input.txt (read from input file). Line-by-line file processing.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Text file transformation, data cleaning, or custom line-by-line processing"
    },
    {
      "id": 203,
      "command": [
        "while",
        "ps"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "while loop",
        "process monitoring",
        "system"
      ],
      "task": "Monitor specific process and restart if it stops running",
      "solution": "while true; do if ! pgrep -f \"my_application\" > /dev/null; then echo \"$(date): Process not found, restarting...\"; /path/to/my_application & fi; sleep 30; done",
      "explanation": "while true (continuous monitoring) do if ! pgrep -f \"my_application\" > /dev/null (if process not found, pgrep searches by name) then echo \"$(date): Process not found, restarting...\" (timestamped alert) /path/to/my_application & (restart in background) sleep 30 (check every 30 seconds) done. Process watchdog implementation.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Ensure application path is correct and executable",
      "use_case": "Service monitoring and automatic restart for critical applications"
    },
    {
      "id": 204,
      "command": [
        "while",
        "netstat"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "network",
      "tags": [
        "while loop",
        "network monitoring",
        "connections"
      ],
      "task": "Monitor network connections and log suspicious activity",
      "solution": "while true; do netstat -tn | awk '/ESTABLISHED/ && $5 !~ /^192\\.168|^10\\.|^172\\.(1[6-9]|2[0-9]|3[01])/ {print strftime(\"%Y-%m-%d %H:%M:%S\"), \"External connection:\", $5}' >> suspicious.log; sleep 60; done",
      "explanation": "while true (continuous monitoring) do netstat -tn (show TCP connections numerically) | awk '/ESTABLISHED/ && $5 !~ /^192\\.168|^10\\.|^172\\.(1[6-9]|2[0-9]|3[01])/' (filter established connections from non-private IPs) {print strftime(...), \"External connection:\", $5} (log with timestamp) >> suspicious.log; sleep 60 done. Network security monitoring.",
      "execution_time": "long-running",
      "requirements": [
        "netstat",
        "awk"
      ],
      "warnings": null,
      "use_case": "Security monitoring for detecting external network connections"
    },
    {
      "id": 205,
      "command": [
        "for",
        "git"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "version control",
      "tags": [
        "for loop",
        "git",
        "repository management"
      ],
      "task": "Update multiple git repositories in parallel",
      "solution": "for repo in ~/projects/*/; do (cd \"$repo\" && echo \"Updating $(basename \"$repo\")\" && git pull origin main 2>&1) & done; wait",
      "explanation": "for repo in ~/projects/*/ (iterate through subdirectories in projects) do (cd \"$repo\" && echo \"Updating $(basename \"$repo\")\" && git pull origin main 2>&1) & (subshell: change to repo, show name, pull updates, run in background with &) done; wait (wait for all background jobs to complete). Parallel git repository updates.",
      "execution_time": "1-5 min",
      "requirements": [
        "git"
      ],
      "warnings": "Ensure repositories have no uncommitted changes",
      "use_case": "Mass repository updates for development environment synchronization"
    },
    {
      "id": 206,
      "command": [
        "while",
        "df"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "while loop",
        "disk monitoring",
        "alerts"
      ],
      "task": "Monitor disk usage and send alert when threshold exceeded",
      "solution": "while true; do usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//'); if [ \"$usage\" -gt 80 ]; then echo \"ALERT: Disk usage at ${usage}% on $(date)\" | mail -s \"Disk Space Warning\" admin@company.com; fi; sleep 3600; done",
      "explanation": "while true (continuous monitoring) do usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//') (get disk usage percentage, remove % symbol) if [ \"$usage\" -gt 80 ] (if usage > 80%) then echo... | mail (send email alert) sleep 3600 (check hourly) done. Automated disk space monitoring with email alerts.",
      "execution_time": "long-running",
      "requirements": [
        "mail"
      ],
      "warnings": "Configure mail system for email alerts",
      "use_case": "Proactive system monitoring and automated alerting for disk space management"
    },
    {
      "id": 207,
      "command": [
        "for",
        "imagemagick"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "multimedia",
      "tags": [
        "for loop",
        "image processing",
        "batch"
      ],
      "task": "Batch resize and optimize images with different sizes",
      "solution": "for size in 100x100 300x300 800x600; do mkdir -p \"resized_$size\"; for img in *.jpg; do convert \"$img\" -resize \"$size>\" -quality 85 \"resized_$size/$img\"; done; done",
      "explanation": "for size in 100x100 300x300 800x600 (iterate through target sizes) do mkdir -p \"resized_$size\" (create output directory) for img in *.jpg (nested loop through images) do convert \"$img\" -resize \"$size>\" (resize only if larger, preserve aspect ratio) -quality 85 \"resized_$size/$img\" (optimize quality, save to size-specific directory). Nested loops for multi-size image processing.",
      "execution_time": "5+ min",
      "requirements": [
        "imagemagick"
      ],
      "warnings": null,
      "use_case": "Web development image optimization for responsive design and different screen sizes"
    },
    {
      "id": 208,
      "command": [
        "while",
        "tail"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "monitoring",
      "tags": [
        "while loop",
        "log monitoring",
        "pattern detection"
      ],
      "task": "Monitor log file for error patterns and trigger actions",
      "solution": "tail -f application.log | while read line; do if echo \"$line\" | grep -q \"ERROR\\|FATAL\"; then echo \"$(date): Found error: $line\" >> error_alerts.log; echo \"Error detected\" | mail -s \"Application Error\" admin@company.com; fi; done",
      "explanation": "tail -f application.log (follow log file) | while read line (read each new line) do if echo \"$line\" | grep -q \"ERROR\\|FATAL\" (check for error patterns) then echo \"$(date): Found error: $line\" >> error_alerts.log (log with timestamp) echo \"Error detected\" | mail... (send email alert) done. Real-time log monitoring with automated response.",
      "execution_time": "long-running",
      "requirements": [
        "mail"
      ],
      "warnings": "Press Ctrl+C to stop monitoring",
      "use_case": "Production system monitoring with immediate error response and notification"
    },
    {
      "id": 209,
      "command": [
        "for",
        "jq"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "for loop",
        "json",
        "api processing"
      ],
      "task": "Fetch and merge data from multiple API endpoints",
      "solution": "for endpoint in users orders products; do curl -s \"https://api.example.com/$endpoint\" | jq '.' > \"${endpoint}.json\"; done; jq -s '{users: .[0], orders: .[1], products: .[2]}' users.json orders.json products.json > merged_data.json",
      "explanation": "for endpoint in users orders products (iterate through API endpoints) do curl -s \"https://api.example.com/$endpoint\" (fetch data silently) | jq '.' > \"${endpoint}.json\" (format and save JSON) done; jq -s '{users: .[0], orders: .[1], products: .[2]}' (slurp files and create merged structure) > merged_data.json. API data aggregation workflow.",
      "execution_time": "< 1 min",
      "requirements": [
        "curl",
        "jq"
      ],
      "warnings": null,
      "use_case": "Data integration from multiple microservices or API sources"
    },
    {
      "id": 210,
      "command": [
        "while",
        "inotifywait"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "monitoring",
      "tags": [
        "while loop",
        "file watching",
        "automation"
      ],
      "task": "Watch directory for file changes and auto-process new files",
      "solution": "while inotifywait -e create -e moved_to /watch/directory; do for file in /watch/directory/*.txt; do if [ -f \"$file\" ]; then echo \"Processing $file\"; mv \"$file\" /processed/; fi; done; done",
      "explanation": "while inotifywait -e create -e moved_to /watch/directory (wait for file creation or move events) do for file in /watch/directory/*.txt (check for text files) do if [ -f \"$file\" ] (if file exists) then echo \"Processing $file\"; mv \"$file\" /processed/ (move to processed directory) done. Real-time file processing automation.",
      "execution_time": "long-running",
      "requirements": [
        "inotify-tools"
      ],
      "warnings": null,
      "use_case": "Automated file processing workflow for document management or data ingestion"
    },
    {
      "id": 211,
      "command": [
        "for",
        "awk",
        "sort"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "for loop",
        "log analysis",
        "statistics"
      ],
      "task": "Analyze multiple log files and generate comprehensive report",
      "solution": "for log in *.log; do echo \"=== $log ===\"; awk '{print $1}' \"$log\" | sort | uniq -c | sort -nr | head -5; awk '/ERROR/ {count++} END {print \"Errors:\", count+0}' \"$log\"; echo; done > analysis_report.txt",
      "explanation": "for log in *.log (iterate through log files) do echo \"=== $log ===\" (file separator) awk '{print $1}' \"$log\" | sort | uniq -c | sort -nr | head -5 (top 5 IP addresses by frequency) awk '/ERROR/ {count++} END {print \"Errors:\", count+0}' (count errors) done > analysis_report.txt. Comprehensive log analysis with multiple metrics.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security analysis and system health reporting from multiple log sources"
    },
    {
      "id": 212,
      "command": [
        "while",
        "read",
        "mysql"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "database",
      "tags": [
        "while loop",
        "database",
        "batch processing"
      ],
      "task": "Read CSV file and insert data into database with error handling",
      "solution": "while IFS=',' read -r id name email status; do if mysql -u user -p'password' -e \"INSERT INTO users (id, name, email, status) VALUES ('$id', '$name', '$email', '$status');\" database; then echo \"Inserted: $name\"; else echo \"Failed: $name\" >> failed_inserts.log; fi; done < users.csv",
      "explanation": "while IFS=',' read -r id name email status (read CSV with comma delimiter) do if mysql -u user -p'password' -e \"INSERT INTO users...\" (attempt database insert) then echo \"Inserted: $name\" (success message) else echo \"Failed: $name\" >> failed_inserts.log (log failures) done < users.csv. Database bulk insert with error handling and logging.",
      "execution_time": "1-5 min",
      "requirements": [
        "mysql"
      ],
      "warnings": "Store database credentials securely, validate input data",
      "use_case": "ETL processes for importing CSV data into databases with error tracking"
    },
    {
      "id": 213,
      "command": [
        "for",
        "parallel"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "automation",
      "tags": [
        "for loop",
        "parallel processing",
        "optimization"
      ],
      "task": "Process large dataset files in parallel with load balancing",
      "solution": "for file in data_*.csv; do echo \"$file\"; done | parallel -j 4 'echo \"Processing {}\"; awk -F\",\" \"NR>1 {sum+=\\$3} END {print \\\"{}: Total \\\" sum}\" {}'",
      "explanation": "for file in data_*.csv; do echo \"$file\"; done (list files) | parallel -j 4 (process 4 files simultaneously) 'echo \"Processing {}\"; awk -F\",\" \"NR>1 {sum+=\\$3} END {print \\\"{}: Total \\\" sum}\" {}' (process each file: skip header, sum column 3, report total). Parallel processing for performance optimization.",
      "execution_time": "1-5 min",
      "requirements": [
        "parallel"
      ],
      "warnings": null,
      "use_case": "High-performance data processing for large datasets with optimal CPU utilization"
    },
    {
      "id": 214,
      "command": [
        "while",
        "sleep",
        "uptime"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "while loop",
        "system monitoring",
        "performance"
      ],
      "task": "Create system load monitoring script with threshold alerts",
      "solution": "while true; do load=$(uptime | awk '{print $(NF-2)}' | sed 's/,//'); if (( $(echo \"$load > 2.0\" | bc -l) )); then echo \"$(date): High load detected: $load\" | tee -a load_alerts.log; fi; sleep 60; done",
      "explanation": "while true (continuous monitoring) do load=$(uptime | awk '{print $(NF-2)}' | sed 's/,//') (extract 1-minute load average, remove comma) if (( $(echo \"$load > 2.0\" | bc -l) )) (compare float values using bc calculator) then echo... | tee -a load_alerts.log (log and display alert) sleep 60 (check every minute). System load monitoring with threshold alerting.",
      "execution_time": "long-running",
      "requirements": [
        "bc"
      ],
      "warnings": null,
      "use_case": "Server performance monitoring and load balancing decision support"
    },
    {
      "id": 215,
      "command": [
        "for",
        "rsync"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "backup",
      "tags": [
        "for loop",
        "backup",
        "synchronization"
      ],
      "task": "Synchronize multiple directory pairs with incremental backup",
      "solution": "for sync_pair in \"src1/:dest1/\" \"src2/:dest2/\" \"src3/:dest3/\"; do src=${sync_pair%:*}; dest=${sync_pair#*:}; echo \"Syncing $src to $dest\"; rsync -av --delete \"$src\" \"$dest\" && echo \"Success: $src\" || echo \"Failed: $src\"; done",
      "explanation": "for sync_pair in \"src1/:dest1/\"... (iterate through source:destination pairs) do src=${sync_pair%:*} (extract source path before colon) dest=${sync_pair#*:} (extract destination after colon) rsync -av --delete \"$src\" \"$dest\" (sync with archive mode and deletion) && echo \"Success\" || echo \"Failed\" (status reporting). Batch synchronization with error handling.",
      "execution_time": "5+ min",
      "requirements": [
        "rsync"
      ],
      "warnings": "Test sync pairs before running, --delete removes files from destination",
      "use_case": "Automated backup workflow for multiple directory pairs with status reporting"
    },
    {
      "id": 216,
      "command": [
        "while",
        "ping"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "network",
      "tags": [
        "while loop",
        "network monitoring",
        "connectivity"
      ],
      "task": "Monitor network connectivity and measure response times",
      "solution": "while true; do for host in google.com github.com stackoverflow.com; do ping -c 1 \"$host\" | awk '/time=/ {print strftime(\"%Y-%m-%d %H:%M:%S\"), \"'$host':\", $7}' || echo \"$(date): $host: UNREACHABLE\"; done; sleep 30; done",
      "explanation": "while true (continuous monitoring) do for host in google.com... (test multiple hosts) do ping -c 1 \"$host\" (send single ping) | awk '/time=/ {print strftime(...), \"'$host':\", $7}' (extract and format response time) || echo \"UNREACHABLE\" (if ping fails) sleep 30 (test every 30 seconds). Network connectivity monitoring with response time tracking.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": null,
      "use_case": "Network performance monitoring and connectivity verification for remote work"
    },
    {
      "id": 217,
      "command": [
        "for",
        "while",
        "find"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "automation",
      "tags": [
        "nested loops",
        "complex workflow",
        "file management"
      ],
      "task": "Complex nested loop for directory cleanup with age-based archiving",
      "solution": "for base_dir in /var/log /home/user/projects /opt/apps; do while read -r dir; do echo \"Processing: $dir\"; find \"$dir\" -type f -mtime +30 -name \"*.log\" -exec tar -czf \"${dir}/archive_$(date +%Y%m%d).tar.gz\" {} + -delete 2>/dev/null; done < <(find \"$base_dir\" -type d -maxdepth 2); done",
      "explanation": "for base_dir in /var/log... (iterate base directories) do while read -r dir (read each subdirectory) do find \"$dir\" -type f -mtime +30 -name \"*.log\" (find old log files) -exec tar -czf \"${dir}/archive_$(date +%Y%m%d).tar.gz\" {} + (archive them) -delete (remove originals) done < <(find \"$base_dir\" -type d -maxdepth 2) (process substitution for directory list). Complex nested automation for systematic cleanup.",
      "execution_time": "5+ min",
      "requirements": [
        "tar"
      ],
      "warnings": "Test on non-critical directories first, archives old files and deletes originals",
      "use_case": "Enterprise system maintenance with automated archiving and cleanup workflows"
    },
    {
      "id": 218,
      "command": [
        "date"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "date operations",
      "tags": [
        "date",
        "formatting",
        "timestamp"
      ],
      "task": "Display current date in various formats for logging and file naming",
      "solution": "echo \"ISO: $(date +%Y-%m-%d)\"; echo \"US: $(date +%m/%d/%Y)\"; echo \"Timestamp: $(date +%Y%m%d_%H%M%S)\"",
      "explanation": "date +%Y-%m-%d (ISO format YYYY-MM-DD) date +%m/%d/%Y (US format MM/DD/YYYY) date +%Y%m%d_%H%M%S (compact timestamp for filenames). Different format strings serve different purposes: ISO for standards compliance, US for regional preferences, compact for file naming.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Logging systems, file naming conventions, and timestamp standardization"
    },
    {
      "id": 219,
      "command": [
        "date"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "date operations",
      "tags": [
        "date",
        "calculation",
        "relative"
      ],
      "task": "Calculate dates relative to current date for backup rotation",
      "solution": "echo \"Today: $(date +%Y-%m-%d)\"; echo \"Yesterday: $(date -d 'yesterday' +%Y-%m-%d)\"; echo \"Last week: $(date -d '7 days ago' +%Y-%m-%d)\"; echo \"Next month: $(date -d '+1 month' +%Y-%m-%d)\"",
      "explanation": "date -d 'yesterday' (calculate yesterday's date) date -d '7 days ago' (date 7 days in the past) date -d '+1 month' (date 1 month in the future). The -d flag accepts natural language expressions for relative date calculations, useful for backup rotation and scheduling.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Backup rotation policies, log retention, and scheduled task planning"
    },
    {
      "id": 220,
      "command": [
        "date",
        "git"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "version control",
      "tags": [
        "date",
        "git",
        "commit",
        "automation"
      ],
      "task": "Create git commit with formatted timestamp and automated message",
      "solution": "git add . && git commit -m \"Auto-backup: $(date '+%Y-%m-%d %H:%M:%S') - $(git diff --cached --stat | wc -l) files changed\"",
      "explanation": "git add . (stage all changes) && git commit -m \"Auto-backup: $(date '+%Y-%m-%d %H:%M:%S') (formatted timestamp) - $(git diff --cached --stat | wc -l) files changed\" (count changed files in commit message). Creates descriptive commit messages with timestamp and change summary.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Automated backup commits with descriptive timestamps and change metrics"
    },
    {
      "id": 221,
      "command": [
        "date",
        "git"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "version control",
      "tags": [
        "date",
        "git",
        "log",
        "analysis"
      ],
      "task": "Analyze git commits from specific date range and generate report",
      "solution": "start_date=$(date -d '30 days ago' +%Y-%m-%d); git log --since=\"$start_date\" --pretty=format:'%h %ad %an %s' --date=short | awk '{print $2}' | sort | uniq -c | awk '{printf \"Date: %s, Commits: %d\\n\", $2, $1}'",
      "explanation": "start_date=$(date -d '30 days ago' +%Y-%m-%d) (calculate date 30 days ago) git log --since=\"$start_date\" --pretty=format:'%h %ad %an %s' --date=short (get commits since that date with hash, date, author, subject) | awk '{print $2}' | sort | uniq -c (extract dates, count commits per date) | awk '{printf...} (format output). Git activity analysis with date calculations.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Development productivity analysis and team activity reporting"
    },
    {
      "id": 222,
      "command": [
        "date",
        "find"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": [
        "date",
        "file cleanup",
        "age-based"
      ],
      "task": "Find and archive files older than specific date using date calculations",
      "solution": "cutoff_date=$(date -d '90 days ago' +%Y-%m-%d); find /data -type f -not -newermt \"$cutoff_date\" -exec tar -czf \"archive_$(date +%Y%m%d).tar.gz\" {} + && find /data -type f -not -newermt \"$cutoff_date\" -delete",
      "explanation": "cutoff_date=$(date -d '90 days ago' +%Y-%m-%d) (calculate cutoff date) find /data -type f -not -newermt \"$cutoff_date\" (find files not newer than cutoff date) -exec tar -czf \"archive_$(date +%Y%m%d).tar.gz\" {} + (archive old files) && find... -delete (delete archived files). Age-based file management with date calculations.",
      "execution_time": "5+ min",
      "requirements": [
        "tar"
      ],
      "warnings": "Test find command before adding -delete, archives and removes old files",
      "use_case": "Automated data retention policies and storage management"
    },
    {
      "id": 223,
      "command": [
        "date",
        "git"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "version control",
      "tags": [
        "date",
        "git",
        "branch",
        "workflow"
      ],
      "task": "Create feature branch with date-based naming and initial commit",
      "solution": "branch_name=\"feature/$(date +%Y%m%d)_$(whoami)_new_feature\"; git checkout -b \"$branch_name\" && echo \"# Feature started on $(date '+%Y-%m-%d %H:%M:%S')\" > FEATURE.md && git add FEATURE.md && git commit -m \"Initialize feature branch: $branch_name\"",
      "explanation": "branch_name=\"feature/$(date +%Y%m%d)_$(whoami)_new_feature\" (create branch name with date and username) git checkout -b \"$branch_name\" (create and switch to branch) echo \"# Feature started on $(date '+%Y-%m-%d %H:%M:%S')\" > FEATURE.md (create feature documentation with timestamp) git add FEATURE.md && git commit -m \"Initialize feature branch: $branch_name\" (commit initial documentation). Standardized feature branch workflow.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Standardized development workflow with traceable feature branches"
    },
    {
      "id": 224,
      "command": [
        "date",
        "cron"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "automation",
      "tags": [
        "date",
        "cron",
        "scheduling",
        "backup"
      ],
      "task": "Create cron job for daily backups with date-based file naming",
      "solution": "echo \"0 2 * * * tar -czf /backup/daily_backup_\\$(date +\\%Y\\%m\\%d).tar.gz /important/data && echo \\\"Backup completed: \\$(date)\\\" >> /var/log/backup.log\" | crontab -",
      "explanation": "echo \"0 2 * * *\" (daily at 2 AM) tar -czf /backup/daily_backup_\\$(date +\\%Y\\%m\\%d).tar.gz (create compressed backup with date in filename, escaping $ and % for cron) /important/data && echo \\\"Backup completed: \\$(date)\\\" >> /var/log/backup.log (log completion with timestamp) | crontab - (install cron job). Automated backup scheduling with date-based naming.",
      "execution_time": "< 1 min",
      "requirements": [
        "cron",
        "tar"
      ],
      "warnings": "Ensure backup directory exists and has sufficient space",
      "use_case": "Automated daily backups with organized file naming and logging"
    },
    {
      "id": 225,
      "command": [
        "date",
        "git"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "version control",
      "tags": [
        "date",
        "git",
        "release",
        "tagging"
      ],
      "task": "Create release tag with version numbering based on date and commit count",
      "solution": "version=\"v$(date +%Y.%m).$(git rev-list --count HEAD)\"; git tag -a \"$version\" -m \"Release $version - $(date '+%Y-%m-%d %H:%M:%S') - $(git log --oneline $(git describe --tags --abbrev=0)..HEAD | wc -l) commits since last tag\" && echo \"Created release: $version\"",
      "explanation": "version=\"v$(date +%Y.%m).$(git rev-list --count HEAD)\" (create version: vYYYY.MM.commit_count) git tag -a \"$version\" -m \"Release $version - $(date '+%Y-%m-%d %H:%M:%S') - $(git log --oneline $(git describe --tags --abbrev=0)..HEAD | wc -l) commits since last tag\" (create annotated tag with detailed message including timestamp and commit count since last tag). Automated semantic versioning with date-based releases.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Automated release management with semantic versioning and detailed release notes"
    },
    {
      "id": 226,
      "command": [
        "date",
        "awk"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": [
        "date",
        "log analysis",
        "time-based filtering"
      ],
      "task": "Analyze log files for entries within specific time window",
      "solution": "start_time=$(date -d '2 hours ago' '+%H:%M:%S'); end_time=$(date '+%H:%M:%S'); awk -v start=\"$start_time\" -v end=\"$end_time\" '$2 >= start && $2 <= end {print}' /var/log/application.log | wc -l",
      "explanation": "start_time=$(date -d '2 hours ago' '+%H:%M:%S') (calculate start time 2 hours ago) end_time=$(date '+%H:%M:%S') (current time) awk -v start=\"$start_time\" -v end=\"$end_time\" (pass times as awk variables) '$2 >= start && $2 <= end {print}' (filter log entries by time range, assuming time is in 2nd column) | wc -l (count matching entries). Time-based log analysis.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Performance monitoring and incident analysis within specific time windows"
    },
    {
      "id": 227,
      "command": [
        "date",
        "git"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "version control",
      "tags": [
        "date",
        "git",
        "statistics",
        "productivity"
      ],
      "task": "Generate development productivity report with date-based metrics",
      "solution": "echo \"Development Report - $(date '+%Y-%m-%d')\"; echo \"Last 7 days:\"; git log --since=\"7 days ago\" --author=\"$(git config user.name)\" --pretty=format:'%ad' --date=short | sort | uniq -c; echo \"Total commits: $(git log --since=\"7 days ago\" --author=\"$(git config user.name)\" --oneline | wc -l)\"",
      "explanation": "echo \"Development Report - $(date '+%Y-%m-%d')\" (report header with current date) git log --since=\"7 days ago\" --author=\"$(git config user.name)\" --pretty=format:'%ad' --date=short (get commit dates for current author in last 7 days) | sort | uniq -c (count commits per day) echo \"Total commits: $(git log --since=\"7 days ago\" --author=\"$(git config user.name)\" --oneline | wc -l)\" (total commit count). Personal productivity analysis.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Personal development tracking and productivity analysis for time management"
    },
    {
      "id": 228,
      "command": [
        "date",
        "mysql"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "database",
      "tags": [
        "date",
        "database",
        "backup",
        "automation"
      ],
      "task": "Create database backup with timestamp and retention management",
      "solution": "backup_date=$(date +%Y%m%d_%H%M%S); mysqldump -u user -p'password' database_name > \"backup_${backup_date}.sql\" && gzip \"backup_${backup_date}.sql\" && find /backup/path -name \"backup_*.sql.gz\" -mtime +7 -delete",
      "explanation": "backup_date=$(date +%Y%m%d_%H%M%S) (create timestamp for backup filename) mysqldump -u user -p'password' database_name > \"backup_${backup_date}.sql\" (create database dump with timestamped filename) && gzip \"backup_${backup_date}.sql\" (compress backup) && find /backup/path -name \"backup_*.sql.gz\" -mtime +7 -delete (clean up backups older than 7 days). Database backup with automated retention.",
      "execution_time": "1-5 min",
      "requirements": [
        "mysql",
        "gzip"
      ],
      "warnings": "Store database credentials securely, test restore procedures",
      "use_case": "Automated database backup with retention policies for production systems"
    },
    {
      "id": 229,
      "command": [
        "date",
        "git"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "version control",
      "tags": [
        "date",
        "git",
        "workflow",
        "automation"
      ],
      "task": "Automated daily standup report from git activity with date filtering",
      "solution": "today=$(date +%Y-%m-%d); yesterday=$(date -d 'yesterday' +%Y-%m-%d); echo \"Daily Standup - $today\"; echo \"Yesterday's work:\"; git log --since=\"$yesterday 00:00\" --until=\"$yesterday 23:59\" --author=\"$(git config user.name)\" --pretty=format:'- %s (%h)'; echo -e \"\\n\\nToday's plan:\"; git branch -r | grep feature | head -3",
      "explanation": "today=$(date +%Y-%m-%d); yesterday=$(date -d 'yesterday' +%Y-%m-%d) (calculate dates) echo \"Daily Standup - $today\" (report header) git log --since=\"$yesterday 00:00\" --until=\"$yesterday 23:59\" --author=\"$(git config user.name)\" --pretty=format:'- %s (%h)' (list yesterday's commits with subject and hash) git branch -r | grep feature | head -3 (show feature branches as today's plan). Automated standup report generation.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Agile development workflow automation and team communication enhancement"
    },
    {
      "id": 230,
      "command": [
        "date",
        "git",
        "jq"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "automation",
      "tags": [
        "date",
        "git",
        "json",
        "reporting"
      ],
      "task": "Generate comprehensive project status report in JSON format with date analytics",
      "solution": "report_date=$(date -Iseconds); git_stats=$(git log --since=\"30 days ago\" --pretty=format:'%ad,%an,%s' --date=short); echo \"{\\\"report_date\\\": \\\"$report_date\\\", \\\"project\\\": \\\"$(basename $(git rev-parse --show-toplevel))\\\", \\\"period\\\": \\\"last_30_days\\\", \\\"total_commits\\\": $(echo \"$git_stats\" | wc -l), \\\"active_days\\\": $(echo \"$git_stats\" | cut -d',' -f1 | sort -u | wc -l), \\\"contributors\\\": $(echo \"$git_stats\" | cut -d',' -f2 | sort -u | wc -l)}\" | jq '.'",
      "explanation": "report_date=$(date -Iseconds) (ISO 8601 timestamp) git_stats=$(git log --since=\"30 days ago\" --pretty=format:'%ad,%an,%s' --date=short) (get 30-day commit data) echo \"{...}\" (create JSON with project statistics: report date, project name from git root, total commits, active days, contributors) | jq '.' (format JSON). Comprehensive project analytics in structured format.",
      "execution_time": "< 1 min",
      "requirements": [
        "git",
        "jq"
      ],
      "warnings": null,
      "use_case": "Project management dashboards and automated reporting for stakeholder communication"
    },
    {
      "id": 231,
      "command": [
        "awk",
        "grep"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "log analysis",
      "tags": [
        "log parsing",
        "error extraction",
        "filtering"
      ],
      "task": "Extract all error messages from Apache log with timestamps",
      "solution": "awk '$9 >= 400 {print $4, $5, $7, $9, $10}' access.log | grep -v \"favicon\\|robots.txt\"",
      "explanation": "awk '$9 >= 400' (filter HTTP status codes 400 and above - client and server errors) {print $4, $5, $7, $9, $10} (print timestamp, request path, status code, response size) | grep -v \"favicon\\|robots.txt\" (exclude common non-critical requests). Extracts meaningful error entries from Apache access logs.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Web server troubleshooting and error pattern identification"
    },
    {
      "id": 232,
      "command": [
        "awk"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "log analysis",
      "tags": [
        "log statistics",
        "ip analysis",
        "frequency"
      ],
      "task": "Analyze top IP addresses by request count with bandwidth usage",
      "solution": "awk '{ip[$1]++; bytes[$1]+=$10} END {for(i in ip) printf \"%-15s %8d requests %12d bytes\\n\", i, ip[i], bytes[i]}' access.log | sort -k2 -nr | head -10",
      "explanation": "awk '{ip[$1]++; bytes[$1]+=$10}' (count requests per IP and sum bandwidth usage) END {for(i in ip) printf...} (format output with IP, request count, total bytes) | sort -k2 -nr (sort by request count descending) | head -10 (top 10 IPs). Comprehensive IP analysis with traffic metrics.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "DDoS detection, bandwidth analysis, and suspicious activity identification"
    },
    {
      "id": 233,
      "command": [
        "awk",
        "date"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": [
        "time-based analysis",
        "hourly stats",
        "traffic patterns"
      ],
      "task": "Generate hourly traffic statistics from log files",
      "solution": "awk '{gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; traffic[hour]++; bytes[hour]+=$10} END {for(h=0; h<24; h++) printf \"%02d:00 - %8d requests, %12d bytes\\n\", h, traffic[h]+0, bytes[h]+0}' access.log",
      "explanation": "awk '{gsub(/\\[|\\]/, \"\", $4)}' (remove brackets from timestamp) split($4, dt, \":\"); hour=dt[2] (extract hour from timestamp) traffic[hour]++; bytes[hour]+=$10 (count requests and sum bytes per hour) END {for(h=0; h<24; h++) printf...} (output 24-hour statistics with zero-padding). Hourly traffic pattern analysis.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Traffic pattern analysis, capacity planning, and peak hour identification"
    },
    {
      "id": 234,
      "command": [
        "grep",
        "awk",
        "sort"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "log analysis",
      "tags": [
        "security analysis",
        "attack detection",
        "suspicious patterns"
      ],
      "task": "Detect potential security attacks in web server logs",
      "solution": "grep -E \"(sql|script|alert|union|select|drop|exec|javascript|<script)\" access.log | awk '{print $1, $7}' | sort | uniq -c | sort -nr | awk '$1 > 5 {printf \"SUSPICIOUS IP: %-15s Count: %d Path: %s\\n\", $2, $1, $3}'",
      "explanation": "grep -E \"(sql|script|alert|union|select|drop|exec|javascript|<script)\" (find common attack patterns: SQL injection, XSS, script injection) | awk '{print $1, $7}' (extract IP and request path) | sort | uniq -c | sort -nr (count occurrences, sort by frequency) | awk '$1 > 5' (filter IPs with more than 5 suspicious requests). Security threat detection pipeline.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Web application security monitoring and attack pattern detection"
    },
    {
      "id": 235,
      "command": [
        "awk"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "log analysis",
      "tags": [
        "response time analysis",
        "performance metrics",
        "percentiles"
      ],
      "task": "Calculate response time statistics and identify slow requests",
      "solution": "awk '$NF ~ /^[0-9]+$/ {times[++i]=$NF; sum+=$NF; if($NF>max) max=$NF; if(min==\"\" || $NF<min) min=$NF} END {asort(times); n=length(times); p95=times[int(n*0.95)]; p99=times[int(n*0.99)]; printf \"Requests: %d\\nAvg: %.2fms\\nMin: %dms\\nMax: %dms\\n95th: %dms\\n99th: %dms\\n\", n, sum/n, min, max, p95, p99}' response_time.log",
      "explanation": "awk '$NF ~ /^[0-9]+$/' (ensure last field is numeric response time) {times[++i]=$NF; sum+=$NF; ...} (collect response times, calculate sum, min, max) END {asort(times); n=length(times)} (sort times array) p95=times[int(n*0.95)]; p99=times[int(n*0.99)] (calculate 95th and 99th percentiles) printf... (output comprehensive statistics). Performance analysis with percentile calculations.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Application performance monitoring and SLA compliance verification"
    },
    {
      "id": 236,
      "command": [
        "awk",
        "date"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": [
        "log rotation",
        "archive analysis",
        "time filtering"
      ],
      "task": "Parse rotated logs and extract entries from specific date range",
      "solution": "for log in access.log access.log.1 access.log.2.gz; do if [[ $log == *.gz ]]; then zcat \"$log\"; else cat \"$log\"; fi; done | awk -v start=\"$(date -d '7 days ago' '+%d/%b/%Y')\" -v end=\"$(date '+%d/%b/%Y')\" '$4 ~ start || $4 ~ end {print}'",
      "explanation": "for log in access.log access.log.1 access.log.2.gz (iterate through current and rotated logs) do if [[ $log == *.gz ]]; then zcat \"$log\"; else cat \"$log\"; fi (handle compressed logs with zcat, regular logs with cat) done | awk -v start=\"$(date -d '7 days ago' '+%d/%b/%Y')\" -v end=\"$(date '+%d/%b/%Y')\" (pass date range as awk variables) '$4 ~ start || $4 ~ end' (filter entries matching date range). Multi-log analysis across rotation.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Historical log analysis across log rotation periods"
    },
    {
      "id": 237,
      "command": [
        "awk",
        "jq"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "log analysis",
      "tags": [
        "json logs",
        "structured parsing",
        "application logs"
      ],
      "task": "Parse JSON application logs and extract error details with context",
      "solution": "awk '/^{.*}$/ {print}' app.log | jq -r 'select(.level == \"ERROR\" or .level == \"FATAL\") | \"\\(.timestamp) [\\(.level)] \\(.message) - Context: \\(.context // \"none\")\"' | awk '{count++} END {print \"\\nTotal errors found:\", count+0}'",
      "explanation": "awk '/^{.*}$/ {print}' (extract lines that look like JSON objects) | jq -r 'select(.level == \"ERROR\" or .level == \"FATAL\")' (filter error and fatal level messages) | \"\\(.timestamp) [\\(.level)] \\(.message) - Context: \\(.context // \"none\")\" (format output with timestamp, level, message, and context with fallback) | awk '{count++} END {print \"\\nTotal errors found:\", count+0}' (count total errors). Structured JSON log analysis.",
      "execution_time": "< 1 min",
      "requirements": [
        "jq"
      ],
      "warnings": null,
      "use_case": "Modern application debugging with structured logging analysis"
    },
    {
      "id": 238,
      "command": [
        "awk",
        "gnuplot"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "log visualization",
      "tags": [
        "log visualization",
        "graphing",
        "traffic trends"
      ],
      "task": "Generate traffic visualization from log data",
      "solution": "awk '{gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; traffic[hour]++} END {for(h=0; h<24; h++) printf \"%02d %d\\n\", h, traffic[h]+0}' access.log > traffic_data.txt && gnuplot -e \"set terminal png; set output 'traffic.png'; set xlabel 'Hour'; set ylabel 'Requests'; plot 'traffic_data.txt' with lines title 'Hourly Traffic'\"",
      "explanation": "awk '{gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; traffic[hour]++}' (extract hour from log timestamp and count requests) END {for(h=0; h<24; h++) printf \"%02d %d\\n\", h, traffic[h]+0} (output hourly data) > traffic_data.txt && gnuplot -e \"set terminal png; set output 'traffic.png'; set xlabel 'Hour'; set ylabel 'Requests'; plot 'traffic_data.txt' with lines title 'Hourly Traffic'\" (generate PNG graph). Log data visualization pipeline.",
      "execution_time": "< 1 min",
      "requirements": [
        "gnuplot"
      ],
      "warnings": null,
      "use_case": "Visual traffic analysis and presentation-ready charts for management"
    },
    {
      "id": 239,
      "command": [
        "awk",
        "geoip"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": [
        "geolocation",
        "ip analysis",
        "geographic stats"
      ],
      "task": "Analyze geographic distribution of web traffic",
      "solution": "awk '{print $1}' access.log | sort -u | while read ip; do country=$(geoiplookup \"$ip\" | awk -F': ' '/Country/ {print $2}' | cut -d',' -f1); echo \"$ip,$country\"; done | awk -F',' '{country[$2]++} END {for(c in country) printf \"%-20s %d\\n\", c, country[c]}' | sort -k2 -nr",
      "explanation": "awk '{print $1}' access.log | sort -u (extract unique IP addresses) | while read ip; do country=$(geoiplookup \"$ip\" | awk -F': ' '/Country/ {print $2}' | cut -d',' -f1) (lookup country for each IP) echo \"$ip,$country\" done | awk -F',' '{country[$2]++}' (count visits per country) END {for(c in country) printf...} | sort -k2 -nr (sort by visit count). Geographic traffic analysis with GeoIP lookup.",
      "execution_time": "1-5 min",
      "requirements": [
        "geoip"
      ],
      "warnings": "Requires GeoIP database installation",
      "use_case": "International traffic analysis and geographic user distribution insights"
    },
    {
      "id": 240,
      "command": [
        "awk",
        "mail"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log monitoring",
      "tags": [
        "alerting",
        "threshold monitoring",
        "automated response"
      ],
      "task": "Monitor error rate and send alerts when threshold exceeded",
      "solution": "awk 'BEGIN {total=0; errors=0} {total++; if($9 >= 500) errors++} END {rate=errors/total*100; if(rate > 5) {printf \"ALERT: Error rate %.2f%% (%d errors in %d requests)\\n\", rate, errors, total; system(\"echo \\\"Error rate alert: \" rate \"% at $(date)\\\" | mail -s \\\"High Error Rate\\\" admin@company.com\")}}' access.log",
      "explanation": "awk 'BEGIN {total=0; errors=0}' (initialize counters) {total++; if($9 >= 500) errors++} (count total requests and server errors 500+) END {rate=errors/total*100} (calculate error percentage) if(rate > 5) {...system(\"echo...| mail...\")} (if error rate > 5%, send email alert with details). Automated threshold-based alerting system.",
      "execution_time": "< 1 min",
      "requirements": [
        "mail"
      ],
      "warnings": "Configure mail system for email delivery",
      "use_case": "Production monitoring with automated alerting for service quality degradation"
    },
    {
      "id": 241,
      "command": [
        "awk",
        "sed"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "log processing",
      "tags": [
        "log cleaning",
        "anonymization",
        "privacy"
      ],
      "task": "Anonymize log files by masking IP addresses and sensitive data",
      "solution": "awk '{gsub(/[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}/, \"XXX.XXX.XXX.XXX\", $1); gsub(/email=[^&]*/, \"email=***@***.***\"); print}' access.log | sed 's/password=[^&]*/password=***/g' > anonymized.log",
      "explanation": "awk '{gsub(/[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}/, \"XXX.XXX.XXX.XXX\", $1)}' (replace IP addresses with placeholder) gsub(/email=[^&]*/, \"email=***@***.***\") (mask email parameters) print | sed 's/password=[^&]*/password=***/g' (mask password parameters) > anonymized.log. Data privacy protection for log sharing.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "GDPR compliance and safe log sharing for analysis or debugging"
    },
    {
      "id": 242,
      "command": [
        "awk",
        "logrotate"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log management",
      "tags": [
        "log rotation",
        "compression",
        "retention"
      ],
      "task": "Implement custom log rotation with size and age-based rules",
      "solution": "size=$(stat -c%s access.log); if [ \"$size\" -gt 104857600 ]; then timestamp=$(date +%Y%m%d_%H%M%S); mv access.log \"access.log.$timestamp\" && gzip \"access.log.$timestamp\" && touch access.log && find . -name \"access.log.*.gz\" -mtime +30 -delete; fi",
      "explanation": "size=$(stat -c%s access.log) (get file size in bytes) if [ \"$size\" -gt 104857600 ] (if size > 100MB) then timestamp=$(date +%Y%m%d_%H%M%S) (create timestamp) mv access.log \"access.log.$timestamp\" (rotate log with timestamp) && gzip \"access.log.$timestamp\" (compress rotated log) && touch access.log (create new log file) && find . -name \"access.log.*.gz\" -mtime +30 -delete (cleanup logs older than 30 days). Custom log rotation implementation.",
      "execution_time": "< 1 min",
      "requirements": [
        "gzip"
      ],
      "warnings": "Ensure application can handle log file rotation",
      "use_case": "Custom log management for applications without built-in rotation"
    },
    {
      "id": 243,
      "command": [
        "awk",
        "tail"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "log monitoring",
      "tags": [
        "real-time monitoring",
        "pattern detection",
        "live analysis"
      ],
      "task": "Real-time monitoring of multiple log types with pattern detection",
      "solution": "tail -f error.log access.log debug.log | awk '/ERROR|CRITICAL/ {print \"[ERROR]\" strftime(\"%H:%M:%S\") \" \" $0} /404|500/ {print \"[HTTP]\" strftime(\"%H:%M:%S\") \" \" $0} /DEBUG/ && /SQL/ {print \"[SQL]\" strftime(\"%H:%M:%S\") \" \" $0}'",
      "explanation": "tail -f error.log access.log debug.log (follow multiple log files simultaneously) | awk '/ERROR|CRITICAL/ {print \"[ERROR]\" strftime(\"%H:%M:%S\") \" \" $0}' (detect and timestamp error messages) /404|500/ {print \"[HTTP]\" strftime(\"%H:%M:%S\") \" \" $0} (detect HTTP errors) /DEBUG/ && /SQL/ {print \"[SQL]\" strftime(\"%H:%M:%S\") \" \" $0} (detect SQL debug messages). Multi-log real-time pattern detection.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Press Ctrl+C to stop monitoring",
      "use_case": "Development environment monitoring and live debugging across multiple services"
    },
    {
      "id": 244,
      "command": [
        "awk",
        "sqlite3"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "log analysis",
      "tags": [
        "database storage",
        "structured analysis",
        "sql queries"
      ],
      "task": "Import log data into SQLite database for complex analysis",
      "solution": "awk -F' ' '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7); printf \"INSERT INTO logs VALUES(\\047%s\\047,\\047%s\\047,\\047%s\\047,%d,%d);\\n\", $1, $4, $7, $9, $10}' access.log | sqlite3 -init <(echo 'CREATE TABLE logs(ip TEXT, timestamp TEXT, path TEXT, status INT, bytes INT);') logs.db",
      "explanation": "awk -F' ' '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7)}' (clean timestamp and path fields) printf \"INSERT INTO logs VALUES(\\047%s\\047,\\047%s\\047,\\047%s\\047,%d,%d);\\n\", $1, $4, $7, $9, $10 (generate SQL INSERT statements) | sqlite3 -init <(echo 'CREATE TABLE logs...') logs.db (create database table and import data). Log data warehousing for complex SQL analysis.",
      "execution_time": "1-5 min",
      "requirements": [
        "sqlite3"
      ],
      "warnings": null,
      "use_case": "Complex log analytics requiring SQL capabilities and data persistence"
    },
    {
      "id": 245,
      "command": [
        "awk",
        "curl"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log integration",
      "tags": [
        "api integration",
        "log forwarding",
        "external systems"
      ],
      "task": "Forward critical log events to external monitoring system via API",
      "solution": "tail -f application.log | awk '/CRITICAL|FATAL/ {gsub(/\"/, \"\\\\\\\"\", $0); system(\"curl -X POST -H \\\"Content-Type: application/json\\\" -d \\\"{\\\\\\\"timestamp\\\\\\\":\\\\\\\"\" strftime(\"%Y-%m-%dT%H:%M:%S\") \"\\\\\\\",\\\\\\\"level\\\\\\\":\\\\\\\"CRITICAL\\\\\\\",\\\\\\\"message\\\\\\\":\\\\\\\"\" $0 \"\\\\\\\"}\\\" https://monitoring.example.com/api/alerts\")}",
      "explanation": "tail -f application.log (follow log file) | awk '/CRITICAL|FATAL/' (detect critical events) {gsub(/\"/, \"\\\\\\\"\", $0)} (escape quotes for JSON) system(\"curl -X POST -H \\\"Content-Type: application/json\\\" -d \\\"...\\\" https://monitoring.example.com/api/alerts\") (send JSON payload to monitoring API). Real-time log event forwarding to external systems.",
      "execution_time": "long-running",
      "requirements": [
        "curl"
      ],
      "warnings": "Ensure API endpoint is accessible and authentication is configured",
      "use_case": "Integration with external monitoring platforms and incident management systems"
    },
    {
      "id": 246,
      "command": [
        "awk",
        "git"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log analysis",
      "tags": [
        "application logs",
        "deployment correlation",
        "version tracking"
      ],
      "task": "Correlate application errors with git deployment history",
      "solution": "error_time=$(awk '/ERROR/ {print $1, $2; exit}' app.log | xargs -I {} date -d '{}' '+%Y-%m-%d %H:%M:%S'); git log --since=\"$error_time\" --until=\"$(date '+%Y-%m-%d %H:%M:%S')\" --pretty=format:'%h %ad %s' --date=local | head -5 && echo && awk -v since=\"$error_time\" '$1 \" \" $2 >= since {print \"Error:\", $0}' app.log | head -10",
      "explanation": "error_time=$(awk '/ERROR/ {print $1, $2; exit}' app.log | xargs -I {} date -d '{}' '+%Y-%m-%d %H:%M:%S') (extract timestamp of first error and convert to standard format) git log --since=\"$error_time\" --until=\"$(date '+%Y-%m-%d %H:%M:%S')\" (show git commits since first error) awk -v since=\"$error_time\" '$1 \" \" $2 >= since {print \"Error:\", $0}' app.log (show errors since that time). Error-deployment correlation analysis.",
      "execution_time": "< 1 min",
      "requirements": [
        "git"
      ],
      "warnings": null,
      "use_case": "Root cause analysis linking application errors to recent code deployments"
    },
    {
      "id": 247,
      "command": [
        "awk",
        "rsyslog"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "log management",
      "tags": [
        "centralized logging",
        "syslog",
        "remote forwarding"
      ],
      "task": "Parse and forward application logs to centralized syslog server",
      "solution": "tail -f app.log | awk '{if(/ERROR/) priority=\"3\"; else if(/WARN/) priority=\"4\"; else priority=\"6\"; gsub(/\"/, \"\", $0); system(\"logger -p local0.\" priority \" -t myapp \\\"\" $0 \"\\\"\")}' && echo \"Configure rsyslog: echo 'local0.* @@syslog-server:514' >> /etc/rsyslog.conf\"",
      "explanation": "tail -f app.log (follow application log) | awk '{if(/ERROR/) priority=\"3\"; else if(/WARN/) priority=\"4\"; else priority=\"6\"}' (map log levels to syslog priorities: ERROR=3, WARN=4, others=6) gsub(/\"/, \"\", $0) (clean quotes) system(\"logger -p local0.\" priority \" -t myapp \\\"\" $0 \"\\\"\") (send to syslog with facility local0 and tag myapp). Centralized logging integration.",
      "execution_time": "long-running",
      "requirements": [
        "rsyslog"
      ],
      "warnings": "Configure rsyslog to forward to remote server",
      "use_case": "Enterprise log aggregation and centralized monitoring infrastructure"
    },
    {
      "id": 248,
      "command": [
        "awk",
        "prometheus"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "log metrics",
      "tags": [
        "metrics extraction",
        "prometheus",
        "monitoring"
      ],
      "task": "Extract metrics from logs and expose in Prometheus format",
      "solution": "awk 'BEGIN {print \"# HELP http_requests_total Total HTTP requests\"; print \"# TYPE http_requests_total counter\"} {status[$9]++; method[$6]++; total++} END {for(s in status) printf \"http_requests_total{status=\\\"%s\\\"} %d\\n\", s, status[s]; print \"# HELP http_requests_by_method_total Total requests by method\"; print \"# TYPE http_requests_by_method_total counter\"; for(m in method) printf \"http_requests_by_method_total{method=\\\"%s\\\"} %d\\n\", m, method[m]}' access.log > metrics.prom",
      "explanation": "awk 'BEGIN {print \"# HELP...\"; print \"# TYPE...\"}' (output Prometheus metric headers) {status[$9]++; method[$6]++; total++} (count by status code and HTTP method) END {for(s in status) printf \"http_requests_total{status=\\\"%s\\\"} %d\\n\", s, status[s]} (output status metrics) for(m in method) printf \"http_requests_by_method_total{method=\\\"%s\\\"} %d\\n\", m, method[m] (output method metrics) > metrics.prom. Prometheus-compatible metrics generation.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Custom application metrics for Prometheus monitoring and Grafana dashboards"
    },
    {
      "id": 249,
      "command": [
        "awk",
        "logstash"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "log processing",
      "tags": [
        "elk stack",
        "structured processing",
        "pipeline"
      ],
      "task": "Prepare logs for ELK stack ingestion with structured formatting",
      "solution": "awk '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7); printf \"{\\\"@timestamp\\\":\\\"%s\\\",\\\"client_ip\\\":\\\"%s\\\",\\\"method\\\":\\\"%s\\\",\\\"path\\\":\\\"%s\\\",\\\"status\\\":%d,\\\"bytes\\\":%d,\\\"user_agent\\\":\\\"%s\\\"}\\n\", $4, $1, $6, $7, $9, $10, substr($0, index($0, $12))}' access.log | head -5 && echo '# Logstash config:' && echo 'input { file { path => \"/path/to/logs/*.json\" codec => \"json\" } }'",
      "explanation": "awk '{gsub(/\\[|\\]/, \"\", $4); gsub(/\"/, \"\", $7)}' (clean timestamp and path) printf \"{\\\"@timestamp\\\":\\\"%s\\\",\\\"client_ip\\\":\\\"%s\\\",\\\"method\\\":\\\"%s\\\",\\\"path\\\":\\\"%s\\\",\\\"status\\\":%d,\\\"bytes\\\":%d,\\\"user_agent\\\":\\\"%s\\\"}\\n\" (format as JSON with ELK-friendly field names) echo 'input { file...}' (show Logstash configuration). ELK stack data preparation pipeline.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Modern log analytics pipeline for Elasticsearch, Logstash, Kibana stack"
    },
    {
      "id": 250,
      "command": [
        "awk",
        "machine learning"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "log analysis",
      "tags": [
        "anomaly detection",
        "pattern analysis",
        "ml preparation"
      ],
      "task": "Extract features from logs for machine learning anomaly detection",
      "solution": "awk 'BEGIN {print \"timestamp,hour,status,bytes,response_time,user_agent_length,path_length\"} {gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]; ua_len=length(substr($0, index($0, $12))); path_len=length($7); rt=$NF; printf \"%s,%d,%d,%d,%d,%d,%d\\n\", $4, hour, $9, $10, rt, ua_len, path_len}' access.log > ml_features.csv && echo \"# Use with: python -c 'import pandas as pd; from sklearn.ensemble import IsolationForest; df=pd.read_csv(\\\"ml_features.csv\\\"); model=IsolationForest(); anomalies=model.fit_predict(df.select_dtypes(include=[\\\"number\\\"])); print(\\\"Anomalies:\\\", sum(anomalies==-1))'\"",
      "explanation": "awk 'BEGIN {print \"timestamp,hour,status,bytes,response_time,user_agent_length,path_length\"}' (CSV header with feature names) {gsub(/\\[|\\]/, \"\", $4); split($4, dt, \":\"); hour=dt[2]} (extract timestamp and hour) ua_len=length(substr($0, index($0, $12))); path_len=length($7) (calculate string lengths as features) printf... (output CSV with numerical features) echo \"# Use with: python...\" (show ML usage example). Feature engineering for log anomaly detection.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Advanced log analytics with machine learning for automated anomaly detection"
    },
    {
      "id": 251,
      "command": [
        "curl"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "web",
      "tags": [
        "http",
        "api",
        "basic"
      ],
      "task": "Download a webpage and save it to a file",
      "solution": "curl -o webpage.html https://example.com",
      "explanation": "curl (transfer data to/from server) -o webpage.html (save output to specified file) https://example.com (target URL)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "Quick webpage backup or offline viewing"
    },
    {
      "id": 252,
      "command": [
        "curl"
      ],
      "difficulty": 1,
      "rating": 2,
      "category": "web",
      "tags": [
        "http",
        "headers",
        "debugging"
      ],
      "task": "Check HTTP headers of a website without downloading content",
      "solution": "curl -I https://example.com",
      "explanation": "curl (transfer data) -I (HEAD request only, fetch headers) https://example.com (target URL)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "Check server response codes, content types, or cache headers"
    },
    {
      "id": 253,
      "command": [
        "curl"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "web",
      "tags": [
        "api",
        "json",
        "post"
      ],
      "task": "Send JSON data to an API endpoint using POST request",
      "solution": "curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"John\",\"age\":30}' https://api.example.com/users",
      "explanation": "curl (transfer data) -X POST (HTTP POST method) -H \"Content-Type: application/json\" (set content type header) -d '{\"name\":\"John\",\"age\":30}' (JSON data payload) https://api.example.com/users (API endpoint)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "API testing, creating new resources via REST APIs"
    },
    {
      "id": 254,
      "command": [
        "curl"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "web",
      "tags": [
        "authentication",
        "api",
        "token"
      ],
      "task": "Access API with bearer token authentication",
      "solution": "curl -H \"Authorization: Bearer YOUR_TOKEN_HERE\" https://api.example.com/protected",
      "explanation": "curl (transfer data) -H \"Authorization: Bearer YOUR_TOKEN_HERE\" (set authorization header with bearer token) https://api.example.com/protected (protected API endpoint)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection",
        "valid API token"
      ],
      "warnings": null,
      "use_case": "Accessing protected APIs, OAuth2 authentication"
    },
    {
      "id": 255,
      "command": [
        "curl"
      ],
      "difficulty": 2,
      "rating": 2,
      "category": "web",
      "tags": [
        "download",
        "progress",
        "large files"
      ],
      "task": "Download large file with progress bar and resume capability",
      "solution": "curl -C - -# -o largefile.zip https://example.com/largefile.zip",
      "explanation": "curl (transfer data) -C - (continue/resume download from where it left off) -# (show progress bar instead of progress meter) -o largefile.zip (save to file) https://example.com/largefile.zip (source URL)",
      "execution_time": "5+ min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "Downloading large files with unreliable connections"
    },
    {
      "id": 256,
      "command": [
        "curl"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "web",
      "tags": [
        "forms",
        "post",
        "data"
      ],
      "task": "Submit form data with multiple fields",
      "solution": "curl -X POST -d \"username=john&password=secret&email=john@example.com\" https://example.com/login",
      "explanation": "curl (transfer data) -X POST (HTTP POST method) -d \"username=john&password=secret&email=john@example.com\" (form data with multiple fields) https://example.com/login (form action URL)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": "Be careful with credentials in command history",
      "use_case": "Automated form submissions, login testing"
    },
    {
      "id": 257,
      "command": [
        "curl"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "web",
      "tags": [
        "cookies",
        "session",
        "login"
      ],
      "task": "Login and maintain session using cookies for subsequent requests",
      "solution": "curl -c cookies.txt -d \"username=john&password=secret\" https://example.com/login && curl -b cookies.txt https://example.com/dashboard",
      "explanation": "curl -c cookies.txt (save cookies to file) -d \"username=john&password=secret\" (login data) https://example.com/login (login endpoint) && (execute next command if first succeeds) curl -b cookies.txt (use saved cookies) https://example.com/dashboard (protected page)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": "Cookies file may contain sensitive session data",
      "use_case": "Automated workflows requiring authentication state"
    },
    {
      "id": 258,
      "command": [
        "curl"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "web",
      "tags": [
        "file upload",
        "multipart",
        "api"
      ],
      "task": "Upload file to server using multipart form data",
      "solution": "curl -X POST -F \"file=@document.pdf\" -F \"description=Important document\" https://api.example.com/upload",
      "explanation": "curl (transfer data) -X POST (HTTP POST method) -F \"file=@document.pdf\" (upload file using @ prefix) -F \"description=Important document\" (additional form field) https://api.example.com/upload (upload endpoint)",
      "execution_time": "1-5 min",
      "requirements": [
        "internet connection",
        "file to upload"
      ],
      "warnings": null,
      "use_case": "File uploads to web services, API file submissions"
    },
    {
      "id": 259,
      "command": [
        "curl"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "monitoring",
      "tags": [
        "timing",
        "performance",
        "debugging"
      ],
      "task": "Measure detailed timing information for HTTP request",
      "solution": "curl -w \"@curl-format.txt\" -o /dev/null -s https://example.com",
      "explanation": "curl (transfer data) -w \"@curl-format.txt\" (use custom format file for timing output) -o /dev/null (discard response body) -s (silent, no progress) https://example.com (target URL). Format file contains timing variables like %{time_total}, %{time_connect}",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection",
        "curl-format.txt file"
      ],
      "warnings": null,
      "use_case": "Performance testing, debugging slow API responses"
    },
    {
      "id": 260,
      "command": [
        "curl"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "web",
      "tags": [
        "proxy",
        "network",
        "testing"
      ],
      "task": "Make request through SOCKS proxy with custom user agent",
      "solution": "curl --socks5 127.0.0.1:1080 -H \"User-Agent: MyBot/1.0\" https://httpbin.org/ip",
      "explanation": "curl (transfer data) --socks5 127.0.0.1:1080 (use SOCKS5 proxy on localhost port 1080) -H \"User-Agent: MyBot/1.0\" (set custom user agent header) https://httpbin.org/ip (service that returns your IP)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection",
        "SOCKS proxy running on port 1080"
      ],
      "warnings": null,
      "use_case": "Anonymous browsing, testing through different network paths"
    },
    {
      "id": 261,
      "command": [
        "curl"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "automation",
      "tags": [
        "json",
        "api",
        "processing"
      ],
      "task": "Fetch JSON data and extract specific fields using curl and jq",
      "solution": "curl -s https://api.github.com/users/octocat | jq '.name, .public_repos, .followers'",
      "explanation": "curl -s (silent mode, no progress) https://api.github.com/users/octocat (GitHub API endpoint) | (pipe output) jq '.name, .public_repos, .followers' (extract specific JSON fields)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection",
        "jq"
      ],
      "warnings": null,
      "use_case": "API data extraction, automated reporting from web services"
    },
    {
      "id": 262,
      "command": [
        "curl"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": [
        "parallel",
        "multiple",
        "performance"
      ],
      "task": "Download multiple files in parallel with different retry policies",
      "solution": "curl -Z --retry 3 --retry-delay 2 -o \"file_#1.html\" \"https://example.com/page[1-5].html\"",
      "explanation": "curl (transfer data) -Z (parallel transfers) --retry 3 (retry failed transfers 3 times) --retry-delay 2 (wait 2 seconds between retries) -o \"file_#1.html\" (output filename pattern with sequence) \"https://example.com/page[1-5].html\" (URL pattern for pages 1-5)",
      "execution_time": "1-5 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "Bulk downloading with fault tolerance, web scraping"
    },
    {
      "id": 263,
      "command": [
        "curl"
      ],
      "difficulty": 4,
      "rating": 4,
      "category": "security",
      "tags": [
        "ssl",
        "certificates",
        "verification"
      ],
      "task": "Check SSL certificate details and save certificate chain",
      "solution": "curl -vI --cert-status -o /dev/null https://example.com 2>&1 | grep -E '(subject:|issuer:|expire date:)'",
      "explanation": "curl (transfer data) -vI (verbose headers only) --cert-status (check certificate status via OCSP) -o /dev/null (discard body) https://example.com (target site) 2>&1 (redirect stderr to stdout) | grep -E (filter for certificate info patterns)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "SSL certificate monitoring, security auditing"
    },
    {
      "id": 264,
      "command": [
        "curl"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": [
        "rate limiting",
        "automation",
        "api"
      ],
      "task": "Make API calls with rate limiting and automatic retry on specific HTTP codes",
      "solution": "curl --retry 5 --retry-connrefused --retry-delay 10 --max-time 30 -H \"X-RateLimit-Bypass: false\" https://api.example.com/data || echo \"Failed after retries\"",
      "explanation": "curl (transfer data) --retry 5 (retry up to 5 times) --retry-connrefused (retry on connection refused) --retry-delay 10 (wait 10 seconds between retries) --max-time 30 (timeout after 30 seconds) -H \"X-RateLimit-Bypass: false\" (custom header) https://api.example.com/data (API endpoint) || echo \"Failed after retries\" (fallback message)",
      "execution_time": "1-5 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "Robust API interactions with unreliable services"
    },
    {
      "id": 265,
      "command": [
        "curl"
      ],
      "difficulty": 3,
      "rating": 3,
      "category": "web",
      "tags": [
        "redirects",
        "location",
        "following"
      ],
      "task": "Follow redirects and show the final URL destination",
      "solution": "curl -Ls -o /dev/null -w \"%{url_effective}\" https://bit.ly/shortened-url",
      "explanation": "curl (transfer data) -L (follow redirects) -s (silent) -o /dev/null (discard response body) -w \"%{url_effective}\" (output final URL after redirects) https://bit.ly/shortened-url (shortened URL)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "URL unshortening, tracking redirect chains"
    },
    {
      "id": 266,
      "command": [
        "curl"
      ],
      "difficulty": 5,
      "rating": 5,
      "category": "automation",
      "tags": [
        "api",
        "pagination",
        "json",
        "complex"
      ],
      "task": "Fetch all pages of paginated API results and combine into single JSON file",
      "solution": "page=1; echo '[' > all_data.json; while true; do response=$(curl -s \"https://api.example.com/data?page=$page\"); if [[ $(echo \"$response\" | jq '.results | length') -eq 0 ]]; then break; fi; if [[ $page -gt 1 ]]; then echo ',' >> all_data.json; fi; echo \"$response\" | jq '.results[]' >> all_data.json; ((page++)); done; echo ']' >> all_data.json",
      "explanation": "Complex script that initializes page counter, creates JSON array, loops through API pages using curl with pagination parameter, checks if results exist using jq, handles JSON comma separation, extracts results array, and combines all pages into single JSON file",
      "execution_time": "5+ min",
      "requirements": [
        "internet connection",
        "jq",
        "bash"
      ],
      "warnings": "May create large files, can make many API requests",
      "use_case": "Complete dataset extraction from paginated APIs"
    },
    {
      "id": 267,
      "command": [
        "curl"
      ],
      "difficulty": 2,
      "rating": 3,
      "category": "web",
      "tags": [
        "basic auth",
        "credentials",
        "api"
      ],
      "task": "Access API endpoint with basic authentication",
      "solution": "curl -u username:password https://api.example.com/protected/data",
      "explanation": "curl (transfer data) -u username:password (basic authentication credentials) https://api.example.com/protected/data (protected endpoint requiring authentication)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection",
        "valid credentials"
      ],
      "warnings": "Credentials visible in command history and process list",
      "use_case": "Accessing protected APIs, legacy authentication systems"
    },
    {
      "id": 268,
      "command": [
        "curl"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "web",
      "tags": [
        "webhook",
        "testing",
        "local"
      ],
      "task": "Test webhook by sending POST data to local development server",
      "solution": "curl -X POST -H \"Content-Type: application/json\" -d @webhook_payload.json http://localhost:3000/webhook",
      "explanation": "curl (transfer data) -X POST (HTTP POST method) -H \"Content-Type: application/json\" (set JSON content type) -d @webhook_payload.json (send data from file using @ prefix) http://localhost:3000/webhook (local webhook endpoint)",
      "execution_time": "< 1 min",
      "requirements": [
        "local server running",
        "webhook_payload.json file"
      ],
      "warnings": null,
      "use_case": "Webhook development, API testing, local service integration testing"
    },
    {
      "id": 269,
      "command": [
        "curl"
      ],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": [
        "health check",
        "monitoring",
        "status"
      ],
      "task": "Create health check script that tests multiple endpoints and reports status",
      "solution": "for url in \"https://api1.example.com/health\" \"https://api2.example.com/status\" \"https://db.example.com/ping\"; do status=$(curl -s -o /dev/null -w \"%{http_code}\" \"$url\"); echo \"$url: $status\"; done",
      "explanation": "for loop through array of health check URLs, curl -s (silent) -o /dev/null (discard body) -w \"%{http_code}\" (output only HTTP status code) for each URL, echo the URL and its status code",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection"
      ],
      "warnings": null,
      "use_case": "Service monitoring, automated health checks, uptime verification"
    },
    {
      "id": 270,
      "command": [
        "curl"
      ],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": [
        "api",
        "csv",
        "data processing"
      ],
      "task": "Fetch API data, convert JSON to CSV, and save with timestamp",
      "solution": "timestamp=$(date +%Y%m%d_%H%M%S); curl -s https://api.example.com/users | jq -r '.[] | [.id, .name, .email, .created_at] | @csv' > \"users_$timestamp.csv\"",
      "explanation": "timestamp=$(date +%Y%m%d_%H%M%S) (create timestamp variable), curl -s (silent API call) https://api.example.com/users (API endpoint) | jq -r (raw output) '.[] | [.id, .name, .email, .created_at] | @csv' (extract fields and format as CSV) > \"users_$timestamp.csv\" (save to timestamped file)",
      "execution_time": "< 1 min",
      "requirements": [
        "internet connection",
        "jq"
      ],
      "warnings": null,
      "use_case": "Data export, API data archiving, automated reporting"
    }
  ]
}
