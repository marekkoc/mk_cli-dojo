{
  "tasks": [
    {
      "id": 193,
      "command": ["for"],
      "difficulty": 1,
      "rating": 2,
      "category": "scripting",
      "tags": ["for loop", "basic", "iteration"],
      "task": "Print numbers from 1 to 10 using for loop",
      "solution": "for i in {1..10}; do echo $i; done",
      "explanation": "for i in {1..10} (brace expansion creates sequence 1, 2, 3...10) do echo $i (print current number) done (end loop). Bash brace expansion {start..end} generates numeric sequences efficiently.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Basic iteration for numbering, indexing, or simple counting operations"
    },
    {
      "id": 194,
      "command": ["for"],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": ["for loop", "files", "batch processing"],
      "task": "Rename all .txt files by adding current date prefix",
      "solution": "for file in *.txt; do mv \"$file\" \"$(date +%Y%m%d)_$file\"; done",
      "explanation": "for file in *.txt (iterate through all .txt files in current directory) do mv \"$file\" \"$(date +%Y%m%d)_$file\" (rename each file with YYYYMMDD prefix using command substitution) done. Quotes protect filenames with spaces.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Batch file renaming with timestamps for organization or backup purposes"
    },
    {
      "id": 195,
      "command": ["for", "mkdir"],
      "difficulty": 2,
      "rating": 3,
      "category": "file management",
      "tags": ["for loop", "directory creation", "structure"],
      "task": "Create project directory structure for months of the year",
      "solution": "for month in Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec; do mkdir -p \"2024_$month\"; done",
      "explanation": "for month in Jan Feb Mar... (iterate through month abbreviations) do mkdir -p \"2024_$month\" (create directory with year prefix, -p creates parent dirs if needed) done. Creates organized directory structure for project planning.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Project organization, creating systematic directory structures for planning"
    },
    {
      "id": 196,
      "command": ["for", "cp"],
      "difficulty": 3,
      "rating": 4,
      "category": "backup",
      "tags": ["for loop", "backup", "multiple copies"],
      "task": "Create multiple numbered backups of important file",
      "solution": "for i in {1..5}; do cp important.txt \"backup_${i}_$(date +%H%M%S).txt\"; sleep 1; done",
      "explanation": "for i in {1..5} (create 5 backups) do cp important.txt \"backup_${i}_$(date +%H%M%S).txt\" (copy with numbered and timestamped filename) sleep 1 (wait 1 second for different timestamps) done. Creates multiple backup versions with unique names.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Create multiple backup versions before risky operations or for version control"
    },
    {
      "id": 197,
      "command": ["for", "find"],
      "difficulty": 3,
      "rating": 4,
      "category": "file management",
      "tags": ["for loop", "file processing", "compression"],
      "task": "Find and compress all log files in multiple directories",
      "solution": "for dir in /var/log /home/user/logs /opt/app/logs; do find \"$dir\" -name \"*.log\" -exec gzip {} \\; 2>/dev/null; done",
      "explanation": "for dir in /var/log /home/user/logs /opt/app/logs (iterate through multiple log directories) do find \"$dir\" -name \"*.log\" -exec gzip {} \\; (find and compress log files) 2>/dev/null (suppress error messages) done. Systematic log compression across directories.",
      "execution_time": "1-5 min",
      "requirements": ["gzip"],
      "warnings": "Ensure log files are not actively being written to",
      "use_case": "System maintenance and log rotation across multiple applications"
    },
    {
      "id": 198,
      "command": ["for", "curl"],
      "difficulty": 4,
      "rating": 5,
      "category": "web",
      "tags": ["for loop", "api", "monitoring"],
      "task": "Check multiple website endpoints and report status",
      "solution": "for url in https://api1.com/health https://api2.com/status https://api3.com/ping; do echo -n \"$url: \"; curl -s -o /dev/null -w \"%{http_code}\" \"$url\" || echo \"FAILED\"; echo; done",
      "explanation": "for url in https://api1.com/health... (iterate through API endpoints) do echo -n \"$url: \" (print URL without newline) curl -s -o /dev/null -w \"%{http_code}\" \"$url\" (silent request, discard output, show only HTTP status) || echo \"FAILED\" (if curl fails) echo (add newline). Systematic endpoint monitoring.",
      "execution_time": "< 1 min",
      "requirements": ["curl"],
      "warnings": null,
      "use_case": "API health monitoring and service availability checking"
    },
    {
      "id": 199,
      "command": ["for", "ssh"],
      "difficulty": 4,
      "rating": 4,
      "category": "system admin",
      "tags": ["for loop", "remote", "deployment"],
      "task": "Deploy script to multiple servers and execute remotely",
      "solution": "for server in web1.com web2.com web3.com; do echo \"Deploying to $server\"; scp deploy.sh \"$server:/tmp/\" && ssh \"$server\" 'chmod +x /tmp/deploy.sh && /tmp/deploy.sh'; done",
      "explanation": "for server in web1.com web2.com web3.com (iterate through server list) do echo \"Deploying to $server\" (show progress) scp deploy.sh \"$server:/tmp/\" (copy script to server) && ssh \"$server\" 'chmod +x /tmp/deploy.sh && /tmp/deploy.sh' (if copy succeeds, make executable and run). Automated deployment workflow.",
      "execution_time": "1-5 min",
      "requirements": ["ssh", "scp"],
      "warnings": "Ensure SSH keys are configured for passwordless access",
      "use_case": "Automated deployment and configuration management across server fleet"
    },
    {
      "id": 200,
      "command": ["for", "awk"],
      "difficulty": 3,
      "rating": 4,
      "category": "data analysis",
      "tags": ["for loop", "csv processing", "reports"],
      "task": "Process multiple CSV files and generate summary report",
      "solution": "for file in *.csv; do echo \"Processing $file:\"; awk -F',' 'NR>1 {sum+=$3; count++} END {printf \"Records: %d, Total: %.2f, Average: %.2f\\n\", count, sum, sum/count}' \"$file\"; done",
      "explanation": "for file in *.csv (iterate through CSV files) do echo \"Processing $file:\" (show current file) awk -F',' 'NR>1 {sum+=$3; count++} END {...}' (skip header, sum column 3, calculate statistics) \"$file\" done. Batch CSV analysis with individual file reporting.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Batch data analysis for financial reports or statistical summaries"
    },
    {
      "id": 201,
      "command": ["while"],
      "difficulty": 2,
      "rating": 3,
      "category": "monitoring",
      "tags": ["while loop", "file monitoring", "real-time"],
      "task": "Monitor file size and alert when it exceeds limit",
      "solution": "while true; do size=$(stat -f%z file.log 2>/dev/null || stat -c%s file.log 2>/dev/null); if [ \"$size\" -gt 1048576 ]; then echo \"File size exceeded 1MB: $size bytes\"; break; fi; sleep 5; done",
      "explanation": "while true (infinite loop) do size=$(stat -f%z file.log 2>/dev/null || stat -c%s file.log 2>/dev/null) (get file size, try macOS then Linux format) if [ \"$size\" -gt 1048576 ] (if size > 1MB) then echo... break (alert and exit loop) sleep 5 (wait 5 seconds) done. Continuous file size monitoring.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Press Ctrl+C to stop if size limit not reached",
      "use_case": "Monitor log file growth and trigger alerts or rotation when limits exceeded"
    },
    {
      "id": 202,
      "command": ["while", "read"],
      "difficulty": 3,
      "rating": 4,
      "category": "data processing",
      "tags": ["while loop", "file reading", "line processing"],
      "task": "Read file line by line and process each line with custom logic",
      "solution": "while IFS= read -r line; do echo \"Processing: $line\"; echo \"$line\" | tr '[:lower:]' '[:upper:]' >> output.txt; done < input.txt",
      "explanation": "while IFS= read -r line (read line preserving whitespace, -r prevents backslash interpretation) do echo \"Processing: $line\" (show progress) echo \"$line\" | tr '[:lower:]' '[:upper:]' >> output.txt (convert to uppercase, append to output) done < input.txt (read from input file). Line-by-line file processing.",
      "execution_time": "< 1 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Text file transformation, data cleaning, or custom line-by-line processing"
    },
    {
      "id": 203,
      "command": ["while", "ps"],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": ["while loop", "process monitoring", "system"],
      "task": "Monitor specific process and restart if it stops running",
      "solution": "while true; do if ! pgrep -f \"my_application\" > /dev/null; then echo \"$(date): Process not found, restarting...\"; /path/to/my_application & fi; sleep 30; done",
      "explanation": "while true (continuous monitoring) do if ! pgrep -f \"my_application\" > /dev/null (if process not found, pgrep searches by name) then echo \"$(date): Process not found, restarting...\" (timestamped alert) /path/to/my_application & (restart in background) sleep 30 (check every 30 seconds) done. Process watchdog implementation.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": "Ensure application path is correct and executable",
      "use_case": "Service monitoring and automatic restart for critical applications"
    },
    {
      "id": 204,
      "command": ["while", "netstat"],
      "difficulty": 4,
      "rating": 4,
      "category": "network",
      "tags": ["while loop", "network monitoring", "connections"],
      "task": "Monitor network connections and log suspicious activity",
      "solution": "while true; do netstat -tn | awk '/ESTABLISHED/ && $5 !~ /^192\\.168|^10\\.|^172\\.(1[6-9]|2[0-9]|3[01])/ {print strftime(\"%Y-%m-%d %H:%M:%S\"), \"External connection:\", $5}' >> suspicious.log; sleep 60; done",
      "explanation": "while true (continuous monitoring) do netstat -tn (show TCP connections numerically) | awk '/ESTABLISHED/ && $5 !~ /^192\\.168|^10\\.|^172\\.(1[6-9]|2[0-9]|3[01])/' (filter established connections from non-private IPs) {print strftime(...), \"External connection:\", $5} (log with timestamp) >> suspicious.log; sleep 60 done. Network security monitoring.",
      "execution_time": "long-running",
      "requirements": ["netstat", "awk"],
      "warnings": null,
      "use_case": "Security monitoring for detecting external network connections"
    },
    {
      "id": 205,
      "command": ["for", "git"],
      "difficulty": 4,
      "rating": 5,
      "category": "version control",
      "tags": ["for loop", "git", "repository management"],
      "task": "Update multiple git repositories in parallel",
      "solution": "for repo in ~/projects/*/; do (cd \"$repo\" && echo \"Updating $(basename \"$repo\")\" && git pull origin main 2>&1) & done; wait",
      "explanation": "for repo in ~/projects/*/ (iterate through subdirectories in projects) do (cd \"$repo\" && echo \"Updating $(basename \"$repo\")\" && git pull origin main 2>&1) & (subshell: change to repo, show name, pull updates, run in background with &) done; wait (wait for all background jobs to complete). Parallel git repository updates.",
      "execution_time": "1-5 min",
      "requirements": ["git"],
      "warnings": "Ensure repositories have no uncommitted changes",
      "use_case": "Mass repository updates for development environment synchronization"
    },
    {
      "id": 206,
      "command": ["while", "df"],
      "difficulty": 3,
      "rating": 4,
      "category": "monitoring",
      "tags": ["while loop", "disk monitoring", "alerts"],
      "task": "Monitor disk usage and send alert when threshold exceeded",
      "solution": "while true; do usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//'); if [ \"$usage\" -gt 80 ]; then echo \"ALERT: Disk usage at ${usage}% on $(date)\" | mail -s \"Disk Space Warning\" admin@company.com; fi; sleep 3600; done",
      "explanation": "while true (continuous monitoring) do usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//') (get disk usage percentage, remove % symbol) if [ \"$usage\" -gt 80 ] (if usage > 80%) then echo... | mail (send email alert) sleep 3600 (check hourly) done. Automated disk space monitoring with email alerts.",
      "execution_time": "long-running",
      "requirements": ["mail"],
      "warnings": "Configure mail system for email alerts",
      "use_case": "Proactive system monitoring and automated alerting for disk space management"
    },
    {
      "id": 207,
      "command": ["for", "imagemagick"],
      "difficulty": 3,
      "rating": 4,
      "category": "multimedia",
      "tags": ["for loop", "image processing", "batch"],
      "task": "Batch resize and optimize images with different sizes",
      "solution": "for size in 100x100 300x300 800x600; do mkdir -p \"resized_$size\"; for img in *.jpg; do convert \"$img\" -resize \"$size>\" -quality 85 \"resized_$size/$img\"; done; done",
      "explanation": "for size in 100x100 300x300 800x600 (iterate through target sizes) do mkdir -p \"resized_$size\" (create output directory) for img in *.jpg (nested loop through images) do convert \"$img\" -resize \"$size>\" (resize only if larger, preserve aspect ratio) -quality 85 \"resized_$size/$img\" (optimize quality, save to size-specific directory). Nested loops for multi-size image processing.",
      "execution_time": "5+ min",
      "requirements": ["imagemagick"],
      "warnings": null,
      "use_case": "Web development image optimization for responsive design and different screen sizes"
    },
    {
      "id": 208,
      "command": ["while", "tail"],
      "difficulty": 4,
      "rating": 5,
      "category": "monitoring",
      "tags": ["while loop", "log monitoring", "pattern detection"],
      "task": "Monitor log file for error patterns and trigger actions",
      "solution": "tail -f application.log | while read line; do if echo \"$line\" | grep -q \"ERROR\\|FATAL\"; then echo \"$(date): Found error: $line\" >> error_alerts.log; echo \"Error detected\" | mail -s \"Application Error\" admin@company.com; fi; done",
      "explanation": "tail -f application.log (follow log file) | while read line (read each new line) do if echo \"$line\" | grep -q \"ERROR\\|FATAL\" (check for error patterns) then echo \"$(date): Found error: $line\" >> error_alerts.log (log with timestamp) echo \"Error detected\" | mail... (send email alert) done. Real-time log monitoring with automated response.",
      "execution_time": "long-running",
      "requirements": ["mail"],
      "warnings": "Press Ctrl+C to stop monitoring",
      "use_case": "Production system monitoring with immediate error response and notification"
    },
    {
      "id": 209,
      "command": ["for", "jq"],
      "difficulty": 4,
      "rating": 5,
      "category": "data analysis",
      "tags": ["for loop", "json", "api processing"],
      "task": "Fetch and merge data from multiple API endpoints",
      "solution": "for endpoint in users orders products; do curl -s \"https://api.example.com/$endpoint\" | jq '.' > \"${endpoint}.json\"; done; jq -s '{users: .[0], orders: .[1], products: .[2]}' users.json orders.json products.json > merged_data.json",
      "explanation": "for endpoint in users orders products (iterate through API endpoints) do curl -s \"https://api.example.com/$endpoint\" (fetch data silently) | jq '.' > \"${endpoint}.json\" (format and save JSON) done; jq -s '{users: .[0], orders: .[1], products: .[2]}' (slurp files and create merged structure) > merged_data.json. API data aggregation workflow.",
      "execution_time": "< 1 min",
      "requirements": ["curl", "jq"],
      "warnings": null,
      "use_case": "Data integration from multiple microservices or API sources"
    },
    {
      "id": 210,
      "command": ["while", "inotifywait"],
      "difficulty": 4,
      "rating": 5,
      "category": "monitoring",
      "tags": ["while loop", "file watching", "automation"],
      "task": "Watch directory for file changes and auto-process new files",
      "solution": "while inotifywait -e create -e moved_to /watch/directory; do for file in /watch/directory/*.txt; do if [ -f \"$file\" ]; then echo \"Processing $file\"; mv \"$file\" /processed/; fi; done; done",
      "explanation": "while inotifywait -e create -e moved_to /watch/directory (wait for file creation or move events) do for file in /watch/directory/*.txt (check for text files) do if [ -f \"$file\" ] (if file exists) then echo \"Processing $file\"; mv \"$file\" /processed/ (move to processed directory) done. Real-time file processing automation.",
      "execution_time": "long-running",
      "requirements": ["inotify-tools"],
      "warnings": null,
      "use_case": "Automated file processing workflow for document management or data ingestion"
    },
    {
      "id": 211,
      "command": ["for", "awk", "sort"],
      "difficulty": 4,
      "rating": 4,
      "category": "data analysis",
      "tags": ["for loop", "log analysis", "statistics"],
      "task": "Analyze multiple log files and generate comprehensive report",
      "solution": "for log in *.log; do echo \"=== $log ===\"; awk '{print $1}' \"$log\" | sort | uniq -c | sort -nr | head -5; awk '/ERROR/ {count++} END {print \"Errors:\", count+0}' \"$log\"; echo; done > analysis_report.txt",
      "explanation": "for log in *.log (iterate through log files) do echo \"=== $log ===\" (file separator) awk '{print $1}' \"$log\" | sort | uniq -c | sort -nr | head -5 (top 5 IP addresses by frequency) awk '/ERROR/ {count++} END {print \"Errors:\", count+0}' (count errors) done > analysis_report.txt. Comprehensive log analysis with multiple metrics.",
      "execution_time": "1-5 min",
      "requirements": null,
      "warnings": null,
      "use_case": "Security analysis and system health reporting from multiple log sources"
    },
    {
      "id": 212,
      "command": ["while", "read", "mysql"],
      "difficulty": 5,
      "rating": 5,
      "category": "database",
      "tags": ["while loop", "database", "batch processing"],
      "task": "Read CSV file and insert data into database with error handling",
      "solution": "while IFS=',' read -r id name email status; do if mysql -u user -p'password' -e \"INSERT INTO users (id, name, email, status) VALUES ('$id', '$name', '$email', '$status');\" database; then echo \"Inserted: $name\"; else echo \"Failed: $name\" >> failed_inserts.log; fi; done < users.csv",
      "explanation": "while IFS=',' read -r id name email status (read CSV with comma delimiter) do if mysql -u user -p'password' -e \"INSERT INTO users...\" (attempt database insert) then echo \"Inserted: $name\" (success message) else echo \"Failed: $name\" >> failed_inserts.log (log failures) done < users.csv. Database bulk insert with error handling and logging.",
      "execution_time": "1-5 min",
      "requirements": ["mysql"],
      "warnings": "Store database credentials securely, validate input data",
      "use_case": "ETL processes for importing CSV data into databases with error tracking"
    },
    {
      "id": 213,
      "command": ["for", "parallel"],
      "difficulty": 5,
      "rating": 5,
      "category": "automation",
      "tags": ["for loop", "parallel processing", "optimization"],
      "task": "Process large dataset files in parallel with load balancing",
      "solution": "for file in data_*.csv; do echo \"$file\"; done | parallel -j 4 'echo \"Processing {}\"; awk -F\",\" \"NR>1 {sum+=\\$3} END {print \\\"{}: Total \\\" sum}\" {}'",
      "explanation": "for file in data_*.csv; do echo \"$file\"; done (list files) | parallel -j 4 (process 4 files simultaneously) 'echo \"Processing {}\"; awk -F\",\" \"NR>1 {sum+=\\$3} END {print \\\"{}: Total \\\" sum}\" {}' (process each file: skip header, sum column 3, report total). Parallel processing for performance optimization.",
      "execution_time": "1-5 min",
      "requirements": ["parallel"],
      "warnings": null,
      "use_case": "High-performance data processing for large datasets with optimal CPU utilization"
    },
    {
      "id": 214,
      "command": ["while", "sleep", "uptime"],
      "difficulty": 3,
      "rating": 3,
      "category": "monitoring",
      "tags": ["while loop", "system monitoring", "performance"],
      "task": "Create system load monitoring script with threshold alerts",
      "solution": "while true; do load=$(uptime | awk '{print $(NF-2)}' | sed 's/,//'); if (( $(echo \"$load > 2.0\" | bc -l) )); then echo \"$(date): High load detected: $load\" | tee -a load_alerts.log; fi; sleep 60; done",
      "explanation": "while true (continuous monitoring) do load=$(uptime | awk '{print $(NF-2)}' | sed 's/,//') (extract 1-minute load average, remove comma) if (( $(echo \"$load > 2.0\" | bc -l) )) (compare float values using bc calculator) then echo... | tee -a load_alerts.log (log and display alert) sleep 60 (check every minute). System load monitoring with threshold alerting.",
      "execution_time": "long-running",
      "requirements": ["bc"],
      "warnings": null,
      "use_case": "Server performance monitoring and load balancing decision support"
    },
    {
      "id": 215,
      "command": ["for", "rsync"],
      "difficulty": 4,
      "rating": 4,
      "category": "backup",
      "tags": ["for loop", "backup", "synchronization"],
      "task": "Synchronize multiple directory pairs with incremental backup",
      "solution": "for sync_pair in \"src1/:dest1/\" \"src2/:dest2/\" \"src3/:dest3/\"; do src=${sync_pair%:*}; dest=${sync_pair#*:}; echo \"Syncing $src to $dest\"; rsync -av --delete \"$src\" \"$dest\" && echo \"Success: $src\" || echo \"Failed: $src\"; done",
      "explanation": "for sync_pair in \"src1/:dest1/\"... (iterate through source:destination pairs) do src=${sync_pair%:*} (extract source path before colon) dest=${sync_pair#*:} (extract destination after colon) rsync -av --delete \"$src\" \"$dest\" (sync with archive mode and deletion) && echo \"Success\" || echo \"Failed\" (status reporting). Batch synchronization with error handling.",
      "execution_time": "5+ min",
      "requirements": ["rsync"],
      "warnings": "Test sync pairs before running, --delete removes files from destination",
      "use_case": "Automated backup workflow for multiple directory pairs with status reporting"
    },
    {
      "id": 216,
      "command": ["while", "ping"],
      "difficulty": 3,
      "rating": 4,
      "category": "network",
      "tags": ["while loop", "network monitoring", "connectivity"],
      "task": "Monitor network connectivity and measure response times",
      "solution": "while true; do for host in google.com github.com stackoverflow.com; do ping -c 1 \"$host\" | awk '/time=/ {print strftime(\"%Y-%m-%d %H:%M:%S\"), \"'$host':\", $7}' || echo \"$(date): $host: UNREACHABLE\"; done; sleep 30; done",
      "explanation": "while true (continuous monitoring) do for host in google.com... (test multiple hosts) do ping -c 1 \"$host\" (send single ping) | awk '/time=/ {print strftime(...), \"'$host':\", $7}' (extract and format response time) || echo \"UNREACHABLE\" (if ping fails) sleep 30 (test every 30 seconds). Network connectivity monitoring with response time tracking.",
      "execution_time": "long-running",
      "requirements": null,
      "warnings": null,
      "use_case": "Network performance monitoring and connectivity verification for remote work"
    },
    {
      "id": 217,
      "command": ["for", "while", "find"],
      "difficulty": 5,
      "rating": 5,
      "category": "automation",
      "tags": ["nested loops", "complex workflow", "file management"],
      "task": "Complex nested loop for directory cleanup with age-based archiving",
      "solution": "for base_dir in /var/log /home/user/projects /opt/apps; do while read -r dir; do echo \"Processing: $dir\"; find \"$dir\" -type f -mtime +30 -name \"*.log\" -exec tar -czf \"${dir}/archive_$(date +%Y%m%d).tar.gz\" {} + -delete 2>/dev/null; done < <(find \"$base_dir\" -type d -maxdepth 2); done",
      "explanation": "for base_dir in /var/log... (iterate base directories) do while read -r dir (read each subdirectory) do find \"$dir\" -type f -mtime +30 -name \"*.log\" (find old log files) -exec tar -czf \"${dir}/archive_$(date +%Y%m%d).tar.gz\" {} + (archive them) -delete (remove originals) done < <(find \"$base_dir\" -type d -maxdepth 2) (process substitution for directory list). Complex nested automation for systematic cleanup.",
      "execution_time": "5+ min",
      "requirements": ["tar"],
      "warnings": "Test on non-critical directories first, archives old files and deletes originals",
      "use_case": "Enterprise system maintenance with automated archiving and cleanup workflows"
    }
  ]
}